{
  "dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1",
  "evaluation_time": 480.8411798477173,
  "kg_co2_emissions": 0.010090518897462273,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.8840674733265781,
          "accuracy_threshold": 0.9207137823104858,
          "ap": 0.8051144963019613,
          "f1": 0.7371534195933457,
          "f1_threshold": 0.9082125425338745,
          "precision": 0.6915606936416185,
          "recall": 0.7891820580474934
        },
        "dot": {
          "accuracy": 0.7841091971150981,
          "accuracy_threshold": 413.59814453125,
          "ap": 0.4517539453432835,
          "f1": 0.4916557284830302,
          "f1_threshold": 375.728271484375,
          "precision": 0.38132635253054104,
          "recall": 0.691820580474934
        },
        "euclidean": {
          "accuracy": 0.883769446265721,
          "accuracy_threshold": 8.28347396850586,
          "ap": 0.8019206667546761,
          "f1": 0.7374414779197772,
          "f1_threshold": 8.709039688110352,
          "precision": 0.7084852905421833,
          "recall": 0.7688654353562006
        },
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.8051144963019613,
        "manhattan": {
          "accuracy": 0.8837098408535495,
          "accuracy_threshold": 210.15908813476562,
          "ap": 0.8012737421311333,
          "f1": 0.7372975645938928,
          "f1_threshold": 223.08999633789062,
          "precision": 0.6936496859734822,
          "recall": 0.7868073878627968
        },
        "max": {
          "accuracy": 0.8840674733265781,
          "ap": 0.8051144963019613,
          "f1": 0.7374414779197772
        },
        "similarity": {
          "accuracy": 0.8840674733265781,
          "accuracy_threshold": 0.9207137823104858,
          "ap": 0.8051144963019613,
          "f1": 0.7371534195933457,
          "f1_threshold": 0.9082125425338745,
          "precision": 0.6915606936416185,
          "recall": 0.7891820580474934
        }
      }
    ]
  },
  "task_name": "TwitterSemEval2015"
}