{
  "dataset_revision": "7335288588f14e5a687d97fc979194c2abe6f4e7",
  "evaluation_time": 212.57563042640686,
  "kg_co2_emissions": 0.004380188482183933,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.7084548104956269,
          "accuracy_threshold": 0.9219532012939453,
          "ap": 0.7604086544447293,
          "f1": 0.733609958506224,
          "f1_threshold": 0.909163236618042,
          "precision": 0.6443148688046647,
          "recall": 0.8516377649325626
        },
        "dot": {
          "accuracy": 0.5908649173955296,
          "accuracy_threshold": 324.45526123046875,
          "ap": 0.6195641030735163,
          "f1": 0.6813793103448277,
          "f1_threshold": 304.9032897949219,
          "precision": 0.5306122448979592,
          "recall": 0.9518304431599229
        },
        "euclidean": {
          "accuracy": 0.6997084548104956,
          "accuracy_threshold": 7.2771501541137695,
          "ap": 0.7527764211977888,
          "f1": 0.726072607260726,
          "f1_threshold": 8.229911804199219,
          "precision": 0.6349206349206349,
          "recall": 0.8477842003853564
        },
        "hf_subset": "default",
        "languages": [
          "fas-Arab"
        ],
        "main_score": 0.7604086544447293,
        "manhattan": {
          "accuracy": 0.6997084548104956,
          "accuracy_threshold": 186.48233032226562,
          "ap": 0.7520673127006794,
          "f1": 0.7257525083612039,
          "f1_threshold": 207.35400390625,
          "precision": 0.6410635155096012,
          "recall": 0.8362235067437379
        },
        "max": {
          "accuracy": 0.7084548104956269,
          "ap": 0.7604086544447293,
          "f1": 0.733609958506224
        },
        "similarity": {
          "accuracy": 0.7084548104956269,
          "accuracy_threshold": 0.9219532012939453,
          "ap": 0.7604086544447293,
          "f1": 0.733609958506224,
          "f1_threshold": 0.909163236618042,
          "precision": 0.6443148688046647,
          "recall": 0.8516377649325626
        }
      }
    ]
  },
  "task_name": "FarsTail"
}