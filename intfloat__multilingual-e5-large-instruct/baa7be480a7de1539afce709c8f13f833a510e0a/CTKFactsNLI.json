{
  "dataset_revision": "387ae4582c8054cb52ef57ef0941f19bd8012abf",
  "evaluation_time": 233.94566941261292,
  "kg_co2_emissions": 0.004946001816208977,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.7306666666666667,
          "accuracy_threshold": 0.837274432182312,
          "ap": 0.8622623155045248,
          "f1": 0.8327974276527331,
          "f1_threshold": 0.8007029294967651,
          "precision": 0.7154696132596685,
          "recall": 0.9961538461538462
        },
        "dot": {
          "accuracy": 0.6986666666666667,
          "accuracy_threshold": 268.61279296875,
          "ap": 0.7614738123838667,
          "f1": 0.8214849921011058,
          "f1_threshold": 268.61279296875,
          "precision": 0.6970509383378016,
          "recall": 1.0
        },
        "euclidean": {
          "accuracy": 0.728,
          "accuracy_threshold": 11.788536071777344,
          "ap": 0.858362007970999,
          "f1": 0.8258064516129032,
          "f1_threshold": 12.835622787475586,
          "precision": 0.7111111111111111,
          "recall": 0.9846153846153847
        },
        "hf_subset": "default",
        "languages": [
          "ces-Latn"
        ],
        "main_score": 0.8622623155045248,
        "manhattan": {
          "accuracy": 0.7253333333333334,
          "accuracy_threshold": 300.62896728515625,
          "ap": 0.8584794319244803,
          "f1": 0.8281505728314238,
          "f1_threshold": 318.29815673828125,
          "precision": 0.7207977207977208,
          "recall": 0.9730769230769231
        },
        "max": {
          "accuracy": 0.7306666666666667,
          "ap": 0.8622623155045248,
          "f1": 0.8327974276527331
        },
        "similarity": {
          "accuracy": 0.7306666666666667,
          "accuracy_threshold": 0.837274432182312,
          "ap": 0.8622623155045248,
          "f1": 0.8327974276527331,
          "f1_threshold": 0.8007029294967651,
          "precision": 0.7154696132596685,
          "recall": 0.9961538461538462
        }
      }
    ],
    "validation": [
      {
        "cosine": {
          "accuracy": 0.6721311475409836,
          "accuracy_threshold": 0.8477356433868408,
          "ap": 0.7888547369791221,
          "f1": 0.7848101265822784,
          "f1_threshold": 0.8136670589447021,
          "precision": 0.657243816254417,
          "recall": 0.9738219895287958
        },
        "dot": {
          "accuracy": 0.639344262295082,
          "accuracy_threshold": 280.9153137207031,
          "ap": 0.6984078196207233,
          "f1": 0.7717842323651452,
          "f1_threshold": 280.9153137207031,
          "precision": 0.6391752577319587,
          "recall": 0.9738219895287958
        },
        "euclidean": {
          "accuracy": 0.6754098360655738,
          "accuracy_threshold": 10.723621368408203,
          "ap": 0.7886134074002941,
          "f1": 0.7796610169491526,
          "f1_threshold": 12.123062133789062,
          "precision": 0.6548042704626335,
          "recall": 0.9633507853403142
        },
        "hf_subset": "default",
        "languages": [
          "ces-Latn"
        ],
        "main_score": 0.7888547369791221,
        "manhattan": {
          "accuracy": 0.6688524590163935,
          "accuracy_threshold": 272.55804443359375,
          "ap": 0.7851198949918382,
          "f1": 0.7796610169491526,
          "f1_threshold": 306.90093994140625,
          "precision": 0.6548042704626335,
          "recall": 0.9633507853403142
        },
        "max": {
          "accuracy": 0.6754098360655738,
          "ap": 0.7888547369791221,
          "f1": 0.7848101265822784
        },
        "similarity": {
          "accuracy": 0.6721311475409836,
          "accuracy_threshold": 0.8477356433868408,
          "ap": 0.7888547369791221,
          "f1": 0.7848101265822784,
          "f1_threshold": 0.8136670589447021,
          "precision": 0.657243816254417,
          "recall": 0.9738219895287958
        }
      }
    ]
  },
  "task_name": "CTKFactsNLI"
}