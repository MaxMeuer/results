{
  "dataset_revision": "c1f9ee939b7d05667af864ee1cb066393154bf85",
  "evaluation_time": 5203.255213260651,
  "kg_co2_emissions": 0.10812533235873056,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "accuracy": 0.6021484375,
        "f1": 0.6010977477402546,
        "f1_weighted": 0.6010856460662031,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6021484375,
        "scores_per_experiment": [
          {
            "accuracy": 0.59521484375,
            "f1": 0.5933421973341851,
            "f1_weighted": 0.5933200126372883
          },
          {
            "accuracy": 0.61767578125,
            "f1": 0.6152363472637827,
            "f1_weighted": 0.6152279657816048
          },
          {
            "accuracy": 0.599609375,
            "f1": 0.5996801649551042,
            "f1_weighted": 0.599667540977176
          },
          {
            "accuracy": 0.60546875,
            "f1": 0.605958980459474,
            "f1_weighted": 0.6059533211364558
          },
          {
            "accuracy": 0.623046875,
            "f1": 0.6213505819425623,
            "f1_weighted": 0.6213478681768329
          },
          {
            "accuracy": 0.58984375,
            "f1": 0.5890776087918803,
            "f1_weighted": 0.5890476275607068
          },
          {
            "accuracy": 0.591796875,
            "f1": 0.5912803519383234,
            "f1_weighted": 0.591269665255592
          },
          {
            "accuracy": 0.60107421875,
            "f1": 0.5986844327706914,
            "f1_weighted": 0.598672828471144
          },
          {
            "accuracy": 0.60107421875,
            "f1": 0.5998834620142433,
            "f1_weighted": 0.5998811575433236
          },
          {
            "accuracy": 0.5966796875,
            "f1": 0.5964833499323,
            "f1_weighted": 0.5964684731219068
          }
        ]
      }
    ]
  },
  "task_name": "YelpReviewFullClassification"
}