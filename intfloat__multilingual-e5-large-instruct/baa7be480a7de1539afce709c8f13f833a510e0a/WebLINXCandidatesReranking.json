{
  "dataset_revision": "ed1c933c2b3617e5700d8a7ebe07f5975969a453",
  "evaluation_time": 2105.398950099945,
  "kg_co2_emissions": 1.2109873434922542,
  "mteb_version": "1.12.66",
  "scores": {
    "test_cat": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.062114655787408594,
        "map": 0.08028822795193595,
        "mrr": 0.062114655787408594,
        "nAUC_map_diff1": 0.02185378678223826,
        "nAUC_map_max": 0.06040467735746234,
        "nAUC_map_std": 0.2572580271979405,
        "nAUC_mrr_diff1": 0.01144876195578513,
        "nAUC_mrr_max": 0.04269864145056061,
        "nAUC_mrr_std": 0.2190129627005768
      }
    ],
    "test_geo": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.08406870342387023,
        "map": 0.10397573352631313,
        "mrr": 0.08406870342387023,
        "nAUC_map_diff1": 0.14733593547746737,
        "nAUC_map_max": -0.0674027513315895,
        "nAUC_map_std": 0.028571016425291758,
        "nAUC_mrr_diff1": 0.15167660948297668,
        "nAUC_mrr_max": -0.05571386404697181,
        "nAUC_mrr_std": 0.025415032184386607
      }
    ],
    "test_iid": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.08438086407488354,
        "map": 0.10248305510910562,
        "mrr": 0.08438086407488354,
        "nAUC_map_diff1": 0.12878387656654258,
        "nAUC_map_max": 0.0723956941567537,
        "nAUC_map_std": 0.18614083615493776,
        "nAUC_mrr_diff1": 0.12552861582121808,
        "nAUC_mrr_max": 0.07331007698473264,
        "nAUC_mrr_std": 0.17984640587875772
      }
    ],
    "test_vis": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.09487991874741593,
        "map": 0.11313641914331299,
        "mrr": 0.09487991874741593,
        "nAUC_map_diff1": 0.18026286287152934,
        "nAUC_map_max": 0.0535344921268622,
        "nAUC_map_std": 0.1346601540770047,
        "nAUC_mrr_diff1": 0.18564157645034904,
        "nAUC_mrr_max": 0.05947139571248723,
        "nAUC_mrr_std": 0.11643698580858522
      }
    ],
    "test_web": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.07615513550628054,
        "map": 0.0943159664975099,
        "mrr": 0.07615513550628054,
        "nAUC_map_diff1": 0.11567186822430392,
        "nAUC_map_max": 0.08757881320564899,
        "nAUC_map_std": 0.07133276057574378,
        "nAUC_mrr_diff1": 0.12298514227010804,
        "nAUC_mrr_max": 0.08674682253522749,
        "nAUC_mrr_std": 0.06765945526785667
      }
    ],
    "validation": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.11338500298915365,
        "map": 0.13125875145394364,
        "mrr": 0.11338500298915365,
        "nAUC_map_diff1": 0.16389558464819495,
        "nAUC_map_max": -0.00507614099423878,
        "nAUC_map_std": 0.015544888944390206,
        "nAUC_mrr_diff1": 0.16697501531695805,
        "nAUC_mrr_max": -0.010169582168171069,
        "nAUC_mrr_std": 0.010034735928273565
      }
    ]
  },
  "task_name": "WebLINXCandidatesReranking"
}