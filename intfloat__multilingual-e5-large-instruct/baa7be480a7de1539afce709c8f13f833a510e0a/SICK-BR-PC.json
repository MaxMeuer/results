{
  "dataset_revision": "0cdfb1d51ef339011c067688a3b75b82f927c097",
  "evaluation_time": 47.425488233566284,
  "kg_co2_emissions": 0.0009794284248440528,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.7955182072829131,
          "accuracy_threshold": 0.99505615234375,
          "ap": 0.22864911260494253,
          "f1": 0.3955696202531645,
          "f1_threshold": 0.8814955949783325,
          "precision": 0.25667351129363447,
          "recall": 0.8620689655172413
        },
        "dot": {
          "accuracy": 0.8109243697478992,
          "accuracy_threshold": 443.33697509765625,
          "ap": 0.37810454020399953,
          "f1": 0.42701525054466233,
          "f1_threshold": 407.8190612792969,
          "precision": 0.31210191082802546,
          "recall": 0.6758620689655173
        },
        "euclidean": {
          "accuracy": 0.7955182072829131,
          "accuracy_threshold": 1.9365200996398926,
          "ap": 0.2180941665443573,
          "f1": 0.3896499238964992,
          "f1_threshold": 10.644598960876465,
          "precision": 0.25,
          "recall": 0.8827586206896552
        },
        "hf_subset": "default",
        "languages": [
          "por-Latn"
        ],
        "main_score": 0.37810454020399953,
        "manhattan": {
          "accuracy": 0.7955182072829131,
          "accuracy_threshold": 48.5556755065918,
          "ap": 0.2189302468829674,
          "f1": 0.38772663877266383,
          "f1_threshold": 293.34954833984375,
          "precision": 0.243006993006993,
          "recall": 0.9586206896551724
        },
        "max": {
          "accuracy": 0.8109243697478992,
          "ap": 0.37810454020399953,
          "f1": 0.42701525054466233
        },
        "similarity": {
          "accuracy": 0.7955182072829131,
          "accuracy_threshold": 0.99505615234375,
          "ap": 0.22864911260494253,
          "f1": 0.3955696202531645,
          "f1_threshold": 0.8814955949783325,
          "precision": 0.25667351129363447,
          "recall": 0.8620689655172413
        }
      }
    ]
  },
  "task_name": "SICK-BR-PC"
}