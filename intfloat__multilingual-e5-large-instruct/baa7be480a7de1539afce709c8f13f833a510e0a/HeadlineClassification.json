{
  "dataset_revision": "2fe05ee6b5832cda29f2ef7aaad7b7fe6a3609eb",
  "evaluation_time": 109.34058713912964,
  "kg_co2_emissions": 0.0022654289273039458,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "accuracy": 0.83203125,
        "f1": 0.8323161769983727,
        "f1_weighted": 0.8323040959284755,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.83203125,
        "scores_per_experiment": [
          {
            "accuracy": 0.79736328125,
            "f1": 0.7992353395366263,
            "f1_weighted": 0.7992509090990209
          },
          {
            "accuracy": 0.84375,
            "f1": 0.8445865340538274,
            "f1_weighted": 0.8445734050030747
          },
          {
            "accuracy": 0.84619140625,
            "f1": 0.8463822104699047,
            "f1_weighted": 0.8463605520825168
          },
          {
            "accuracy": 0.83154296875,
            "f1": 0.831602801283351,
            "f1_weighted": 0.8315777649827764
          },
          {
            "accuracy": 0.84326171875,
            "f1": 0.8443597573997869,
            "f1_weighted": 0.8443455698393091
          },
          {
            "accuracy": 0.82177734375,
            "f1": 0.8220282170231212,
            "f1_weighted": 0.821997759814843
          },
          {
            "accuracy": 0.830078125,
            "f1": 0.8304057443421726,
            "f1_weighted": 0.8303968399551782
          },
          {
            "accuracy": 0.82666015625,
            "f1": 0.8257040521858142,
            "f1_weighted": 0.8256942059795125
          },
          {
            "accuracy": 0.82861328125,
            "f1": 0.8278064419087103,
            "f1_weighted": 0.8278057741435398
          },
          {
            "accuracy": 0.85107421875,
            "f1": 0.8510506717804129,
            "f1_weighted": 0.8510381783849847
          }
        ]
      }
    ]
  },
  "task_name": "HeadlineClassification"
}