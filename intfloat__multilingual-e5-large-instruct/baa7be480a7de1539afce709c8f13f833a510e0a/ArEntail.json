{
  "dataset_revision": "4da4316c6e3287746ab74ff67dd252ad128fceff",
  "evaluation_time": 60.425726890563965,
  "kg_co2_emissions": 0.001265449562094313,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.841,
          "accuracy_threshold": 0.9029719233512878,
          "ap": 0.9204426587521549,
          "f1": 0.8441558441558442,
          "f1_threshold": 0.8945757150650024,
          "precision": 0.78719723183391,
          "recall": 0.91
        },
        "dot": {
          "accuracy": 0.668,
          "accuracy_threshold": 342.13812255859375,
          "ap": 0.7016085068944693,
          "f1": 0.7009113504556753,
          "f1_threshold": 325.286865234375,
          "precision": 0.5983026874115983,
          "recall": 0.846
        },
        "euclidean": {
          "accuracy": 0.839,
          "accuracy_threshold": 8.219171524047852,
          "ap": 0.9183555860909787,
          "f1": 0.8388388388388388,
          "f1_threshold": 8.549856185913086,
          "precision": 0.8396793587174348,
          "recall": 0.838
        },
        "hf_subset": "default",
        "languages": [
          "ara-Arab"
        ],
        "main_score": 0.9204426587521549,
        "manhattan": {
          "accuracy": 0.84,
          "accuracy_threshold": 213.686279296875,
          "ap": 0.9167892446222994,
          "f1": 0.8385826771653544,
          "f1_threshold": 219.0496826171875,
          "precision": 0.8255813953488372,
          "recall": 0.852
        },
        "max": {
          "accuracy": 0.841,
          "ap": 0.9204426587521549,
          "f1": 0.8441558441558442
        },
        "similarity": {
          "accuracy": 0.841,
          "accuracy_threshold": 0.9029719233512878,
          "ap": 0.9204426587521549,
          "f1": 0.8441558441558442,
          "f1_threshold": 0.8945757150650024,
          "precision": 0.78719723183391,
          "recall": 0.91
        }
      }
    ]
  },
  "task_name": "ArEntail"
}