{
  "dataset_revision": "1beac1b941da76a9c51e3e5b39d230fde9a80983",
  "evaluation_time": 123.47890591621399,
  "kg_co2_emissions": 0.0025705946260193816,
  "mteb_version": "1.12.41",
  "scores": {
    "train": [
      {
        "accuracy": 0.602978515625,
        "f1": 0.6008888836220622,
        "f1_weighted": 0.6010892992824716,
        "hf_subset": "default",
        "languages": [
          "hin-Deva"
        ],
        "main_score": 0.6008888836220622,
        "scores_per_experiment": [
          {
            "accuracy": 0.6376953125,
            "f1": 0.644041681668858,
            "f1_weighted": 0.6382509482709546
          },
          {
            "accuracy": 0.66015625,
            "f1": 0.6593511765583687,
            "f1_weighted": 0.660722163258153
          },
          {
            "accuracy": 0.61962890625,
            "f1": 0.620644789592176,
            "f1_weighted": 0.6202897627266879
          },
          {
            "accuracy": 0.58203125,
            "f1": 0.5870861910093308,
            "f1_weighted": 0.5812283876963837
          },
          {
            "accuracy": 0.611328125,
            "f1": 0.6103660573174786,
            "f1_weighted": 0.612581216547144
          },
          {
            "accuracy": 0.5615234375,
            "f1": 0.5375525540777853,
            "f1_weighted": 0.5522452019737499
          },
          {
            "accuracy": 0.6025390625,
            "f1": 0.6100015876982279,
            "f1_weighted": 0.6010048637657638
          },
          {
            "accuracy": 0.5517578125,
            "f1": 0.5362168239068882,
            "f1_weighted": 0.5553869777406956
          },
          {
            "accuracy": 0.56982421875,
            "f1": 0.5538848693820793,
            "f1_weighted": 0.5580657601733248
          },
          {
            "accuracy": 0.63330078125,
            "f1": 0.6497431050094283,
            "f1_weighted": 0.6311177106718582
          }
        ]
      }
    ]
  },
  "task_name": "SentimentAnalysisHindi"
}