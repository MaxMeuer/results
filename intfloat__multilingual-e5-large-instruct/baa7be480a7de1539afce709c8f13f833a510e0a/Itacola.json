{
  "dataset_revision": "f8f98e5c4d3059cf1a00c8eb3d70aa271423f636",
  "evaluation_time": 55.68232011795044,
  "kg_co2_emissions": 0.0011425375355081236,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "accuracy": 0.480923076923077,
        "ap": 0.839550299115402,
        "ap_weighted": 0.839550299115402,
        "f1": 0.41507059125640156,
        "f1_weighted": 0.5402590683883628,
        "hf_subset": "default",
        "languages": [
          "ita-Latn"
        ],
        "main_score": 0.480923076923077,
        "scores_per_experiment": [
          {
            "accuracy": 0.6461538461538462,
            "ap": 0.8384946278798522,
            "ap_weighted": 0.8384946278798522,
            "f1": 0.4792319803815017,
            "f1_weighted": 0.680929234856418
          },
          {
            "accuracy": 0.41846153846153844,
            "ap": 0.8369703610243077,
            "ap_weighted": 0.8369703610243077,
            "f1": 0.3835588943366477,
            "f1_weighted": 0.4839039961957086
          },
          {
            "accuracy": 0.41333333333333333,
            "ap": 0.834170670466736,
            "ap_weighted": 0.834170670466736,
            "f1": 0.3778002891772728,
            "f1_weighted": 0.47951934691157483
          },
          {
            "accuracy": 0.41025641025641024,
            "ap": 0.8370768292155409,
            "ap_weighted": 0.8370768292155409,
            "f1": 0.37861952395466764,
            "f1_weighted": 0.47453681106040546
          },
          {
            "accuracy": 0.6502564102564102,
            "ap": 0.8461660943162823,
            "ap_weighted": 0.8461660943162823,
            "f1": 0.49877813272147004,
            "f1_weighted": 0.6872781534599125
          },
          {
            "accuracy": 0.38974358974358975,
            "ap": 0.8414770258773312,
            "ap_weighted": 0.8414770258773312,
            "f1": 0.36870266970349563,
            "f1_weighted": 0.44754701614587067
          },
          {
            "accuracy": 0.4574358974358974,
            "ap": 0.8320859160925832,
            "ap_weighted": 0.8320859160925832,
            "f1": 0.3998575785325236,
            "f1_weighted": 0.5270255875012333
          },
          {
            "accuracy": 0.5158974358974359,
            "ap": 0.8481555734497652,
            "ap_weighted": 0.8481555734497652,
            "f1": 0.44932655581402026,
            "f1_weighted": 0.5803081989279975
          },
          {
            "accuracy": 0.4523076923076923,
            "ap": 0.8353430725677472,
            "ap_weighted": 0.8353430725677472,
            "f1": 0.40113871635610765,
            "f1_weighted": 0.5208920741094654
          },
          {
            "accuracy": 0.4553846153846154,
            "ap": 0.8455628202638741,
            "ap_weighted": 0.8455628202638741,
            "f1": 0.4136915715863084,
            "f1_weighted": 0.5206502647150419
          }
        ]
      }
    ]
  },
  "task_name": "Itacola"
}