{
  "dataset_revision": "557bf94ac6177cc442f42d0b09b6e4b76e8f47c9",
  "evaluation_time": 225.80577087402344,
  "kg_co2_emissions": 0.004625053371063252,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "accuracy": 0.6698578199052132,
        "ap": 0.2461529228468548,
        "ap_weighted": 0.2461529228468548,
        "f1": 0.5841701334843001,
        "f1_weighted": 0.7087653520101735,
        "hf_subset": "default",
        "languages": [
          "ara-Arab"
        ],
        "main_score": 0.6698578199052132,
        "scores_per_experiment": [
          {
            "accuracy": 0.714218009478673,
            "ap": 0.2540485181239487,
            "ap_weighted": 0.2540485181239487,
            "f1": 0.6120011795520325,
            "f1_weighted": 0.7460250932139251
          },
          {
            "accuracy": 0.6289099526066351,
            "ap": 0.2271507239082719,
            "ap_weighted": 0.2271507239082719,
            "f1": 0.5547754258607155,
            "f1_weighted": 0.6770414281594447
          },
          {
            "accuracy": 0.6511848341232227,
            "ap": 0.2690310296586883,
            "ap_weighted": 0.2690310296586883,
            "f1": 0.5898415421721368,
            "f1_weighted": 0.6965908982634874
          },
          {
            "accuracy": 0.5744075829383887,
            "ap": 0.25015691594136635,
            "ap_weighted": 0.25015691594136635,
            "f1": 0.5364207625406014,
            "f1_weighted": 0.6257275257274519
          },
          {
            "accuracy": 0.6341232227488152,
            "ap": 0.23318516123572633,
            "ap_weighted": 0.23318516123572633,
            "f1": 0.5612336700336701,
            "f1_weighted": 0.6815861873075144
          },
          {
            "accuracy": 0.704739336492891,
            "ap": 0.23753779776214354,
            "ap_weighted": 0.23753779776214354,
            "f1": 0.5964544056073291,
            "f1_weighted": 0.7371356515885715
          },
          {
            "accuracy": 0.6815165876777252,
            "ap": 0.22346584728945923,
            "ap_weighted": 0.22346584728945923,
            "f1": 0.576157800152807,
            "f1_weighted": 0.7183721331237041
          },
          {
            "accuracy": 0.7090047393364929,
            "ap": 0.22679493756526953,
            "ap_weighted": 0.22679493756526953,
            "f1": 0.5896366741673255,
            "f1_weighted": 0.7385845234996784
          },
          {
            "accuracy": 0.7720379146919432,
            "ap": 0.28326746187537666,
            "ap_weighted": 0.28326746187537666,
            "f1": 0.65382829930033,
            "f1_weighted": 0.7899658936686111
          },
          {
            "accuracy": 0.628436018957346,
            "ap": 0.25689083510829785,
            "ap_weighted": 0.25689083510829785,
            "f1": 0.5713515754560531,
            "f1_weighted": 0.6766241855493466
          }
        ]
      }
    ]
  },
  "task_name": "TweetSarcasmClassification"
}