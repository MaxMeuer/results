{
  "dataset_revision": "9abd46cf7fc8b4c64290f26993c540b92aa145ac",
  "evaluation_time": 416.97210001945496,
  "kg_co2_emissions": 0.008804715091212561,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "accuracy": 0.971533203125,
        "f1": 0.9713275565609637,
        "f1_weighted": 0.9713315901457446,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.971533203125,
        "scores_per_experiment": [
          {
            "accuracy": 0.9833984375,
            "f1": 0.9833491945325898,
            "f1_weighted": 0.98335203310137
          },
          {
            "accuracy": 0.97900390625,
            "f1": 0.9788279918206628,
            "f1_weighted": 0.9788312044249172
          },
          {
            "accuracy": 0.96875,
            "f1": 0.9684460109055852,
            "f1_weighted": 0.9684567039537219
          },
          {
            "accuracy": 0.96630859375,
            "f1": 0.9659514729901364,
            "f1_weighted": 0.9659527237333564
          },
          {
            "accuracy": 0.9775390625,
            "f1": 0.977445549696675,
            "f1_weighted": 0.9774502949487364
          },
          {
            "accuracy": 0.9716796875,
            "f1": 0.9714147868631261,
            "f1_weighted": 0.9714047243450024
          },
          {
            "accuracy": 0.96875,
            "f1": 0.9685031138134731,
            "f1_weighted": 0.9685046979060001
          },
          {
            "accuracy": 0.95849609375,
            "f1": 0.9583550968460323,
            "f1_weighted": 0.958369985643819
          },
          {
            "accuracy": 0.974609375,
            "f1": 0.9743506876286306,
            "f1_weighted": 0.974364557327766
          },
          {
            "accuracy": 0.966796875,
            "f1": 0.9666316605127251,
            "f1_weighted": 0.9666289760727576
          }
        ]
      }
    ]
  },
  "task_name": "DBpediaClassification"
}