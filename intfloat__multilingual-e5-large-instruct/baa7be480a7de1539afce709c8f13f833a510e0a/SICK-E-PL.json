{
  "dataset_revision": "71bba34b0ece6c56dfcf46d9758a27f7a90f17e9",
  "evaluation_time": 168.28358936309814,
  "kg_co2_emissions": 0.0034922070781003805,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.8516102731349368,
          "accuracy_threshold": 0.9577335119247437,
          "ap": 0.8052513329387154,
          "f1": 0.739565367368058,
          "f1_threshold": 0.9491519927978516,
          "precision": 0.717056856187291,
          "recall": 0.7635327635327636
        },
        "dot": {
          "accuracy": 0.7441907867916836,
          "accuracy_threshold": 470.32452392578125,
          "ap": 0.5301531730823887,
          "f1": 0.5558194774346793,
          "f1_threshold": 420.08001708984375,
          "precision": 0.44150943396226416,
          "recall": 0.75
        },
        "euclidean": {
          "accuracy": 0.8463106400326131,
          "accuracy_threshold": 6.256237030029297,
          "ap": 0.7958555499165105,
          "f1": 0.7273358805140674,
          "f1_threshold": 6.787625312805176,
          "precision": 0.7098305084745763,
          "recall": 0.7457264957264957
        },
        "hf_subset": "default",
        "languages": [
          "pol-Latn"
        ],
        "main_score": 0.8052513329387154,
        "manhattan": {
          "accuracy": 0.8461068079902161,
          "accuracy_threshold": 158.2515869140625,
          "ap": 0.794378411405253,
          "f1": 0.7256637168141593,
          "f1_threshold": 175.37828063964844,
          "precision": 0.6949152542372882,
          "recall": 0.7592592592592593
        },
        "max": {
          "accuracy": 0.8516102731349368,
          "ap": 0.8052513329387154,
          "f1": 0.739565367368058
        },
        "similarity": {
          "accuracy": 0.8516102731349368,
          "accuracy_threshold": 0.9577335119247437,
          "ap": 0.8052513329387154,
          "f1": 0.739565367368058,
          "f1_threshold": 0.9491519927978516,
          "precision": 0.717056856187291,
          "recall": 0.7635327635327636
        }
      }
    ]
  },
  "task_name": "SICK-E-PL"
}