{
  "dataset_revision": "ef5c383d1b87eb8feccde3dfb7f95e42b1b050dd",
  "evaluation_time": 446.55503273010254,
  "kg_co2_emissions": 0.009443255732758234,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "cosine_pearson": 0.8377206894617504,
        "cosine_spearman": 0.8169802722041329,
        "euclidean_pearson": 0.8198344010988823,
        "euclidean_spearman": 0.8185117198223483,
        "hf_subset": "afr",
        "languages": [
          "afr-Latn"
        ],
        "main_score": 0.8169802722041329,
        "manhattan_pearson": 0.8174904564038882,
        "manhattan_spearman": 0.8158431882906974,
        "pearson": 0.8377206894617504,
        "spearman": 0.8169802722041329
      },
      {
        "cosine_pearson": 0.8178561236346722,
        "cosine_spearman": 0.811914861647479,
        "euclidean_pearson": 0.816919042891485,
        "euclidean_spearman": 0.8225164850571547,
        "hf_subset": "amh",
        "languages": [
          "amh-Ethi"
        ],
        "main_score": 0.811914861647479,
        "manhattan_pearson": 0.8160771777679756,
        "manhattan_spearman": 0.8212693058539557,
        "pearson": 0.8178561236346722,
        "spearman": 0.811914861647479
      },
      {
        "cosine_pearson": 0.6240018224085423,
        "cosine_spearman": 0.6280280850971894,
        "euclidean_pearson": 0.6597168436287232,
        "euclidean_spearman": 0.6684439896668474,
        "hf_subset": "arb",
        "languages": [
          "arb-Arab"
        ],
        "main_score": 0.6280280850971894,
        "manhattan_pearson": 0.6555929970623539,
        "manhattan_spearman": 0.6637034427165964,
        "pearson": 0.6240018224085423,
        "spearman": 0.6280280850971894
      },
      {
        "cosine_pearson": 0.5348721985998843,
        "cosine_spearman": 0.48062213101714696,
        "euclidean_pearson": 0.568568816619263,
        "euclidean_spearman": 0.5189608609116371,
        "hf_subset": "arq",
        "languages": [
          "arq-Arab"
        ],
        "main_score": 0.48062213101714696,
        "manhattan_pearson": 0.566943128942897,
        "manhattan_spearman": 0.5165679612494394,
        "pearson": 0.5348721985998843,
        "spearman": 0.48062213101714696
      },
      {
        "cosine_pearson": 0.479650809427562,
        "cosine_spearman": 0.4621682597571746,
        "euclidean_pearson": 0.45000494896099247,
        "euclidean_spearman": 0.4233283828025373,
        "hf_subset": "ary",
        "languages": [
          "ary-Arab"
        ],
        "main_score": 0.4621682597571746,
        "manhattan_pearson": 0.4467170431241144,
        "manhattan_spearman": 0.4210889436689629,
        "pearson": 0.479650809427562,
        "spearman": 0.4621682597571746
      },
      {
        "cosine_pearson": 0.8238085401789619,
        "cosine_spearman": 0.8087275891328206,
        "euclidean_pearson": 0.8397282351570922,
        "euclidean_spearman": 0.8359158452494954,
        "hf_subset": "eng",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.8087275891328206,
        "manhattan_pearson": 0.8393480097587819,
        "manhattan_spearman": 0.835430962984022,
        "pearson": 0.8238085401789619,
        "spearman": 0.8087275891328206
      },
      {
        "cosine_pearson": 0.6496173486164407,
        "cosine_spearman": 0.6381398965162454,
        "euclidean_pearson": 0.5975194352705578,
        "euclidean_spearman": 0.5839480217307715,
        "hf_subset": "hau",
        "languages": [
          "hau-Latn"
        ],
        "main_score": 0.6381398965162454,
        "manhattan_pearson": 0.5940627714803421,
        "manhattan_spearman": 0.5816779983910441,
        "pearson": 0.6496173486164407,
        "spearman": 0.6381398965162454
      },
      {
        "cosine_pearson": 0.801489208223645,
        "cosine_spearman": 0.8089115817376543,
        "euclidean_pearson": 0.7803162963861071,
        "euclidean_spearman": 0.8053711229634367,
        "hf_subset": "hin",
        "languages": [
          "hin-Deva"
        ],
        "main_score": 0.8089115817376543,
        "manhattan_pearson": 0.7784093307324829,
        "manhattan_spearman": 0.8028737367225485,
        "pearson": 0.801489208223645,
        "spearman": 0.8089115817376543
      },
      {
        "cosine_pearson": 0.49238778276373046,
        "cosine_spearman": 0.4930340664965469,
        "euclidean_pearson": 0.5213625820566371,
        "euclidean_spearman": 0.50348518785778,
        "hf_subset": "ind",
        "languages": [
          "ind-Latn"
        ],
        "main_score": 0.4930340664965469,
        "manhattan_pearson": 0.5222786797978297,
        "manhattan_spearman": 0.5066687443880484,
        "pearson": 0.49238778276373046,
        "spearman": 0.4930340664965469
      },
      {
        "cosine_pearson": 0.6458435845115752,
        "cosine_spearman": 0.6644908492500149,
        "euclidean_pearson": 0.6060329968931575,
        "euclidean_spearman": 0.6032332980246051,
        "hf_subset": "kin",
        "languages": [
          "kin-Latn"
        ],
        "main_score": 0.6644908492500149,
        "manhattan_pearson": 0.6043932073353624,
        "manhattan_spearman": 0.6016911713733173,
        "pearson": 0.6458435845115752,
        "spearman": 0.6644908492500149
      },
      {
        "cosine_pearson": 0.8496174450493333,
        "cosine_spearman": 0.836417143576808,
        "euclidean_pearson": 0.816507802282275,
        "euclidean_spearman": 0.8039512305579856,
        "hf_subset": "mar",
        "languages": [
          "mar-Deva"
        ],
        "main_score": 0.836417143576808,
        "manhattan_pearson": 0.81668220417138,
        "manhattan_spearman": 0.8029813879268143,
        "pearson": 0.8496174450493333,
        "spearman": 0.836417143576808
      },
      {
        "cosine_pearson": 0.8418643533229125,
        "cosine_spearman": 0.8257264112143824,
        "euclidean_pearson": 0.8114229218594952,
        "euclidean_spearman": 0.8094367209875886,
        "hf_subset": "tel",
        "languages": [
          "tel-Telu"
        ],
        "main_score": 0.8257264112143824,
        "manhattan_pearson": 0.8096084715969633,
        "manhattan_spearman": 0.8066850051850285,
        "pearson": 0.8418643533229125,
        "spearman": 0.8257264112143824
      }
    ]
  },
  "task_name": "SemRel24STS"
}