{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "evaluation_time": 444.1776371002197,
  "kg_co2_emissions": 0.009206455354650932,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "accuracy": 0.383154296875,
        "f1": 0.3737902433009545,
        "f1_weighted": 0.3737792171188016,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ],
        "main_score": 0.383154296875,
        "scores_per_experiment": [
          {
            "accuracy": 0.3955078125,
            "f1": 0.38093033836815493,
            "f1_weighted": 0.3808980861720808
          },
          {
            "accuracy": 0.37890625,
            "f1": 0.3611019546835628,
            "f1_weighted": 0.36110510903144044
          },
          {
            "accuracy": 0.38525390625,
            "f1": 0.37941454526270296,
            "f1_weighted": 0.3794436450457319
          },
          {
            "accuracy": 0.3916015625,
            "f1": 0.38885216958095914,
            "f1_weighted": 0.3888521094752168
          },
          {
            "accuracy": 0.39990234375,
            "f1": 0.37666833677632294,
            "f1_weighted": 0.37662510192267656
          },
          {
            "accuracy": 0.37939453125,
            "f1": 0.36043555161312874,
            "f1_weighted": 0.3604086219753525
          },
          {
            "accuracy": 0.3486328125,
            "f1": 0.3524113216828961,
            "f1_weighted": 0.3523541296062369
          },
          {
            "accuracy": 0.40771484375,
            "f1": 0.40329917794712766,
            "f1_weighted": 0.403304851924165
          },
          {
            "accuracy": 0.4013671875,
            "f1": 0.3945576718822603,
            "f1_weighted": 0.3945318367850168
          },
          {
            "accuracy": 0.34326171875,
            "f1": 0.34023136521242936,
            "f1_weighted": 0.34026867925009757
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.382958984375,
        "f1": 0.37359352036696614,
        "f1_weighted": 0.3735772773951741,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ],
        "main_score": 0.382958984375,
        "scores_per_experiment": [
          {
            "accuracy": 0.396484375,
            "f1": 0.3852705440223473,
            "f1_weighted": 0.38523569769284227
          },
          {
            "accuracy": 0.373046875,
            "f1": 0.3527281373006323,
            "f1_weighted": 0.35274085665012034
          },
          {
            "accuracy": 0.3955078125,
            "f1": 0.3890338853969765,
            "f1_weighted": 0.3890417916488284
          },
          {
            "accuracy": 0.39599609375,
            "f1": 0.39060240237866906,
            "f1_weighted": 0.3905838205746724
          },
          {
            "accuracy": 0.39697265625,
            "f1": 0.3759384497263743,
            "f1_weighted": 0.3759027227532442
          },
          {
            "accuracy": 0.3779296875,
            "f1": 0.3625396067038448,
            "f1_weighted": 0.3625157166244809
          },
          {
            "accuracy": 0.3330078125,
            "f1": 0.3367965661525568,
            "f1_weighted": 0.33673466018628145
          },
          {
            "accuracy": 0.41455078125,
            "f1": 0.4104898730012584,
            "f1_weighted": 0.41048158252004974
          },
          {
            "accuracy": 0.37890625,
            "f1": 0.37152785302632163,
            "f1_weighted": 0.3714944609430371
          },
          {
            "accuracy": 0.3671875,
            "f1": 0.3610078859606801,
            "f1_weighted": 0.36104146435818385
          }
        ]
      }
    ]
  },
  "task_name": "FilipinoShopeeReviewsClassification"
}