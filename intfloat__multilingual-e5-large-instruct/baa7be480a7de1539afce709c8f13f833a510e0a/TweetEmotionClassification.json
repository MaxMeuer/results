{
  "dataset_revision": "0ded8ff72cc68cbb7bb5c01b0a9157982b73ddaf",
  "evaluation_time": 144.2693691253662,
  "kg_co2_emissions": 0.00297995056818356,
  "mteb_version": "1.12.41",
  "scores": {
    "train": [
      {
        "accuracy": 0.5818359375,
        "f1": 0.5704413569446525,
        "f1_weighted": 0.5714467715447608,
        "hf_subset": "default",
        "languages": [
          "ara-Arab"
        ],
        "main_score": 0.5818359375,
        "scores_per_experiment": [
          {
            "accuracy": 0.583984375,
            "f1": 0.5676923141613388,
            "f1_weighted": 0.5683296637585304
          },
          {
            "accuracy": 0.59912109375,
            "f1": 0.5831332009384512,
            "f1_weighted": 0.5827003360578288
          },
          {
            "accuracy": 0.56982421875,
            "f1": 0.5643266371162539,
            "f1_weighted": 0.5646027607282641
          },
          {
            "accuracy": 0.58349609375,
            "f1": 0.5840738877433305,
            "f1_weighted": 0.5818393919136371
          },
          {
            "accuracy": 0.56591796875,
            "f1": 0.5552169178425247,
            "f1_weighted": 0.5571683218951029
          },
          {
            "accuracy": 0.61572265625,
            "f1": 0.6006833940774201,
            "f1_weighted": 0.6022359212065349
          },
          {
            "accuracy": 0.54541015625,
            "f1": 0.5352307394696663,
            "f1_weighted": 0.5340389729994893
          },
          {
            "accuracy": 0.583984375,
            "f1": 0.564631601977466,
            "f1_weighted": 0.5683783912879222
          },
          {
            "accuracy": 0.58349609375,
            "f1": 0.5789368579117622,
            "f1_weighted": 0.5812411268371106
          },
          {
            "accuracy": 0.58740234375,
            "f1": 0.570488018208311,
            "f1_weighted": 0.5739328287631866
          }
        ]
      }
    ]
  },
  "task_name": "TweetEmotionClassification"
}