{
  "dataset_revision": "3c976110fc13596004dc36279fc4c453ff2c18aa",
  "evaluation_time": 201.32267689704895,
  "kg_co2_emissions": 0.0042215983376144494,
  "mteb_version": "1.12.41",
  "scores": {
    "test_expert": [
      {
        "cosine": {
          "accuracy": 0.5720588235294117,
          "accuracy_threshold": 0.8829901218414307,
          "ap": 0.5635169794531469,
          "f1": 0.6804461942257217,
          "f1_threshold": 0.8092820644378662,
          "precision": 0.5166915794718485,
          "recall": 0.9961575408261287
        },
        "dot": {
          "accuracy": 0.532843137254902,
          "accuracy_threshold": 345.66693115234375,
          "ap": 0.542324840812722,
          "f1": 0.6764898493778652,
          "f1_threshold": 293.25299072265625,
          "precision": 0.5131644311972181,
          "recall": 0.9923150816522575
        },
        "euclidean": {
          "accuracy": 0.5715686274509804,
          "accuracy_threshold": 9.859917640686035,
          "ap": 0.5617014150030403,
          "f1": 0.6777996070726915,
          "f1_threshold": 12.770276069641113,
          "precision": 0.5141579731743666,
          "recall": 0.9942363112391931
        },
        "hf_subset": "default",
        "languages": [
          "ind-Latn"
        ],
        "main_score": 0.5635169794531469,
        "manhattan": {
          "accuracy": 0.5700980392156862,
          "accuracy_threshold": 251.44558715820312,
          "ap": 0.5607465051117835,
          "f1": 0.6775350505379849,
          "f1_threshold": 338.0154113769531,
          "precision": 0.5128331688055281,
          "recall": 0.9980787704130644
        },
        "max": {
          "accuracy": 0.5720588235294117,
          "ap": 0.5635169794531469,
          "f1": 0.6804461942257217
        },
        "similarity": {
          "accuracy": 0.5720588235294117,
          "accuracy_threshold": 0.8829901218414307,
          "ap": 0.5635169794531469,
          "f1": 0.6804461942257217,
          "f1_threshold": 0.8092820644378662,
          "precision": 0.5166915794718485,
          "recall": 0.9961575408261287
        }
      }
    ]
  },
  "task_name": "indonli"
}