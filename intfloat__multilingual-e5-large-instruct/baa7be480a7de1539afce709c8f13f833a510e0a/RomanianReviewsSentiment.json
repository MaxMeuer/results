{
  "dataset_revision": "358bcc95aeddd5d07a4524ee416f03d993099b23",
  "evaluation_time": 569.5912716388702,
  "kg_co2_emissions": 0.012008584464386896,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "accuracy": 0.585986328125,
        "f1": 0.5110526752965996,
        "f1_weighted": 0.6240073387978423,
        "hf_subset": "default",
        "languages": [
          "ron-Latn"
        ],
        "main_score": 0.585986328125,
        "scores_per_experiment": [
          {
            "accuracy": 0.6181640625,
            "f1": 0.5384717216886648,
            "f1_weighted": 0.6512323791650445
          },
          {
            "accuracy": 0.5068359375,
            "f1": 0.46919778447049987,
            "f1_weighted": 0.5550323734752723
          },
          {
            "accuracy": 0.54931640625,
            "f1": 0.48673752054190483,
            "f1_weighted": 0.5943026728616768
          },
          {
            "accuracy": 0.63525390625,
            "f1": 0.5489273558300323,
            "f1_weighted": 0.6723079504897955
          },
          {
            "accuracy": 0.57080078125,
            "f1": 0.5022756134473647,
            "f1_weighted": 0.609031595445389
          },
          {
            "accuracy": 0.5810546875,
            "f1": 0.49470434893495974,
            "f1_weighted": 0.6103724818942537
          },
          {
            "accuracy": 0.64892578125,
            "f1": 0.5453216638482549,
            "f1_weighted": 0.672834510747001
          },
          {
            "accuracy": 0.6337890625,
            "f1": 0.5291300872713026,
            "f1_weighted": 0.6712766031902132
          },
          {
            "accuracy": 0.61669921875,
            "f1": 0.5386253543943019,
            "f1_weighted": 0.6533586168100506
          },
          {
            "accuracy": 0.4990234375,
            "f1": 0.4571353025387104,
            "f1_weighted": 0.5503242038997272
          }
        ]
      }
    ]
  },
  "task_name": "RomanianReviewsSentiment"
}