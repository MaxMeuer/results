{
  "dataset_revision": "601651fdc45ef243751676e62dd7a19f491c0285",
  "evaluation_time": 184.10000133514404,
  "kg_co2_emissions": 0.0038142117722245058,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "accuracy": 0.645849609375,
        "ap": 0.5961787364539598,
        "ap_weighted": 0.5961787364539598,
        "f1": 0.6433810834261744,
        "f1_weighted": 0.6433810834261744,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.645849609375,
        "scores_per_experiment": [
          {
            "accuracy": 0.6650390625,
            "ap": 0.6084893651070764,
            "ap_weighted": 0.6084893651070764,
            "f1": 0.6648392911500633,
            "f1_weighted": 0.6648392911500633
          },
          {
            "accuracy": 0.69140625,
            "ap": 0.6365698188997821,
            "ap_weighted": 0.6365698188997821,
            "f1": 0.6905773465791136,
            "f1_weighted": 0.6905773465791136
          },
          {
            "accuracy": 0.685546875,
            "ap": 0.6328346946022727,
            "ap_weighted": 0.6328346946022727,
            "f1": 0.6839845427221984,
            "f1_weighted": 0.6839845427221984
          },
          {
            "accuracy": 0.56494140625,
            "ap": 0.5365181291002812,
            "ap_weighted": 0.5365181291002812,
            "f1": 0.5647495321953366,
            "f1_weighted": 0.5647495321953366
          },
          {
            "accuracy": 0.65087890625,
            "ap": 0.5991052308058376,
            "ap_weighted": 0.5991052308058376,
            "f1": 0.6507522569138446,
            "f1_weighted": 0.6507522569138446
          },
          {
            "accuracy": 0.578125,
            "ap": 0.544892723880597,
            "ap_weighted": 0.544892723880597,
            "f1": 0.5778931297709924,
            "f1_weighted": 0.5778931297709924
          },
          {
            "accuracy": 0.70263671875,
            "ap": 0.6382991149956024,
            "ap_weighted": 0.6382991149956024,
            "f1": 0.7017286714089443,
            "f1_weighted": 0.7017286714089443
          },
          {
            "accuracy": 0.634765625,
            "ap": 0.5802442621887967,
            "ap_weighted": 0.5802442621887967,
            "f1": 0.6185706958284158,
            "f1_weighted": 0.6185706958284158
          },
          {
            "accuracy": 0.6552734375,
            "ap": 0.5991798920157068,
            "ap_weighted": 0.5991798920157068,
            "f1": 0.6540457766867174,
            "f1_weighted": 0.6540457766867174
          },
          {
            "accuracy": 0.6298828125,
            "ap": 0.5856541329436451,
            "ap_weighted": 0.5856541329436451,
            "f1": 0.6266695910061171,
            "f1_weighted": 0.6266695910061171
          }
        ]
      }
    ]
  },
  "task_name": "InappropriatenessClassification"
}