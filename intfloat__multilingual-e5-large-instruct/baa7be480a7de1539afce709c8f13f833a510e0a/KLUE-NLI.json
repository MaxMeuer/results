{
  "dataset_revision": "349481ec73fff722f88e0453ca05c77a447d967c",
  "evaluation_time": 131.2500069141388,
  "kg_co2_emissions": 0.002734614393899104,
  "mteb_version": "1.12.41",
  "scores": {
    "validation": [
      {
        "cosine": {
          "accuracy": 0.6405,
          "accuracy_threshold": 0.9469416737556458,
          "ap": 0.7104184727731055,
          "f1": 0.6845124282982792,
          "f1_threshold": 0.8890143632888794,
          "precision": 0.5541795665634675,
          "recall": 0.895
        },
        "dot": {
          "accuracy": 0.5705,
          "accuracy_threshold": 402.07318115234375,
          "ap": 0.5959663871124496,
          "f1": 0.6703183841150292,
          "f1_threshold": 326.95703125,
          "precision": 0.5096304008328996,
          "recall": 0.979
        },
        "euclidean": {
          "accuracy": 0.636,
          "accuracy_threshold": 6.374285697937012,
          "ap": 0.7044096639835191,
          "f1": 0.6844217438890916,
          "f1_threshold": 10.121992111206055,
          "precision": 0.5387708213670305,
          "recall": 0.938
        },
        "hf_subset": "default",
        "languages": [
          "kor-Hang"
        ],
        "main_score": 0.7104184727731055,
        "manhattan": {
          "accuracy": 0.634,
          "accuracy_threshold": 160.3389892578125,
          "ap": 0.7028921802383653,
          "f1": 0.6849816849816849,
          "f1_threshold": 255.2874755859375,
          "precision": 0.5404624277456648,
          "recall": 0.935
        },
        "max": {
          "accuracy": 0.6405,
          "ap": 0.7104184727731055,
          "f1": 0.6849816849816849
        },
        "similarity": {
          "accuracy": 0.6405,
          "accuracy_threshold": 0.9469416737556458,
          "ap": 0.7104184727731055,
          "f1": 0.6845124282982792,
          "f1_threshold": 0.8890143632888794,
          "precision": 0.5541795665634675,
          "recall": 0.895
        }
      }
    ]
  },
  "task_name": "KLUE-NLI"
}