{
  "dataset_revision": "3c976110fc13596004dc36279fc4c453ff2c18aa",
  "evaluation_time": 1.5368905067443848,
  "kg_co2_emissions": 6.047934924689845e-05,
  "mteb_version": "1.12.41",
  "scores": {
    "test_expert": [
      {
        "cosine": {
          "accuracy": 0.5852941176470589,
          "accuracy_threshold": 0.6042376756668091,
          "ap": 0.5758294001852874,
          "f1": 0.6838311019567457,
          "f1_threshold": 0.4158455431461334,
          "precision": 0.532051282051282,
          "recall": 0.9567723342939481
        },
        "dot": {
          "accuracy": 0.5651960784313725,
          "accuracy_threshold": 4.403141021728516,
          "ap": 0.5796747872684556,
          "f1": 0.6823529411764705,
          "f1_threshold": 2.719379186630249,
          "precision": 0.5332612222823148,
          "recall": 0.94716618635927
        },
        "euclidean": {
          "accuracy": 0.5897058823529412,
          "accuracy_threshold": 2.3666300773620605,
          "ap": 0.5736806554527558,
          "f1": 0.6800262812089357,
          "f1_threshold": 3.1763057708740234,
          "precision": 0.5167249126310535,
          "recall": 0.9942363112391931
        },
        "hf_subset": "default",
        "languages": [
          "ind-Latn"
        ],
        "main_score": 0.5796747872684556,
        "manhattan": {
          "accuracy": 0.592156862745098,
          "accuracy_threshold": 50.26770782470703,
          "ap": 0.5751106029282782,
          "f1": 0.67998667998668,
          "f1_threshold": 64.37907409667969,
          "precision": 0.5203873598369011,
          "recall": 0.9807877041306436
        },
        "max": {
          "accuracy": 0.592156862745098,
          "ap": 0.5796747872684556,
          "f1": 0.6838311019567457
        },
        "similarity": {
          "accuracy": 0.5852941176470589,
          "accuracy_threshold": 0.6042376756668091,
          "ap": 0.5758294001852874,
          "f1": 0.6838311019567457,
          "f1_threshold": 0.41584545373916626,
          "precision": 0.532051282051282,
          "recall": 0.9567723342939481
        }
      }
    ]
  },
  "task_name": "indonli"
}