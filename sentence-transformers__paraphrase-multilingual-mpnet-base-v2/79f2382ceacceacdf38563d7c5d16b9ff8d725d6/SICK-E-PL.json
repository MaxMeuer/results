{
  "dataset_revision": "71bba34b0ece6c56dfcf46d9758a27f7a90f17e9",
  "evaluation_time": 2.3301963806152344,
  "kg_co2_emissions": 8.495500889042564e-05,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.8355075417855687,
          "accuracy_threshold": 0.8926613330841064,
          "ap": 0.7722371719275988,
          "f1": 0.7065330107155202,
          "f1_threshold": 0.8690899014472961,
          "precision": 0.6863666890530558,
          "recall": 0.7279202279202279
        },
        "dot": {
          "accuracy": 0.7692621280065226,
          "accuracy_threshold": 6.000165939331055,
          "ap": 0.6006816151879456,
          "f1": 0.587117839121401,
          "f1_threshold": 4.965999603271484,
          "precision": 0.5033078880407125,
          "recall": 0.7044159544159544
        },
        "euclidean": {
          "accuracy": 0.8320423970648186,
          "accuracy_threshold": 1.0302103757858276,
          "ap": 0.7662532410583442,
          "f1": 0.7012546625974907,
          "f1_threshold": 1.32026207447052,
          "precision": 0.6692556634304208,
          "recall": 0.7364672364672364
        },
        "hf_subset": "default",
        "languages": [
          "pol-Latn"
        ],
        "main_score": 0.7722373119715319,
        "manhattan": {
          "accuracy": 0.8316347329800244,
          "accuracy_threshold": 22.401065826416016,
          "ap": 0.7664869944163805,
          "f1": 0.7039835164835164,
          "f1_threshold": 28.024642944335938,
          "precision": 0.6797082228116711,
          "recall": 0.73005698005698
        },
        "max": {
          "accuracy": 0.8355075417855687,
          "ap": 0.7722373119715319,
          "f1": 0.7065330107155202
        },
        "similarity": {
          "accuracy": 0.8355075417855687,
          "accuracy_threshold": 0.8926613926887512,
          "ap": 0.7722373119715319,
          "f1": 0.7065330107155202,
          "f1_threshold": 0.8690898418426514,
          "precision": 0.6863666890530558,
          "recall": 0.7279202279202279
        }
      }
    ]
  },
  "task_name": "SICK-E-PL"
}