{
  "dataset_revision": "8b6510b0b1fa4e4c4f879467980e9be563ec1cdf",
  "evaluation_time": 21.925496339797974,
  "kg_co2_emissions": 0.000980730889295905,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.8851825978965343,
          "accuracy_threshold": 0.7191818952560425,
          "ap": 0.8513840640298656,
          "f1": 0.7718896561911444,
          "f1_threshold": 0.6758555173873901,
          "precision": 0.741679085941381,
          "recall": 0.8046658453957499
        },
        "dot": {
          "accuracy": 0.8686304187526681,
          "accuracy_threshold": 6.133995056152344,
          "ap": 0.8116933384901694,
          "f1": 0.7442820047664782,
          "f1_threshold": 5.590068817138672,
          "precision": 0.691702479338843,
          "recall": 0.8055127810286419
        },
        "euclidean": {
          "accuracy": 0.8859781891566733,
          "accuracy_threshold": 2.2056429386138916,
          "ap": 0.8532492639242918,
          "f1": 0.7763961280714817,
          "f1_threshold": 2.3362722396850586,
          "precision": 0.7516580161476355,
          "recall": 0.8028179858330767
        },
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.8532492639242918,
        "manhattan": {
          "accuracy": 0.8858229518376217,
          "accuracy_threshold": 46.42976760864258,
          "ap": 0.8529817847757581,
          "f1": 0.7761955838295669,
          "f1_threshold": 50.18716812133789,
          "precision": 0.746885899352267,
          "recall": 0.8078995996304281
        },
        "max": {
          "accuracy": 0.8859781891566733,
          "ap": 0.8532492639242918,
          "f1": 0.7763961280714817
        },
        "similarity": {
          "accuracy": 0.8851825978965343,
          "accuracy_threshold": 0.7191819548606873,
          "ap": 0.8513835307375165,
          "f1": 0.7718896561911444,
          "f1_threshold": 0.6758556365966797,
          "precision": 0.741679085941381,
          "recall": 0.8046658453957499
        }
      }
    ]
  },
  "task_name": "TwitterURLCorpus"
}