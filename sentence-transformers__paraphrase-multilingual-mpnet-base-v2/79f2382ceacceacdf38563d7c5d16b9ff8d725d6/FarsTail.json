{
  "dataset_revision": "7335288588f14e5a687d97fc979194c2abe6f4e7",
  "evaluation_time": 1.0994269847869873,
  "kg_co2_emissions": 4.5777109988724494e-05,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.6316812439261419,
          "accuracy_threshold": 0.8228205442428589,
          "ap": 0.6632279644293401,
          "f1": 0.6859205776173285,
          "f1_threshold": 0.664664626121521,
          "precision": 0.5484988452655889,
          "recall": 0.9152215799614644
        },
        "dot": {
          "accuracy": 0.5636540330417882,
          "accuracy_threshold": 3.811570167541504,
          "ap": 0.5858929912735984,
          "f1": 0.6714100905562743,
          "f1_threshold": 1.1635236740112305,
          "precision": 0.5053554040895814,
          "recall": 1.0
        },
        "euclidean": {
          "accuracy": 0.6229348882410107,
          "accuracy_threshold": 1.4606654644012451,
          "ap": 0.626995697629206,
          "f1": 0.6916426512968299,
          "f1_threshold": 1.933762550354004,
          "precision": 0.5523590333716916,
          "recall": 0.9248554913294798
        },
        "hf_subset": "default",
        "languages": [
          "fas-Arab"
        ],
        "main_score": 0.6632279644293401,
        "manhattan": {
          "accuracy": 0.6209912536443148,
          "accuracy_threshold": 31.118915557861328,
          "ap": 0.6251432386240023,
          "f1": 0.6912509038322486,
          "f1_threshold": 41.14084243774414,
          "precision": 0.5532407407407407,
          "recall": 0.9210019267822736
        },
        "max": {
          "accuracy": 0.6316812439261419,
          "ap": 0.6632279644293401,
          "f1": 0.6916426512968299
        },
        "similarity": {
          "accuracy": 0.6316812439261419,
          "accuracy_threshold": 0.8228205442428589,
          "ap": 0.6632279644293401,
          "f1": 0.6859205776173285,
          "f1_threshold": 0.6646646857261658,
          "precision": 0.5484988452655889,
          "recall": 0.9152215799614644
        }
      }
    ]
  },
  "task_name": "FarsTail"
}