{
  "dataset_revision": "7b58f24536063837d644aab9a023c62199b2a612",
  "evaluation_time": 0.5722568035125732,
  "kg_co2_emissions": 2.0834554405477985e-05,
  "mteb_version": "1.12.41",
  "scores": {
    "dev": [
      {
        "cosine": {
          "accuracy": 0.6286644951140065,
          "accuracy_threshold": 0.6105841398239136,
          "ap": 0.6456662563306175,
          "f1": 0.6935483870967742,
          "f1_threshold": 0.5402176380157471,
          "precision": 0.589041095890411,
          "recall": 0.8431372549019608
        },
        "dot": {
          "accuracy": 0.6254071661237784,
          "accuracy_threshold": 3.42142653465271,
          "ap": 0.6305309702444627,
          "f1": 0.6811989100817439,
          "f1_threshold": 2.819490671157837,
          "precision": 0.5841121495327103,
          "recall": 0.8169934640522876
        },
        "euclidean": {
          "accuracy": 0.6026058631921825,
          "accuracy_threshold": 1.9719455242156982,
          "ap": 0.6156720506879636,
          "f1": 0.684863523573201,
          "f1_threshold": 2.387511730194092,
          "precision": 0.552,
          "recall": 0.9019607843137255
        },
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.6456662563306175,
        "manhattan": {
          "accuracy": 0.6058631921824105,
          "accuracy_threshold": 42.25822830200195,
          "ap": 0.6142907551186613,
          "f1": 0.6763285024154588,
          "f1_threshold": 53.34039306640625,
          "precision": 0.5363984674329502,
          "recall": 0.9150326797385621
        },
        "max": {
          "accuracy": 0.6286644951140065,
          "ap": 0.6456662563306175,
          "f1": 0.6935483870967742
        },
        "similarity": {
          "accuracy": 0.6286644951140065,
          "accuracy_threshold": 0.6105841398239136,
          "ap": 0.6456662563306175,
          "f1": 0.6935483870967742,
          "f1_threshold": 0.5402176380157471,
          "precision": 0.589041095890411,
          "recall": 0.8431372549019608
        }
      }
    ]
  },
  "task_name": "TERRa"
}