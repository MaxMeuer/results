{
  "dataset_revision": "0a3d4aa409b22f80eb22cbf59b492637637b536d",
  "evaluation_time": 0.7324635982513428,
  "kg_co2_emissions": 2.690477280220312e-05,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.891,
          "accuracy_threshold": 0.9397704601287842,
          "ap": 0.757685847575087,
          "f1": 0.68,
          "f1_threshold": 0.9337705969810486,
          "precision": 0.74375,
          "recall": 0.6263157894736842
        },
        "dot": {
          "accuracy": 0.848,
          "accuracy_threshold": 7.091057300567627,
          "ap": 0.588454418743574,
          "f1": 0.5761124121779859,
          "f1_threshold": 6.16477108001709,
          "precision": 0.5189873417721519,
          "recall": 0.6473684210526316
        },
        "euclidean": {
          "accuracy": 0.889,
          "accuracy_threshold": 0.672274649143219,
          "ap": 0.7531891900352545,
          "f1": 0.6685082872928177,
          "f1_threshold": 1.0117712020874023,
          "precision": 0.7034883720930233,
          "recall": 0.6368421052631579
        },
        "hf_subset": "default",
        "languages": [
          "pol-Latn"
        ],
        "main_score": 0.757685847575087,
        "manhattan": {
          "accuracy": 0.889,
          "accuracy_threshold": 17.4986572265625,
          "ap": 0.7516708388356144,
          "f1": 0.6702412868632708,
          "f1_threshold": 23.04237937927246,
          "precision": 0.6830601092896175,
          "recall": 0.6578947368421053
        },
        "max": {
          "accuracy": 0.891,
          "ap": 0.757685847575087,
          "f1": 0.68
        },
        "similarity": {
          "accuracy": 0.891,
          "accuracy_threshold": 0.9397704601287842,
          "ap": 0.757685847575087,
          "f1": 0.68,
          "f1_threshold": 0.9337705373764038,
          "precision": 0.74375,
          "recall": 0.6263157894736842
        }
      }
    ]
  },
  "task_name": "CDSC-E"
}