{
  "dataset_revision": "d66bd1f72af766a5cc4b0ca5e00c162f89e8cc46",
  "evaluation_time": 37.398226261138916,
  "kg_co2_emissions": 0.0010571687102120693,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.9968910891089109,
          "accuracy_threshold": 0.8149476051330566,
          "ap": 0.9055298495473484,
          "f1": 0.8382804503582394,
          "f1_threshold": 0.8073698282241821,
          "precision": 0.8584905660377359,
          "recall": 0.819
        },
        "dot": {
          "accuracy": 0.9952376237623762,
          "accuracy_threshold": 6.783668518066406,
          "ap": 0.8126415446043816,
          "f1": 0.7614983404457087,
          "f1_threshold": 6.505136966705322,
          "precision": 0.7240757439134355,
          "recall": 0.803
        },
        "euclidean": {
          "accuracy": 0.997019801980198,
          "accuracy_threshold": 1.7289166450500488,
          "ap": 0.9109871555441386,
          "f1": 0.8483950617283951,
          "f1_threshold": 1.870920181274414,
          "precision": 0.8380487804878048,
          "recall": 0.859
        },
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.9109871555441386,
        "manhattan": {
          "accuracy": 0.9969009900990099,
          "accuracy_threshold": 38.0946159362793,
          "ap": 0.9027093058346849,
          "f1": 0.8386433710174717,
          "f1_threshold": 38.39329147338867,
          "precision": 0.8625792811839323,
          "recall": 0.816
        },
        "max": {
          "accuracy": 0.997019801980198,
          "ap": 0.9109871555441386,
          "f1": 0.8483950617283951
        },
        "similarity": {
          "accuracy": 0.9968910891089109,
          "accuracy_threshold": 0.8149476051330566,
          "ap": 0.9055298495473483,
          "f1": 0.8382804503582394,
          "f1_threshold": 0.8073698282241821,
          "precision": 0.8584905660377359,
          "recall": 0.819
        }
      }
    ],
    "validation": [
      {
        "cosine": {
          "accuracy": 0.997,
          "accuracy_threshold": 0.7875723838806152,
          "ap": 0.9062117706362431,
          "f1": 0.8467374810318665,
          "f1_threshold": 0.7872105240821838,
          "precision": 0.8567041965199591,
          "recall": 0.837
        },
        "dot": {
          "accuracy": 0.9951881188118812,
          "accuracy_threshold": 6.791950225830078,
          "ap": 0.7961694893608173,
          "f1": 0.7547892720306513,
          "f1_threshold": 6.5062947273254395,
          "precision": 0.7242647058823529,
          "recall": 0.788
        },
        "euclidean": {
          "accuracy": 0.9972475247524752,
          "accuracy_threshold": 1.8595691919326782,
          "ap": 0.9152418353690157,
          "f1": 0.857429718875502,
          "f1_threshold": 1.9024853706359863,
          "precision": 0.8608870967741935,
          "recall": 0.854
        },
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.9152418353690157,
        "manhattan": {
          "accuracy": 0.9970693069306931,
          "accuracy_threshold": 40.351783752441406,
          "ap": 0.9081304582735946,
          "f1": 0.8508064516129031,
          "f1_threshold": 40.351783752441406,
          "precision": 0.8577235772357723,
          "recall": 0.844
        },
        "max": {
          "accuracy": 0.9972475247524752,
          "ap": 0.9152418353690157,
          "f1": 0.857429718875502
        },
        "similarity": {
          "accuracy": 0.997,
          "accuracy_threshold": 0.7875723838806152,
          "ap": 0.9062117706362433,
          "f1": 0.8467374810318665,
          "f1_threshold": 0.7872105240821838,
          "precision": 0.8567041965199591,
          "recall": 0.837
        }
      }
    ]
  },
  "task_name": "SprintDuplicateQuestions"
}