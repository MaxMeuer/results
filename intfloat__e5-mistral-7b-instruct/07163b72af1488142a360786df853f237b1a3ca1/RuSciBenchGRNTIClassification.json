{
  "dataset_revision": "673a610d6d3dd91a547a0d57ae1b56f37ebbf6a1",
  "evaluation_time": 40.37564706802368,
  "kg_co2_emissions": 0.012299237631453158,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.646044921875,
        "f1": 0.626122829015861,
        "f1_weighted": 0.6262255303221685,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.646044921875,
        "scores_per_experiment": [
          {
            "accuracy": 0.65478515625,
            "f1": 0.6398436131343698,
            "f1_weighted": 0.6399661461136227
          },
          {
            "accuracy": 0.64306640625,
            "f1": 0.6205037739300531,
            "f1_weighted": 0.6206506979180181
          },
          {
            "accuracy": 0.65283203125,
            "f1": 0.6338377049486523,
            "f1_weighted": 0.633986566693733
          },
          {
            "accuracy": 0.64599609375,
            "f1": 0.623311363310083,
            "f1_weighted": 0.623368782272173
          },
          {
            "accuracy": 0.64599609375,
            "f1": 0.6271726951509954,
            "f1_weighted": 0.6272475698477517
          },
          {
            "accuracy": 0.61328125,
            "f1": 0.588350780572679,
            "f1_weighted": 0.5884197513708848
          },
          {
            "accuracy": 0.64892578125,
            "f1": 0.6270389612560361,
            "f1_weighted": 0.6271513608733408
          },
          {
            "accuracy": 0.6533203125,
            "f1": 0.6360530213267789,
            "f1_weighted": 0.6361279675877307
          },
          {
            "accuracy": 0.658203125,
            "f1": 0.6391480505539959,
            "f1_weighted": 0.6392607936979633
          },
          {
            "accuracy": 0.64404296875,
            "f1": 0.6259683259749662,
            "f1_weighted": 0.6260756668464672
          }
        ]
      }
    ]
  },
  "task_name": "RuSciBenchGRNTIClassification"
}