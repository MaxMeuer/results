{
  "dataset_revision": "71bba34b0ece6c56dfcf46d9758a27f7a90f17e9",
  "evaluation_time": 2.9591171741485596,
  "kg_co2_emissions": 0.00010291754599673164,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.7800652262535671,
          "accuracy_threshold": 0.9356688261032104,
          "ap": 0.6376573820701761,
          "f1": 0.5989553105049332,
          "f1_threshold": 0.8273947238922119,
          "precision": 0.5053868756121449,
          "recall": 0.7350427350427351
        },
        "dot": {
          "accuracy": 0.7800652262535671,
          "accuracy_threshold": 0.9356691837310791,
          "ap": 0.6376571744536119,
          "f1": 0.5989553105049332,
          "f1_threshold": 0.8273944854736328,
          "precision": 0.5053868756121449,
          "recall": 0.7350427350427351
        },
        "euclidean": {
          "accuracy": 0.7800652262535671,
          "accuracy_threshold": 0.3586953580379486,
          "ap": 0.6376572762038006,
          "f1": 0.5989553105049332,
          "f1_threshold": 0.5875462889671326,
          "precision": 0.5053868756121449,
          "recall": 0.7350427350427351
        },
        "hf_subset": "default",
        "languages": [
          "pol-Latn"
        ],
        "main_score": 0.6376586394603856,
        "manhattan": {
          "accuracy": 0.7741540970240521,
          "accuracy_threshold": 8.17251968383789,
          "ap": 0.6270386014522302,
          "f1": 0.5931386447405728,
          "f1_threshold": 12.778223037719727,
          "precision": 0.49269901083372586,
          "recall": 0.7450142450142451
        },
        "max": {
          "accuracy": 0.7800652262535671,
          "ap": 0.6376586394603856,
          "f1": 0.5989553105049332
        },
        "similarity": {
          "accuracy": 0.7800652262535671,
          "accuracy_threshold": 0.9356689453125,
          "ap": 0.6376586394603856,
          "f1": 0.5989553105049332,
          "f1_threshold": 0.8273947834968567,
          "precision": 0.5053868756121449,
          "recall": 0.7350427350427351
        }
      }
    ]
  },
  "task_name": "SICK-E-PL"
}