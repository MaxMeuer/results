{
  "dataset_revision": "4da4316c6e3287746ab74ff67dd252ad128fceff",
  "evaluation_time": 0.8610615730285645,
  "kg_co2_emissions": 3.3309323445889425e-05,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.693,
          "accuracy_threshold": 0.7115099430084229,
          "ap": 0.7264129104971562,
          "f1": 0.7142857142857142,
          "f1_threshold": 0.6404730677604675,
          "precision": 0.6010928961748634,
          "recall": 0.88
        },
        "dot": {
          "accuracy": 0.693,
          "accuracy_threshold": 0.7115101218223572,
          "ap": 0.7264129104971562,
          "f1": 0.7142857142857142,
          "f1_threshold": 0.6404730677604675,
          "precision": 0.6010928961748634,
          "recall": 0.88
        },
        "euclidean": {
          "accuracy": 0.693,
          "accuracy_threshold": 0.7595919966697693,
          "ap": 0.7264129104971562,
          "f1": 0.7142857142857142,
          "f1_threshold": 0.8479704260826111,
          "precision": 0.6010928961748634,
          "recall": 0.88
        },
        "hf_subset": "default",
        "languages": [
          "ara-Arab"
        ],
        "main_score": 0.7271740134037264,
        "manhattan": {
          "accuracy": 0.693,
          "accuracy_threshold": 16.425064086914062,
          "ap": 0.7271740134037264,
          "f1": 0.7161345987920621,
          "f1_threshold": 18.120742797851562,
          "precision": 0.629742033383915,
          "recall": 0.83
        },
        "max": {
          "accuracy": 0.693,
          "ap": 0.7271740134037264,
          "f1": 0.7161345987920621
        },
        "similarity": {
          "accuracy": 0.693,
          "accuracy_threshold": 0.7115099430084229,
          "ap": 0.7264129104971562,
          "f1": 0.7142857142857142,
          "f1_threshold": 0.6404730081558228,
          "precision": 0.6010928961748634,
          "recall": 0.88
        }
      }
    ]
  },
  "task_name": "ArEntail"
}