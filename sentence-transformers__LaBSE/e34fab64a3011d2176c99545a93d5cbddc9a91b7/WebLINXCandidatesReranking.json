{
  "dataset_revision": "ed1c933c2b3617e5700d8a7ebe07f5975969a453",
  "evaluation_time": 2160.5681416988373,
  "kg_co2_emissions": 0.1740581046775493,
  "mteb_version": "1.12.41",
  "scores": {
    "test_cat": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.10186786606028178,
        "map": 0.11422174709742068,
        "mrr": 0.10186786606028178,
        "nAUC_map_diff1": 0.1432009640045274,
        "nAUC_map_max": -0.21939613028982435,
        "nAUC_map_std": 0.03409335133781702,
        "nAUC_mrr_diff1": 0.13712230029076614,
        "nAUC_mrr_max": -0.21666288436117204,
        "nAUC_mrr_std": 0.035844781521287845
      }
    ],
    "test_geo": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.09351090381908118,
        "map": 0.10978601560524066,
        "mrr": 0.09351090381908118,
        "nAUC_map_diff1": 0.09333882502065682,
        "nAUC_map_max": -0.18697992207413677,
        "nAUC_map_std": 0.08273046277052361,
        "nAUC_mrr_diff1": 0.08153325215805265,
        "nAUC_mrr_max": -0.18173915997791204,
        "nAUC_mrr_std": 0.06832490385584356
      }
    ],
    "test_iid": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.07861944499635737,
        "map": 0.09615427254924129,
        "mrr": 0.07861944499635737,
        "nAUC_map_diff1": 0.005501165135920364,
        "nAUC_map_max": -0.12884476964057848,
        "nAUC_map_std": 0.08260502187878563,
        "nAUC_mrr_diff1": 0.020489369815945974,
        "nAUC_mrr_max": -0.1213766132084789,
        "nAUC_mrr_std": 0.07775728867921138
      }
    ],
    "test_vis": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.09787281214234782,
        "map": 0.11441222512906682,
        "mrr": 0.09787281214234782,
        "nAUC_map_diff1": 0.11843801172715392,
        "nAUC_map_max": -0.1781055602178905,
        "nAUC_map_std": 0.05952584073887932,
        "nAUC_mrr_diff1": 0.11947783036441145,
        "nAUC_mrr_max": -0.16520890568812033,
        "nAUC_mrr_std": 0.055608674876678775
      }
    ],
    "test_web": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.07343238822246456,
        "map": 0.08774240414395883,
        "mrr": 0.07343238822246456,
        "nAUC_map_diff1": 0.07979470538931822,
        "nAUC_map_max": 0.0029212564035437823,
        "nAUC_map_std": 0.10006591501634494,
        "nAUC_mrr_diff1": 0.07748353287778809,
        "nAUC_mrr_max": 0.012004108260275615,
        "nAUC_mrr_std": 0.08978608483120569
      }
    ],
    "validation": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.10273111037907348,
        "map": 0.1198135624165418,
        "mrr": 0.10273111037907348,
        "nAUC_map_diff1": 0.1642626469854425,
        "nAUC_map_max": -0.034840235526746234,
        "nAUC_map_std": -0.023275892104813952,
        "nAUC_mrr_diff1": 0.16376174294399,
        "nAUC_mrr_max": -0.03190233859005303,
        "nAUC_mrr_std": -0.02539253522710991
      }
    ]
  },
  "task_name": "WebLINXCandidatesReranking"
}