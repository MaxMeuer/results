{
  "dataset_revision": "d66bd1f72af766a5cc4b0ca5e00c162f89e8cc46",
  "evaluation_time": 38.7517569065094,
  "kg_co2_emissions": 0.0010569635808126707,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.9968217821782178,
          "accuracy_threshold": 0.6832115054130554,
          "ap": 0.8926374032021946,
          "f1": 0.834319526627219,
          "f1_threshold": 0.6612634658813477,
          "precision": 0.8229571984435797,
          "recall": 0.846
        },
        "dot": {
          "accuracy": 0.9968217821782178,
          "accuracy_threshold": 0.6832114458084106,
          "ap": 0.8926375987134976,
          "f1": 0.834319526627219,
          "f1_threshold": 0.6612635850906372,
          "precision": 0.8229571984435797,
          "recall": 0.846
        },
        "euclidean": {
          "accuracy": 0.9968217821782178,
          "accuracy_threshold": 0.7959754467010498,
          "ap": 0.8926374032021944,
          "f1": 0.834319526627219,
          "f1_threshold": 0.8230873942375183,
          "precision": 0.8229571984435797,
          "recall": 0.846
        },
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.8926375987134976,
        "manhattan": {
          "accuracy": 0.9967425742574257,
          "accuracy_threshold": 17.198448181152344,
          "ap": 0.8885929556527079,
          "f1": 0.8295566502463054,
          "f1_threshold": 18.042034149169922,
          "precision": 0.8174757281553398,
          "recall": 0.842
        },
        "max": {
          "accuracy": 0.9968217821782178,
          "ap": 0.8926375987134976,
          "f1": 0.834319526627219
        },
        "similarity": {
          "accuracy": 0.9968217821782178,
          "accuracy_threshold": 0.6832115650177002,
          "ap": 0.8926374032021944,
          "f1": 0.834319526627219,
          "f1_threshold": 0.6612634658813477,
          "precision": 0.8229571984435797,
          "recall": 0.846
        }
      }
    ],
    "validation": [
      {
        "cosine": {
          "accuracy": 0.9966930693069307,
          "accuracy_threshold": 0.6853010654449463,
          "ap": 0.8866239912663719,
          "f1": 0.8243801652892561,
          "f1_threshold": 0.6769940853118896,
          "precision": 0.8525641025641025,
          "recall": 0.798
        },
        "dot": {
          "accuracy": 0.9966930693069307,
          "accuracy_threshold": 0.6853013038635254,
          "ap": 0.8866233954083933,
          "f1": 0.8243801652892561,
          "f1_threshold": 0.6769944429397583,
          "precision": 0.8525641025641025,
          "recall": 0.798
        },
        "euclidean": {
          "accuracy": 0.9966930693069307,
          "accuracy_threshold": 0.7933459281921387,
          "ap": 0.886623991266372,
          "f1": 0.8243801652892561,
          "f1_threshold": 0.8037484884262085,
          "precision": 0.8525641025641025,
          "recall": 0.798
        },
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.886623991266372,
        "manhattan": {
          "accuracy": 0.9965544554455446,
          "accuracy_threshold": 17.36518096923828,
          "ap": 0.8784946352923011,
          "f1": 0.8199076449461262,
          "f1_threshold": 17.605541229248047,
          "precision": 0.8419388830347735,
          "recall": 0.799
        },
        "max": {
          "accuracy": 0.9966930693069307,
          "ap": 0.886623991266372,
          "f1": 0.8243801652892561
        },
        "similarity": {
          "accuracy": 0.9966930693069307,
          "accuracy_threshold": 0.6853011846542358,
          "ap": 0.8866239912663719,
          "f1": 0.8243801652892561,
          "f1_threshold": 0.6769942045211792,
          "precision": 0.8525641025641025,
          "recall": 0.798
        }
      }
    ]
  },
  "task_name": "SprintDuplicateQuestions"
}