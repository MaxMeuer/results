{
  "dataset_revision": "d66bd1f72af766a5cc4b0ca5e00c162f89e8cc46",
  "evaluation_time": 19.929461002349854,
  "kg_co2_emissions": 0.0008985525952732647,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.9974356435643564,
          "accuracy_threshold": 0.9120439291000366,
          "ap": 0.9313702348755108,
          "f1": 0.8683446272991286,
          "f1_threshold": 0.9038693308830261,
          "precision": 0.8414634146341463,
          "recall": 0.897
        },
        "dot": {
          "accuracy": 0.9974356435643564,
          "accuracy_threshold": 0.9120438098907471,
          "ap": 0.9313693908924982,
          "f1": 0.8683446272991286,
          "f1_threshold": 0.9038687944412231,
          "precision": 0.8414634146341463,
          "recall": 0.897
        },
        "euclidean": {
          "accuracy": 0.9974356435643564,
          "accuracy_threshold": 0.4194188714027405,
          "ap": 0.9313702348755106,
          "f1": 0.8683446272991286,
          "f1_threshold": 0.4384760856628418,
          "precision": 0.8414634146341463,
          "recall": 0.897
        },
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.9313702348755108,
        "manhattan": {
          "accuracy": 0.9974356435643564,
          "accuracy_threshold": 10.598981857299805,
          "ap": 0.9309581234705857,
          "f1": 0.8700451580531862,
          "f1_threshold": 10.901311874389648,
          "precision": 0.8731117824773413,
          "recall": 0.867
        },
        "max": {
          "accuracy": 0.9974356435643564,
          "ap": 0.9313702348755108,
          "f1": 0.8700451580531862
        },
        "similarity": {
          "accuracy": 0.9974356435643564,
          "accuracy_threshold": 0.9120439291000366,
          "ap": 0.9313702348755108,
          "f1": 0.8683446272991286,
          "f1_threshold": 0.9038693308830261,
          "precision": 0.8414634146341463,
          "recall": 0.897
        }
      }
    ],
    "validation": [
      {
        "cosine": {
          "accuracy": 0.9976732673267327,
          "accuracy_threshold": 0.9020702838897705,
          "ap": 0.9322754461342071,
          "f1": 0.8833746898263026,
          "f1_threshold": 0.9020702838897705,
          "precision": 0.8768472906403941,
          "recall": 0.89
        },
        "dot": {
          "accuracy": 0.9976732673267327,
          "accuracy_threshold": 0.9020708799362183,
          "ap": 0.9322755297934128,
          "f1": 0.8833746898263026,
          "f1_threshold": 0.9020708799362183,
          "precision": 0.8768472906403941,
          "recall": 0.89
        },
        "euclidean": {
          "accuracy": 0.9976732673267327,
          "accuracy_threshold": 0.4425601363182068,
          "ap": 0.9322754461342071,
          "f1": 0.8833746898263026,
          "f1_threshold": 0.4425601363182068,
          "precision": 0.8768472906403941,
          "recall": 0.89
        },
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.9322755297934128,
        "manhattan": {
          "accuracy": 0.9976930693069307,
          "accuracy_threshold": 11.21102523803711,
          "ap": 0.9318280547928259,
          "f1": 0.8842523596621956,
          "f1_threshold": 11.238419532775879,
          "precision": 0.87857847976308,
          "recall": 0.89
        },
        "max": {
          "accuracy": 0.9976930693069307,
          "ap": 0.9322755297934128,
          "f1": 0.8842523596621956
        },
        "similarity": {
          "accuracy": 0.9976732673267327,
          "accuracy_threshold": 0.9020702838897705,
          "ap": 0.9322754461342071,
          "f1": 0.8833746898263026,
          "f1_threshold": 0.9020702838897705,
          "precision": 0.8768472906403941,
          "recall": 0.89
        }
      }
    ]
  },
  "task_name": "SprintDuplicateQuestions"
}