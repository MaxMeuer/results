{
  "dataset_revision": "ed1c933c2b3617e5700d8a7ebe07f5975969a453",
  "evaluation_time": 5594.434462547302,
  "kg_co2_emissions": 0.48247100030889023,
  "mteb_version": "1.12.41",
  "scores": {
    "test_cat": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.06470750847155342,
        "map": 0.08227677100148094,
        "mrr": 0.06470750847155342,
        "nAUC_map_diff1": 0.087619821574931,
        "nAUC_map_max": 0.09484503772420473,
        "nAUC_map_std": 0.3117338016393657,
        "nAUC_mrr_diff1": 0.09285220128829375,
        "nAUC_mrr_max": 0.1091082073468003,
        "nAUC_mrr_std": 0.28475050346278713
      }
    ],
    "test_geo": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.09496614553062885,
        "map": 0.1148436126383444,
        "mrr": 0.09496614553062885,
        "nAUC_map_diff1": 0.17563542985367078,
        "nAUC_map_max": 0.04544172536256249,
        "nAUC_map_std": 0.07771227779223806,
        "nAUC_mrr_diff1": 0.18201314798193016,
        "nAUC_mrr_max": 0.055954981760257115,
        "nAUC_mrr_std": 0.07809033835007997
      }
    ],
    "test_iid": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.07205664834315738,
        "map": 0.08992356354309242,
        "mrr": 0.07205664834315738,
        "nAUC_map_diff1": 0.07239083262110325,
        "nAUC_map_max": 0.033068607089100165,
        "nAUC_map_std": 0.1781637102124328,
        "nAUC_mrr_diff1": 0.05749640914989006,
        "nAUC_mrr_max": 0.03753934366187255,
        "nAUC_mrr_std": 0.16321853314207158
      }
    ],
    "test_vis": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.08525529250331063,
        "map": 0.10363411697796272,
        "mrr": 0.08525529250331063,
        "nAUC_map_diff1": 0.1297819602819065,
        "nAUC_map_max": 0.04934987135719602,
        "nAUC_map_std": 0.16269627023607688,
        "nAUC_mrr_diff1": 0.1349916200036057,
        "nAUC_mrr_max": 0.06362733897006169,
        "nAUC_mrr_std": 0.13686394868134125
      }
    ],
    "test_web": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.06167757179207561,
        "map": 0.07970141327691567,
        "mrr": 0.06167757179207561,
        "nAUC_map_diff1": 0.1884785655092256,
        "nAUC_map_max": 0.05318744297635535,
        "nAUC_map_std": 0.054772846750127106,
        "nAUC_mrr_diff1": 0.20004096155794832,
        "nAUC_mrr_max": 0.04723318679940537,
        "nAUC_mrr_std": 0.039576631425781716
      }
    ],
    "validation": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.08815044593292096,
        "map": 0.10778669826424032,
        "mrr": 0.08815044593292096,
        "nAUC_map_diff1": 0.1656309644930538,
        "nAUC_map_max": -0.016735690618462033,
        "nAUC_map_std": 0.09450795773500102,
        "nAUC_mrr_diff1": 0.17390200304658396,
        "nAUC_mrr_max": -0.009816730280088318,
        "nAUC_mrr_std": 0.08784336030214239
      }
    ]
  },
  "task_name": "WebLINXCandidatesReranking"
}