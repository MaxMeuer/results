{
  "dataset_revision": "71bba34b0ece6c56dfcf46d9758a27f7a90f17e9",
  "evaluation_time": 2.030200719833374,
  "kg_co2_emissions": 0.00031253943926746173,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.7225845902975948,
          "accuracy_threshold": 0.9504111409187317,
          "ap": 0.4732423798099674,
          "f1": 0.5396825396825397,
          "f1_threshold": 0.7893433570861816,
          "precision": 0.42517425174251744,
          "recall": 0.7386039886039886
        },
        "dot": {
          "accuracy": 0.7225845902975948,
          "accuracy_threshold": 0.9504114985466003,
          "ap": 0.47324315717007714,
          "f1": 0.5396825396825397,
          "f1_threshold": 0.7893433570861816,
          "precision": 0.42517425174251744,
          "recall": 0.7386039886039886
        },
        "euclidean": {
          "accuracy": 0.7225845902975948,
          "accuracy_threshold": 0.3149249255657196,
          "ap": 0.4732423798099674,
          "f1": 0.5396825396825397,
          "f1_threshold": 0.6490864157676697,
          "precision": 0.42517425174251744,
          "recall": 0.7386039886039886
        },
        "hf_subset": "default",
        "languages": [
          "pol-Latn"
        ],
        "main_score": 0.473939199169381,
        "manhattan": {
          "accuracy": 0.7227884223399919,
          "accuracy_threshold": 5.295978546142578,
          "ap": 0.473939199169381,
          "f1": 0.5412503568369968,
          "f1_threshold": 9.46182632446289,
          "precision": 0.45164363982848976,
          "recall": 0.6752136752136753
        },
        "max": {
          "accuracy": 0.7227884223399919,
          "ap": 0.473939199169381,
          "f1": 0.5412503568369968
        },
        "similarity": {
          "accuracy": 0.7225845902975948,
          "accuracy_threshold": 0.9504112005233765,
          "ap": 0.4732423798099674,
          "f1": 0.5396825396825397,
          "f1_threshold": 0.7893434166908264,
          "precision": 0.42517425174251744,
          "recall": 0.7386039886039886
        }
      }
    ]
  },
  "task_name": "SICK-E-PL"
}