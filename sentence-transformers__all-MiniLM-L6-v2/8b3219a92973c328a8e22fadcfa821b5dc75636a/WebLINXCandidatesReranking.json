{
  "dataset_revision": "ed1c933c2b3617e5700d8a7ebe07f5975969a453",
  "evaluation_time": 814.5246469974518,
  "kg_co2_emissions": 0.14119659668924306,
  "mteb_version": "1.12.41",
  "scores": {
    "test_cat": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.06444756554307116,
        "map": 0.08177635963213344,
        "mrr": 0.06444756554307116,
        "nAUC_map_diff1": 0.05208525716609756,
        "nAUC_map_max": 0.001479162409548248,
        "nAUC_map_std": 0.20502428498178948,
        "nAUC_mrr_diff1": 0.04961919560171281,
        "nAUC_mrr_max": 0.0029067052872399062,
        "nAUC_mrr_std": 0.18667473873046592
      }
    ],
    "test_geo": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.08964516576904698,
        "map": 0.10862612194115114,
        "mrr": 0.08964516576904698,
        "nAUC_map_diff1": 0.11219633431148826,
        "nAUC_map_max": -0.13382169435432467,
        "nAUC_map_std": 0.03399710699753707,
        "nAUC_mrr_diff1": 0.1182319151412509,
        "nAUC_mrr_max": -0.129815082247218,
        "nAUC_mrr_std": 0.026518456174383725
      }
    ],
    "test_iid": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.08088421970549925,
        "map": 0.10056553614576741,
        "mrr": 0.08088421970549925,
        "nAUC_map_diff1": 0.19606466408536605,
        "nAUC_map_max": 0.013243549578855476,
        "nAUC_map_std": 0.10963486474533161,
        "nAUC_mrr_diff1": 0.1981021539543045,
        "nAUC_mrr_max": 0.03066709904138471,
        "nAUC_mrr_std": 0.09832977734576513
      }
    ],
    "test_vis": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.08467690712877575,
        "map": 0.10403519108430202,
        "mrr": 0.08467690712877575,
        "nAUC_map_diff1": 0.056719952292641124,
        "nAUC_map_max": 0.02236877721862708,
        "nAUC_map_std": 0.08307636105135005,
        "nAUC_mrr_diff1": 0.0528981587324306,
        "nAUC_mrr_max": 0.020122405279647787,
        "nAUC_mrr_std": 0.06546892532393524
      }
    ],
    "test_web": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.051665303525990545,
        "map": 0.0718716542500926,
        "mrr": 0.051665303525990545,
        "nAUC_map_diff1": 0.06022139939355893,
        "nAUC_map_max": -0.08452798728442251,
        "nAUC_map_std": 0.15210041327689852,
        "nAUC_mrr_diff1": 0.047455220628073926,
        "nAUC_mrr_max": -0.09858157793430801,
        "nAUC_mrr_std": 0.15387863590826173
      }
    ],
    "validation": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.11242542366677648,
        "map": 0.1309929332295911,
        "mrr": 0.11242542366677648,
        "nAUC_map_diff1": 0.22614741980308942,
        "nAUC_map_max": -0.019689070727958436,
        "nAUC_map_std": -0.027388109469129583,
        "nAUC_mrr_diff1": 0.22553566239355183,
        "nAUC_mrr_max": -0.012486752210736432,
        "nAUC_mrr_std": -0.03804973760821171
      }
    ]
  },
  "task_name": "WebLINXCandidatesReranking"
}