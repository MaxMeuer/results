{
  "dataset_revision": "7335288588f14e5a687d97fc979194c2abe6f4e7",
  "evaluation_time": 0.9800069332122803,
  "kg_co2_emissions": 3.0491473563187247e-05,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.5490767735665695,
          "accuracy_threshold": 0.8198857307434082,
          "ap": 0.5521339534137362,
          "f1": 0.6709928617780663,
          "f1_threshold": 0.3342879116535187,
          "precision": 0.5058708414872799,
          "recall": 0.9961464354527938
        },
        "dot": {
          "accuracy": 0.5490767735665695,
          "accuracy_threshold": 0.819885790348053,
          "ap": 0.5521339534137362,
          "f1": 0.6709928617780663,
          "f1_threshold": 0.3342880606651306,
          "precision": 0.5058708414872799,
          "recall": 0.9961464354527938
        },
        "euclidean": {
          "accuracy": 0.5490767735665695,
          "accuracy_threshold": 0.600190281867981,
          "ap": 0.5521339534137362,
          "f1": 0.6709928617780663,
          "f1_threshold": 1.1538543701171875,
          "precision": 0.5058708414872799,
          "recall": 0.9961464354527938
        },
        "hf_subset": "default",
        "languages": [
          "fas-Arab"
        ],
        "main_score": 0.5522199673902122,
        "manhattan": {
          "accuracy": 0.5490767735665695,
          "accuracy_threshold": 9.40936279296875,
          "ap": 0.5522199673902122,
          "f1": 0.6714285714285715,
          "f1_threshold": 17.656423568725586,
          "precision": 0.5063663075416258,
          "recall": 0.9961464354527938
        },
        "max": {
          "accuracy": 0.5490767735665695,
          "ap": 0.5522199673902122,
          "f1": 0.6714285714285715
        },
        "similarity": {
          "accuracy": 0.5490767735665695,
          "accuracy_threshold": 0.8198858499526978,
          "ap": 0.5521339534137362,
          "f1": 0.6709928617780663,
          "f1_threshold": 0.33428800106048584,
          "precision": 0.5058708414872799,
          "recall": 0.9961464354527938
        }
      }
    ]
  },
  "task_name": "FarsTail"
}