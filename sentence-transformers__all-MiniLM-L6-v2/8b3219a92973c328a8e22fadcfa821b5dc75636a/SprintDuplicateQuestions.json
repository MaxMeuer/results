{
  "dataset_revision": "d66bd1f72af766a5cc4b0ca5e00c162f89e8cc46",
  "evaluation_time": 24.342735528945923,
  "kg_co2_emissions": 0.0036280244150174003,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.9978514851485148,
          "accuracy_threshold": 0.7130525708198547,
          "ap": 0.9455063045792447,
          "f1": 0.890126582278481,
          "f1_threshold": 0.7130525708198547,
          "precision": 0.9015384615384615,
          "recall": 0.879
        },
        "dot": {
          "accuracy": 0.9978514851485148,
          "accuracy_threshold": 0.7130526304244995,
          "ap": 0.9455063045792446,
          "f1": 0.890126582278481,
          "f1_threshold": 0.7130526304244995,
          "precision": 0.9015384615384615,
          "recall": 0.879
        },
        "euclidean": {
          "accuracy": 0.9978514851485148,
          "accuracy_threshold": 0.7575584650039673,
          "ap": 0.9455063045792447,
          "f1": 0.890126582278481,
          "f1_threshold": 0.7575584650039673,
          "precision": 0.9015384615384615,
          "recall": 0.879
        },
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.9455063045792447,
        "manhattan": {
          "accuracy": 0.9978415841584158,
          "accuracy_threshold": 11.837489128112793,
          "ap": 0.9454002074215007,
          "f1": 0.8898989898989899,
          "f1_threshold": 11.837489128112793,
          "precision": 0.8989795918367347,
          "recall": 0.881
        },
        "max": {
          "accuracy": 0.9978514851485148,
          "ap": 0.9455063045792447,
          "f1": 0.890126582278481
        },
        "similarity": {
          "accuracy": 0.9978514851485148,
          "accuracy_threshold": 0.7130525708198547,
          "ap": 0.9455063045792447,
          "f1": 0.890126582278481,
          "f1_threshold": 0.7130525708198547,
          "precision": 0.9015384615384615,
          "recall": 0.879
        }
      }
    ],
    "validation": [
      {
        "cosine": {
          "accuracy": 0.9979009900990099,
          "accuracy_threshold": 0.6937724351882935,
          "ap": 0.951119092450298,
          "f1": 0.8950495049504951,
          "f1_threshold": 0.6933528184890747,
          "precision": 0.8862745098039215,
          "recall": 0.904
        },
        "dot": {
          "accuracy": 0.9979009900990099,
          "accuracy_threshold": 0.693772554397583,
          "ap": 0.9511196715693352,
          "f1": 0.8950495049504951,
          "f1_threshold": 0.6933525800704956,
          "precision": 0.8862745098039215,
          "recall": 0.904
        },
        "euclidean": {
          "accuracy": 0.9979009900990099,
          "accuracy_threshold": 0.7825950384140015,
          "ap": 0.9511190924502979,
          "f1": 0.8950495049504951,
          "f1_threshold": 0.7831311225891113,
          "precision": 0.8862745098039215,
          "recall": 0.904
        },
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.9512766112854072,
        "manhattan": {
          "accuracy": 0.9978613861386139,
          "accuracy_threshold": 11.797510147094727,
          "ap": 0.9512766112854072,
          "f1": 0.8938656280428432,
          "f1_threshold": 12.367403984069824,
          "precision": 0.8709677419354839,
          "recall": 0.918
        },
        "max": {
          "accuracy": 0.9979009900990099,
          "ap": 0.9512766112854072,
          "f1": 0.8950495049504951
        },
        "similarity": {
          "accuracy": 0.9979009900990099,
          "accuracy_threshold": 0.6937724351882935,
          "ap": 0.951119092450298,
          "f1": 0.8950495049504951,
          "f1_threshold": 0.6933528184890747,
          "precision": 0.8862745098039215,
          "recall": 0.904
        }
      }
    ]
  },
  "task_name": "SprintDuplicateQuestions"
}