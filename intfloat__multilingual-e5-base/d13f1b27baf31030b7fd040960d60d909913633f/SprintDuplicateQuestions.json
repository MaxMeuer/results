{
  "dataset_revision": "d66bd1f72af766a5cc4b0ca5e00c162f89e8cc46",
  "evaluation_time": 11.91529655456543,
  "kg_co2_emissions": 0.00041044127495354804,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.9974554455445545,
          "accuracy_threshold": 0.9128450155258179,
          "ap": 0.9301550073806472,
          "f1": 0.8690944881889764,
          "f1_threshold": 0.9071769118309021,
          "precision": 0.8556201550387597,
          "recall": 0.883
        },
        "dot": {
          "accuracy": 0.9974554455445545,
          "accuracy_threshold": 0.9128448963165283,
          "ap": 0.9301551354758206,
          "f1": 0.8690944881889764,
          "f1_threshold": 0.907176673412323,
          "precision": 0.8556201550387597,
          "recall": 0.883
        },
        "euclidean": {
          "accuracy": 0.9974554455445545,
          "accuracy_threshold": 0.4175044894218445,
          "ap": 0.9301549011931369,
          "f1": 0.8690944881889764,
          "f1_threshold": 0.4308667778968811,
          "precision": 0.8556201550387597,
          "recall": 0.883
        },
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.9301551354758206,
        "manhattan": {
          "accuracy": 0.9973861386138614,
          "accuracy_threshold": 9.235107421875,
          "ap": 0.9266003155330228,
          "f1": 0.8644763860369611,
          "f1_threshold": 9.235107421875,
          "precision": 0.8881856540084389,
          "recall": 0.842
        },
        "max": {
          "accuracy": 0.9974554455445545,
          "ap": 0.9301551354758206,
          "f1": 0.8690944881889764
        },
        "similarity": {
          "accuracy": 0.9974554455445545,
          "accuracy_threshold": 0.9128450155258179,
          "ap": 0.9301550073806472,
          "f1": 0.8690944881889764,
          "f1_threshold": 0.9071769118309021,
          "precision": 0.8556201550387597,
          "recall": 0.883
        }
      }
    ],
    "validation": [
      {
        "cosine": {
          "accuracy": 0.9975841584158416,
          "accuracy_threshold": 0.9047350883483887,
          "ap": 0.9384308800354154,
          "f1": 0.8811121764141898,
          "f1_threshold": 0.8998415470123291,
          "precision": 0.8462246777163904,
          "recall": 0.919
        },
        "dot": {
          "accuracy": 0.9975841584158416,
          "accuracy_threshold": 0.9047350287437439,
          "ap": 0.9384313251239169,
          "f1": 0.8811121764141898,
          "f1_threshold": 0.8998415470123291,
          "precision": 0.8462246777163904,
          "recall": 0.919
        },
        "euclidean": {
          "accuracy": 0.9975841584158416,
          "accuracy_threshold": 0.4364972412586212,
          "ap": 0.9384308800354153,
          "f1": 0.8811121764141898,
          "f1_threshold": 0.44756782054901123,
          "precision": 0.8462246777163904,
          "recall": 0.919
        },
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.9384313251239169,
        "manhattan": {
          "accuracy": 0.9975544554455446,
          "accuracy_threshold": 9.581111907958984,
          "ap": 0.9357743308873065,
          "f1": 0.8766849725411883,
          "f1_threshold": 9.581111907958984,
          "precision": 0.8753738783649053,
          "recall": 0.878
        },
        "max": {
          "accuracy": 0.9975841584158416,
          "ap": 0.9384313251239169,
          "f1": 0.8811121764141898
        },
        "similarity": {
          "accuracy": 0.9975841584158416,
          "accuracy_threshold": 0.9047350883483887,
          "ap": 0.9384308800354154,
          "f1": 0.8811121764141898,
          "f1_threshold": 0.8998415470123291,
          "precision": 0.8462246777163904,
          "recall": 0.919
        }
      }
    ]
  },
  "task_name": "SprintDuplicateQuestions"
}