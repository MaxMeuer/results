{
  "dataset_revision": "ed1c933c2b3617e5700d8a7ebe07f5975969a453",
  "evaluation_time": 2040.2968409061432,
  "kg_co2_emissions": 0.15526908957699728,
  "mteb_version": "1.12.41",
  "scores": {
    "test_cat": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.055135321919029784,
        "map": 0.07260554160027016,
        "mrr": 0.055135321919029784,
        "nAUC_map_diff1": 0.2233032456812061,
        "nAUC_map_max": 0.0820458172370015,
        "nAUC_map_std": 0.23408552628682933,
        "nAUC_mrr_diff1": 0.21112842172898819,
        "nAUC_mrr_max": 0.0840783483516649,
        "nAUC_mrr_std": 0.22264590844330467
      }
    ],
    "test_geo": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.08208215480387979,
        "map": 0.09993401514697807,
        "mrr": 0.08208215480387979,
        "nAUC_map_diff1": 0.10678574008601974,
        "nAUC_map_max": 0.04232212997658118,
        "nAUC_map_std": 0.11608404848468423,
        "nAUC_mrr_diff1": 0.11165171551299237,
        "nAUC_mrr_max": 0.04958782486249712,
        "nAUC_mrr_std": 0.11048820408872537
      }
    ],
    "test_iid": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.06346777932313397,
        "map": 0.0814239773640172,
        "mrr": 0.06346777932313397,
        "nAUC_map_diff1": 0.1099279502255647,
        "nAUC_map_max": 0.11743042329860093,
        "nAUC_map_std": 0.1708965549496095,
        "nAUC_mrr_diff1": 0.1319933578546779,
        "nAUC_mrr_max": 0.1346873434663218,
        "nAUC_mrr_std": 0.15342379159898428
      }
    ],
    "test_vis": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.07961637215600975,
        "map": 0.09792753992367655,
        "mrr": 0.07961637215600975,
        "nAUC_map_diff1": 0.18153890353018748,
        "nAUC_map_max": 0.14240214168042095,
        "nAUC_map_std": 0.1302824265913266,
        "nAUC_mrr_diff1": 0.1884166887754957,
        "nAUC_mrr_max": 0.14697933967856278,
        "nAUC_mrr_std": 0.11109174953516832
      }
    ],
    "test_web": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.0544228866270851,
        "map": 0.07257827559477567,
        "mrr": 0.0544228866270851,
        "nAUC_map_diff1": 0.14764863208883994,
        "nAUC_map_max": 0.13581748388359743,
        "nAUC_map_std": 0.08657555218634369,
        "nAUC_mrr_diff1": 0.15858412587491644,
        "nAUC_mrr_max": 0.15158335502666898,
        "nAUC_mrr_std": 0.09606990299376819
      }
    ],
    "validation": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.0881894879396801,
        "map": 0.10633466468206378,
        "mrr": 0.0881894879396801,
        "nAUC_map_diff1": 0.08773395874606972,
        "nAUC_map_max": 0.054346604504564765,
        "nAUC_map_std": 0.15521992628368714,
        "nAUC_mrr_diff1": 0.09216877045349178,
        "nAUC_mrr_max": 0.055665986044947977,
        "nAUC_mrr_std": 0.15640446787892365
      }
    ]
  },
  "task_name": "WebLINXCandidatesReranking"
}