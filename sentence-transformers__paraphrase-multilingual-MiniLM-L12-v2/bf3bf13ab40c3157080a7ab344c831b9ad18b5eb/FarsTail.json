{
  "dataset_revision": "7335288588f14e5a687d97fc979194c2abe6f4e7",
  "evaluation_time": 0.6903576850891113,
  "kg_co2_emissions": 2.3024249451430413e-05,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.6239067055393586,
          "accuracy_threshold": 0.774509072303772,
          "ap": 0.6484016684922852,
          "f1": 0.6789940828402367,
          "f1_threshold": 0.616790771484375,
          "precision": 0.5510204081632653,
          "recall": 0.884393063583815
        },
        "dot": {
          "accuracy": 0.5519922254616132,
          "accuracy_threshold": 8.397536277770996,
          "ap": 0.5647177517881756,
          "f1": 0.6714100905562743,
          "f1_threshold": 0.8443653583526611,
          "precision": 0.5053554040895814,
          "recall": 1.0
        },
        "euclidean": {
          "accuracy": 0.6190476190476191,
          "accuracy_threshold": 2.305286169052124,
          "ap": 0.6052430762311987,
          "f1": 0.6822498173849526,
          "f1_threshold": 3.0024240016937256,
          "precision": 0.5494117647058824,
          "recall": 0.8998073217726397
        },
        "hf_subset": "default",
        "languages": [
          "fas-Arab"
        ],
        "main_score": 0.6484016684922852,
        "manhattan": {
          "accuracy": 0.6229348882410107,
          "accuracy_threshold": 38.675540924072266,
          "ap": 0.606826462659015,
          "f1": 0.682124158563949,
          "f1_threshold": 45.12631607055664,
          "precision": 0.5574572127139364,
          "recall": 0.8786127167630058
        },
        "max": {
          "accuracy": 0.6239067055393586,
          "ap": 0.6484016684922852,
          "f1": 0.6822498173849526
        },
        "similarity": {
          "accuracy": 0.6239067055393586,
          "accuracy_threshold": 0.774509072303772,
          "ap": 0.6484016684922852,
          "f1": 0.6789940828402367,
          "f1_threshold": 0.6167908310890198,
          "precision": 0.5510204081632653,
          "recall": 0.884393063583815
        }
      }
    ]
  },
  "task_name": "FarsTail"
}