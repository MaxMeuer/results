{
  "dataset_revision": "0cdfb1d51ef339011c067688a3b75b82f927c097",
  "evaluation_time": 0.7328140735626221,
  "kg_co2_emissions": 2.0055735808047518e-05,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.7983193277310925,
          "accuracy_threshold": 0.9929808974266052,
          "ap": 0.225102620980495,
          "f1": 0.37751004016064255,
          "f1_threshold": 0.331313818693161,
          "precision": 0.23421926910299004,
          "recall": 0.9724137931034482
        },
        "dot": {
          "accuracy": 0.7969187675070029,
          "accuracy_threshold": 29.67612075805664,
          "ap": 0.1932478553455644,
          "f1": 0.3631647211413748,
          "f1_threshold": 5.022735595703125,
          "precision": 0.22364217252396165,
          "recall": 0.9655172413793104
        },
        "euclidean": {
          "accuracy": 0.7983193277310925,
          "accuracy_threshold": 0.6288765072822571,
          "ap": 0.24359687202295566,
          "f1": 0.40234948604992654,
          "f1_threshold": 4.729731559753418,
          "precision": 0.2555970149253731,
          "recall": 0.9448275862068966
        },
        "hf_subset": "default",
        "languages": [
          "por-Latn"
        ],
        "main_score": 0.24388114731054067,
        "manhattan": {
          "accuracy": 0.7983193277310925,
          "accuracy_threshold": 9.763761520385742,
          "ap": 0.24388114731054067,
          "f1": 0.40173410404624277,
          "f1_threshold": 75.771484375,
          "precision": 0.25411334552102377,
          "recall": 0.9586206896551724
        },
        "max": {
          "accuracy": 0.7983193277310925,
          "ap": 0.24388114731054067,
          "f1": 0.40234948604992654
        },
        "similarity": {
          "accuracy": 0.7983193277310925,
          "accuracy_threshold": 0.9929808378219604,
          "ap": 0.225102620980495,
          "f1": 0.37751004016064255,
          "f1_threshold": 0.3313138484954834,
          "precision": 0.23421926910299004,
          "recall": 0.9724137931034482
        }
      }
    ]
  },
  "task_name": "SICK-BR-PC"
}