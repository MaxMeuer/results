{
  "dataset_revision": "f43b4f32987048043a8b31e5e26be4d360c2438f",
  "evaluation_time": 1.1507346630096436,
  "kg_co2_emissions": 4.1105637104602494e-05,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.889795918367347,
          "accuracy_threshold": 0.7011791467666626,
          "ap": 0.935202178372116,
          "f1": 0.9244402985074627,
          "f1_threshold": 0.7011791467666626,
          "precision": 0.8824577025823687,
          "recall": 0.970617042115573
        },
        "dot": {
          "accuracy": 0.8639455782312925,
          "accuracy_threshold": 5.33515739440918,
          "ap": 0.89987053314738,
          "f1": 0.9092558983666061,
          "f1_threshold": 5.33515739440918,
          "precision": 0.8469991546914624,
          "recall": 0.9813907933398629
        },
        "euclidean": {
          "accuracy": 0.8870748299319728,
          "accuracy_threshold": 2.897090435028076,
          "ap": 0.9343833107971657,
          "f1": 0.923992673992674,
          "f1_threshold": 3.11806058883667,
          "precision": 0.8675838349097162,
          "recall": 0.9882468168462292
        },
        "hf_subset": "default",
        "languages": [
          "hye-Armn"
        ],
        "main_score": 0.935202178372116,
        "manhattan": {
          "accuracy": 0.8870748299319728,
          "accuracy_threshold": 45.59459686279297,
          "ap": 0.9340478237854842,
          "f1": 0.9242700729927008,
          "f1_threshold": 50.297698974609375,
          "precision": 0.8650725875320239,
          "recall": 0.9921645445641528
        },
        "max": {
          "accuracy": 0.889795918367347,
          "ap": 0.935202178372116,
          "f1": 0.9244402985074627
        },
        "similarity": {
          "accuracy": 0.889795918367347,
          "accuracy_threshold": 0.7011790871620178,
          "ap": 0.935202064558111,
          "f1": 0.9244402985074627,
          "f1_threshold": 0.7011790871620178,
          "precision": 0.8824577025823687,
          "recall": 0.970617042115573
        }
      }
    ]
  },
  "task_name": "ArmenianParaphrasePC"
}