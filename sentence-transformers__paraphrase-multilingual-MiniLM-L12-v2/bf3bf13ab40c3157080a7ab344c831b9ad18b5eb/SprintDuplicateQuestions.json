{
  "dataset_revision": "d66bd1f72af766a5cc4b0ca5e00c162f89e8cc46",
  "evaluation_time": 31.530270099639893,
  "kg_co2_emissions": 0.0008620443002421587,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.9967326732673267,
          "accuracy_threshold": 0.7888898253440857,
          "ap": 0.8945644386194277,
          "f1": 0.8304213771839671,
          "f1_threshold": 0.7886518239974976,
          "precision": 0.854122621564482,
          "recall": 0.808
        },
        "dot": {
          "accuracy": 0.9925841584158416,
          "accuracy_threshold": 18.545188903808594,
          "ap": 0.586555204568002,
          "f1": 0.603118040089087,
          "f1_threshold": 17.016767501831055,
          "precision": 0.5437751004016064,
          "recall": 0.677
        },
        "euclidean": {
          "accuracy": 0.996970297029703,
          "accuracy_threshold": 3.002713203430176,
          "ap": 0.9110356114373946,
          "f1": 0.8442871587462083,
          "f1_threshold": 3.1387009620666504,
          "precision": 0.8537832310838446,
          "recall": 0.835
        },
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.9114959431815377,
        "manhattan": {
          "accuracy": 0.997009900990099,
          "accuracy_threshold": 47.7529182434082,
          "ap": 0.9114959431815377,
          "f1": 0.8448877805486285,
          "f1_threshold": 49.73625183105469,
          "precision": 0.8427860696517413,
          "recall": 0.847
        },
        "max": {
          "accuracy": 0.997009900990099,
          "ap": 0.9114959431815377,
          "f1": 0.8448877805486285
        },
        "similarity": {
          "accuracy": 0.9967326732673267,
          "accuracy_threshold": 0.7888897657394409,
          "ap": 0.8945644386194277,
          "f1": 0.8304213771839671,
          "f1_threshold": 0.7886518239974976,
          "precision": 0.854122621564482,
          "recall": 0.808
        }
      }
    ],
    "validation": [
      {
        "cosine": {
          "accuracy": 0.9966237623762376,
          "accuracy_threshold": 0.792998731136322,
          "ap": 0.8904784034410862,
          "f1": 0.8246059989832232,
          "f1_threshold": 0.7858679294586182,
          "precision": 0.8386763185108583,
          "recall": 0.811
        },
        "dot": {
          "accuracy": 0.9926336633663366,
          "accuracy_threshold": 19.10186004638672,
          "ap": 0.5882806766617211,
          "f1": 0.6189138576779026,
          "f1_threshold": 17.511728286743164,
          "precision": 0.5818661971830986,
          "recall": 0.661
        },
        "euclidean": {
          "accuracy": 0.9969207920792079,
          "accuracy_threshold": 3.2404069900512695,
          "ap": 0.908599098214019,
          "f1": 0.848661800486618,
          "f1_threshold": 3.242784023284912,
          "precision": 0.8265402843601896,
          "recall": 0.872
        },
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.908599098214019,
        "manhattan": {
          "accuracy": 0.9969207920792079,
          "accuracy_threshold": 49.9119873046875,
          "ap": 0.9082310236995668,
          "f1": 0.8473244968090329,
          "f1_threshold": 49.9119873046875,
          "precision": 0.8322082931533269,
          "recall": 0.863
        },
        "max": {
          "accuracy": 0.9969207920792079,
          "ap": 0.908599098214019,
          "f1": 0.848661800486618
        },
        "similarity": {
          "accuracy": 0.9966237623762376,
          "accuracy_threshold": 0.7929987907409668,
          "ap": 0.8904781542050884,
          "f1": 0.8246059989832232,
          "f1_threshold": 0.7858679294586182,
          "precision": 0.8386763185108583,
          "recall": 0.811
        }
      }
    ]
  },
  "task_name": "SprintDuplicateQuestions"
}