{
  "dataset_revision": "349481ec73fff722f88e0453ca05c77a447d967c",
  "evaluation_time": 1.092728614807129,
  "kg_co2_emissions": 3.563085147112896e-05,
  "mteb_version": "1.12.41",
  "scores": {
    "validation": [
      {
        "cosine": {
          "accuracy": 0.633,
          "accuracy_threshold": 0.7027188539505005,
          "ap": 0.6551644335396335,
          "f1": 0.6805555555555556,
          "f1_threshold": 0.3178684711456299,
          "precision": 0.5212765957446809,
          "recall": 0.98
        },
        "dot": {
          "accuracy": 0.588,
          "accuracy_threshold": 8.938180923461914,
          "ap": 0.6072490669867038,
          "f1": 0.6731301939058172,
          "f1_threshold": 2.505095958709717,
          "precision": 0.5148305084745762,
          "recall": 0.972
        },
        "euclidean": {
          "accuracy": 0.621,
          "accuracy_threshold": 2.8790996074676514,
          "ap": 0.638337350386654,
          "f1": 0.6839121190644932,
          "f1_threshold": 4.016482830047607,
          "precision": 0.5296377607025247,
          "recall": 0.965
        },
        "hf_subset": "default",
        "languages": [
          "kor-Hang"
        ],
        "main_score": 0.6551644335396335,
        "manhattan": {
          "accuracy": 0.623,
          "accuracy_threshold": 46.34177780151367,
          "ap": 0.6381073779495086,
          "f1": 0.6829093514869222,
          "f1_threshold": 61.28252410888672,
          "precision": 0.5321049692908989,
          "recall": 0.953
        },
        "max": {
          "accuracy": 0.633,
          "ap": 0.6551644335396335,
          "f1": 0.6839121190644932
        },
        "similarity": {
          "accuracy": 0.633,
          "accuracy_threshold": 0.7027188539505005,
          "ap": 0.6551644335396335,
          "f1": 0.6805555555555556,
          "f1_threshold": 0.3178684115409851,
          "precision": 0.5212765957446809,
          "recall": 0.98
        }
      }
    ]
  },
  "task_name": "KLUE-NLI"
}