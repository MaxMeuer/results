{
  "dataset_revision": "ed1c933c2b3617e5700d8a7ebe07f5975969a453",
  "evaluation_time": 875.4258122444153,
  "kg_co2_emissions": 0.04726867516162153,
  "mteb_version": "1.12.41",
  "scores": {
    "test_cat": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.06708612002853577,
        "map": 0.08106054836243831,
        "mrr": 0.06708612002853577,
        "nAUC_map_diff1": 0.15118467583112816,
        "nAUC_map_max": -0.06453070280252356,
        "nAUC_map_std": -0.11574298359426839,
        "nAUC_mrr_diff1": 0.13906796796170898,
        "nAUC_mrr_max": -0.06986920493556648,
        "nAUC_mrr_std": -0.12027589156015142
      }
    ],
    "test_geo": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.0705964973458871,
        "map": 0.08888323305918808,
        "mrr": 0.0705964973458871,
        "nAUC_map_diff1": 0.09465906341825822,
        "nAUC_map_max": -0.02387689104192022,
        "nAUC_map_std": 0.09463360541182543,
        "nAUC_mrr_diff1": 0.0770559387192119,
        "nAUC_mrr_max": -0.04674556230484657,
        "nAUC_mrr_std": 0.06691507562726107
      }
    ],
    "test_iid": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.07507643994083493,
        "map": 0.08902255398542558,
        "mrr": 0.07507643994083493,
        "nAUC_map_diff1": 0.1056979253566326,
        "nAUC_map_max": 0.02060936943031173,
        "nAUC_map_std": 0.13545372166277067,
        "nAUC_mrr_diff1": 0.09303749594032895,
        "nAUC_mrr_max": 0.014872645435375092,
        "nAUC_mrr_std": 0.1199648606299963
      }
    ],
    "test_vis": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.055838456560427116,
        "map": 0.07226928316052324,
        "mrr": 0.055838456560427116,
        "nAUC_map_diff1": 0.08403690257053217,
        "nAUC_map_max": -0.00512325940988474,
        "nAUC_map_std": 0.1886411640893075,
        "nAUC_mrr_diff1": 0.07786953474521531,
        "nAUC_mrr_max": -0.014607783785561374,
        "nAUC_mrr_std": 0.17372815286625934
      }
    ],
    "test_web": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.048601771073145124,
        "map": 0.06464357267838311,
        "mrr": 0.048601771073145124,
        "nAUC_map_diff1": 0.10103250788857303,
        "nAUC_map_max": 0.12987950313485122,
        "nAUC_map_std": 0.23186930761459085,
        "nAUC_mrr_diff1": 0.10313655837478831,
        "nAUC_mrr_max": 0.13547698284200516,
        "nAUC_mrr_std": 0.2278814092407324
      }
    ],
    "validation": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.08154197625757964,
        "map": 0.09610543419225534,
        "mrr": 0.08154197625757964,
        "nAUC_map_diff1": 0.062845573639144,
        "nAUC_map_max": -0.010658820060867715,
        "nAUC_map_std": 0.1937140690161118,
        "nAUC_mrr_diff1": 0.045269896011642864,
        "nAUC_mrr_max": -0.019961154552531286,
        "nAUC_mrr_std": 0.20338362423783568
      }
    ]
  },
  "task_name": "WebLINXCandidatesReranking"
}