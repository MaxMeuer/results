{
  "dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1",
  "evaluation_time": 7.395775556564331,
  "kg_co2_emissions": 0.00027339610967899524,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.8607617571675508,
          "accuracy_threshold": 0.725288987159729,
          "ap": 0.7385399411009183,
          "f1": 0.6850702798531088,
          "f1_threshold": 0.6927633285522461,
          "precision": 0.6586316045775505,
          "recall": 0.7137203166226913
        },
        "dot": {
          "accuracy": 0.8607617571675508,
          "accuracy_threshold": 0.7252891659736633,
          "ap": 0.7385396321688664,
          "f1": 0.6850702798531088,
          "f1_threshold": 0.6927635669708252,
          "precision": 0.6586316045775505,
          "recall": 0.7137203166226913
        },
        "euclidean": {
          "accuracy": 0.8607617571675508,
          "accuracy_threshold": 0.7412300705909729,
          "ap": 0.7385398591084057,
          "f1": 0.6850702798531088,
          "f1_threshold": 0.7838835716247559,
          "precision": 0.6586316045775505,
          "recall": 0.7137203166226913
        },
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.7386873205558416,
        "manhattan": {
          "accuracy": 0.8598676759849795,
          "accuracy_threshold": 16.31048583984375,
          "ap": 0.7386873205558416,
          "f1": 0.6855096559662361,
          "f1_threshold": 16.945289611816406,
          "precision": 0.6651774633904195,
          "recall": 0.7071240105540897
        },
        "max": {
          "accuracy": 0.8607617571675508,
          "ap": 0.7386873205558416,
          "f1": 0.6855096559662361
        },
        "similarity": {
          "accuracy": 0.8607617571675508,
          "accuracy_threshold": 0.725288987159729,
          "ap": 0.7385397230849641,
          "f1": 0.6850702798531088,
          "f1_threshold": 0.6927633881568909,
          "precision": 0.6586316045775505,
          "recall": 0.7137203166226913
        }
      }
    ]
  },
  "task_name": "TwitterSemEval2015"
}