{
  "dataset_revision": "0cdfb1d51ef339011c067688a3b75b82f927c097",
  "evaluation_time": 2.3640928268432617,
  "kg_co2_emissions": 0.00016560360621575534,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.7955182072829131,
          "accuracy_threshold": 0.9725824594497681,
          "ap": 0.25580045321834477,
          "f1": 0.43463497453310695,
          "f1_threshold": 0.6443929672241211,
          "precision": 0.2882882882882883,
          "recall": 0.8827586206896552
        },
        "dot": {
          "accuracy": 0.7955182072829131,
          "accuracy_threshold": 0.9725824594497681,
          "ap": 0.25580045321834477,
          "f1": 0.43463497453310695,
          "f1_threshold": 0.6443936824798584,
          "precision": 0.2882882882882883,
          "recall": 0.8827586206896552
        },
        "euclidean": {
          "accuracy": 0.7955182072829131,
          "accuracy_threshold": 0.23414021730422974,
          "ap": 0.25580045321834477,
          "f1": 0.43463497453310695,
          "f1_threshold": 0.843334972858429,
          "precision": 0.2882882882882883,
          "recall": 0.8827586206896552
        },
        "hf_subset": "default",
        "languages": [
          "por-Latn"
        ],
        "main_score": 0.25856428738001996,
        "manhattan": {
          "accuracy": 0.7955182072829131,
          "accuracy_threshold": 11.79654598236084,
          "ap": 0.25856428738001996,
          "f1": 0.440251572327044,
          "f1_threshold": 38.65974044799805,
          "precision": 0.31626506024096385,
          "recall": 0.7241379310344828
        },
        "max": {
          "accuracy": 0.7955182072829131,
          "ap": 0.25856428738001996,
          "f1": 0.440251572327044
        },
        "similarity": {
          "accuracy": 0.7955182072829131,
          "accuracy_threshold": 0.9725824594497681,
          "ap": 0.25580045321834477,
          "f1": 0.43463497453310695,
          "f1_threshold": 0.6443929672241211,
          "precision": 0.2882882882882883,
          "recall": 0.8827586206896552
        }
      }
    ]
  },
  "task_name": "SICK-BR-PC"
}