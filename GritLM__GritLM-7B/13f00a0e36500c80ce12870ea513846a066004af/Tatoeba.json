{
  "dataset_revision": "69e8f12da6e31d59addadda9a9c8a2e601a0e282",
  "evaluation_time": 368.3936696052551,
  "kg_co2_emissions": 0.031602847668253345,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "accuracy": 0.5612648221343873,
        "f1": 0.501347927434884,
        "hf_subset": "csb-eng",
        "languages": [
          "csb-Latn",
          "eng-Latn"
        ],
        "main_score": 0.501347927434884,
        "precision": 0.48258673693456305,
        "recall": 0.5612648221343873
      },
      {
        "accuracy": 0.385,
        "f1": 0.33498369062499495,
        "hf_subset": "ceb-eng",
        "languages": [
          "ceb-Latn",
          "eng-Latn"
        ],
        "main_score": 0.33498369062499495,
        "precision": 0.32104255651755653,
        "recall": 0.385
      },
      {
        "accuracy": 0.954,
        "f1": 0.9408333333333333,
        "hf_subset": "cmn-eng",
        "languages": [
          "cmn-Hans",
          "eng-Latn"
        ],
        "main_score": 0.9408333333333333,
        "precision": 0.9345,
        "recall": 0.954
      },
      {
        "accuracy": 0.4766355140186916,
        "f1": 0.4168892870308483,
        "hf_subset": "uzb-eng",
        "languages": [
          "uzb-Latn",
          "eng-Latn"
        ],
        "main_score": 0.4168892870308483,
        "precision": 0.39912958925809383,
        "recall": 0.4766355140186916
      },
      {
        "accuracy": 0.32926829268292684,
        "f1": 0.2794421215640728,
        "hf_subset": "kur-eng",
        "languages": [
          "kur-Latn",
          "eng-Latn"
        ],
        "main_score": 0.2794421215640728,
        "precision": 0.26520738746348504,
        "recall": 0.32926829268292684
      },
      {
        "accuracy": 0.932,
        "f1": 0.9119666666666667,
        "hf_subset": "ita-eng",
        "languages": [
          "ita-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9119666666666667,
        "precision": 0.9025,
        "recall": 0.932
      },
      {
        "accuracy": 0.595,
        "f1": 0.5354324093469254,
        "hf_subset": "lvs-eng",
        "languages": [
          "lvs-Latn",
          "eng-Latn"
        ],
        "main_score": 0.5354324093469254,
        "precision": 0.5156537581699346,
        "recall": 0.595
      },
      {
        "accuracy": 0.2240566037735849,
        "f1": 0.17125017722730712,
        "hf_subset": "yid-eng",
        "languages": [
          "yid-Hebr",
          "eng-Latn"
        ],
        "main_score": 0.17125017722730712,
        "precision": 0.15906332351465152,
        "recall": 0.2240566037735849
      },
      {
        "accuracy": 0.544,
        "f1": 0.4814156313970267,
        "hf_subset": "gle-eng",
        "languages": [
          "gle-Latn",
          "eng-Latn"
        ],
        "main_score": 0.4814156313970267,
        "precision": 0.46094871975140517,
        "recall": 0.544
      },
      {
        "accuracy": 0.8346456692913385,
        "f1": 0.7910761154855643,
        "hf_subset": "ast-eng",
        "languages": [
          "ast-Latn",
          "eng-Latn"
        ],
        "main_score": 0.7910761154855643,
        "precision": 0.7729658792650917,
        "recall": 0.8346456692913385
      },
      {
        "accuracy": 0.8134328358208955,
        "f1": 0.768407960199005,
        "hf_subset": "ang-eng",
        "languages": [
          "ang-Latn",
          "eng-Latn"
        ],
        "main_score": 0.768407960199005,
        "precision": 0.75,
        "recall": 0.8134328358208955
      },
      {
        "accuracy": 0.33170731707317075,
        "f1": 0.26604529616724737,
        "hf_subset": "jav-eng",
        "languages": [
          "jav-Latn",
          "eng-Latn"
        ],
        "main_score": 0.26604529616724737,
        "precision": 0.2474716340569999,
        "recall": 0.33170731707317075
      },
      {
        "accuracy": 0.931,
        "f1": 0.9123666666666665,
        "hf_subset": "ina-eng",
        "languages": [
          "ina-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9123666666666665,
        "precision": 0.90425,
        "recall": 0.931
      },
      {
        "accuracy": 0.95,
        "f1": 0.9353333333333332,
        "hf_subset": "nob-eng",
        "languages": [
          "nob-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9353333333333332,
        "precision": 0.9285666666666667,
        "recall": 0.95
      },
      {
        "accuracy": 0.924,
        "f1": 0.9043333333333332,
        "hf_subset": "swe-eng",
        "languages": [
          "swe-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9043333333333332,
        "precision": 0.8957777777777778,
        "recall": 0.924
      },
      {
        "accuracy": 0.673,
        "f1": 0.6222771825396826,
        "hf_subset": "lfn-eng",
        "languages": [
          "lfn-Latn",
          "eng-Latn"
        ],
        "main_score": 0.6222771825396826,
        "precision": 0.6050679723502305,
        "recall": 0.673
      },
      {
        "accuracy": 0.888,
        "f1": 0.8575666666666666,
        "hf_subset": "fin-eng",
        "languages": [
          "fin-Latn",
          "eng-Latn"
        ],
        "main_score": 0.8575666666666666,
        "precision": 0.8443611111111111,
        "recall": 0.888
      },
      {
        "accuracy": 0.6763005780346821,
        "f1": 0.6115606936416185,
        "hf_subset": "fry-eng",
        "languages": [
          "fry-Latn",
          "eng-Latn"
        ],
        "main_score": 0.6115606936416185,
        "precision": 0.5847784200385356,
        "recall": 0.6763005780346821
      },
      {
        "accuracy": 0.6068376068376068,
        "f1": 0.5327838827838828,
        "hf_subset": "gsw-eng",
        "languages": [
          "gsw-Latn",
          "eng-Latn"
        ],
        "main_score": 0.5327838827838828,
        "precision": 0.5069190069190069,
        "recall": 0.6068376068376068
      },
      {
        "accuracy": 0.937,
        "f1": 0.9181666666666666,
        "hf_subset": "rus-eng",
        "languages": [
          "rus-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.9181666666666666,
        "precision": 0.9091666666666668,
        "recall": 0.937
      },
      {
        "accuracy": 0.294,
        "f1": 0.24463862124001445,
        "hf_subset": "tat-eng",
        "languages": [
          "tat-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.24463862124001445,
        "precision": 0.23058688387977583,
        "recall": 0.294
      },
      {
        "accuracy": 0.4148471615720524,
        "f1": 0.33786112399649515,
        "hf_subset": "mal-eng",
        "languages": [
          "mal-Mlym",
          "eng-Latn"
        ],
        "main_score": 0.33786112399649515,
        "precision": 0.3147563054768295,
        "recall": 0.4148471615720524
      },
      {
        "accuracy": 0.929,
        "f1": 0.9103666666666668,
        "hf_subset": "hrv-eng",
        "languages": [
          "hrv-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9103666666666668,
        "precision": 0.90175,
        "recall": 0.929
      },
      {
        "accuracy": 0.921,
        "f1": 0.9005333333333333,
        "hf_subset": "ind-eng",
        "languages": [
          "ind-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9005333333333333,
        "precision": 0.8914833333333333,
        "recall": 0.921
      },
      {
        "accuracy": 0.5472312703583062,
        "f1": 0.46272943487927193,
        "hf_subset": "tam-eng",
        "languages": [
          "tam-Taml",
          "eng-Latn"
        ],
        "main_score": 0.46272943487927193,
        "precision": 0.43252287885838375,
        "recall": 0.5472312703583062
      },
      {
        "accuracy": 0.42782608695652175,
        "f1": 0.3626774282835664,
        "hf_subset": "kaz-eng",
        "languages": [
          "kaz-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.3626774282835664,
        "precision": 0.3434654500089283,
        "recall": 0.42782608695652175
      },
      {
        "accuracy": 0.28,
        "f1": 0.2259802070534829,
        "hf_subset": "uig-eng",
        "languages": [
          "uig-Arab",
          "eng-Latn"
        ],
        "main_score": 0.2259802070534829,
        "precision": 0.21032006733576503,
        "recall": 0.28
      },
      {
        "accuracy": 0.8602673147023087,
        "f1": 0.8270970422853776,
        "hf_subset": "slv-eng",
        "languages": [
          "slv-Latn",
          "eng-Latn"
        ],
        "main_score": 0.8270970422853776,
        "precision": 0.8133859862292427,
        "recall": 0.8602673147023087
      },
      {
        "accuracy": 0.5676190476190476,
        "f1": 0.5040606060606061,
        "hf_subset": "pms-eng",
        "languages": [
          "pms-Latn",
          "eng-Latn"
        ],
        "main_score": 0.5040606060606061,
        "precision": 0.48225421481724,
        "recall": 0.5676190476190476
      },
      {
        "accuracy": 0.616,
        "f1": 0.563579975949976,
        "hf_subset": "lit-eng",
        "languages": [
          "lit-Latn",
          "eng-Latn"
        ],
        "main_score": 0.563579975949976,
        "precision": 0.5465956227982544,
        "recall": 0.616
      },
      {
        "accuracy": 0.39416058394160586,
        "f1": 0.34692324706923244,
        "hf_subset": "cha-eng",
        "languages": [
          "cha-Latn",
          "eng-Latn"
        ],
        "main_score": 0.34692324706923244,
        "precision": 0.3350567721005678,
        "recall": 0.39416058394160586
      },
      {
        "accuracy": 0.521,
        "f1": 0.4673166102013928,
        "hf_subset": "est-eng",
        "languages": [
          "est-Latn",
          "eng-Latn"
        ],
        "main_score": 0.4673166102013928,
        "precision": 0.4505030265486148,
        "recall": 0.521
      },
      {
        "accuracy": 0.133,
        "f1": 0.1079603924595304,
        "hf_subset": "mhr-eng",
        "languages": [
          "mhr-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.1079603924595304,
        "precision": 0.10141509483808245,
        "recall": 0.133
      },
      {
        "accuracy": 0.938,
        "f1": 0.9200999999999999,
        "hf_subset": "dan-eng",
        "languages": [
          "dan-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9200999999999999,
        "precision": 0.912,
        "recall": 0.938
      },
      {
        "accuracy": 0.966,
        "f1": 0.956,
        "hf_subset": "pol-eng",
        "languages": [
          "pol-Latn",
          "eng-Latn"
        ],
        "main_score": 0.956,
        "precision": 0.9511666666666666,
        "recall": 0.966
      },
      {
        "accuracy": 0.7003891050583657,
        "f1": 0.6484713729849916,
        "hf_subset": "nov-eng",
        "languages": [
          "nov-Latn",
          "eng-Latn"
        ],
        "main_score": 0.6484713729849916,
        "precision": 0.632338953739732,
        "recall": 0.7003891050583657
      },
      {
        "accuracy": 0.5384615384615384,
        "f1": 0.46087899280206973,
        "hf_subset": "swh-eng",
        "languages": [
          "swh-Latn",
          "eng-Latn"
        ],
        "main_score": 0.46087899280206973,
        "precision": 0.43527583527583524,
        "recall": 0.5384615384615384
      },
      {
        "accuracy": 0.8467153284671532,
        "f1": 0.8125101378751014,
        "hf_subset": "tha-eng",
        "languages": [
          "tha-Thai",
          "eng-Latn"
        ],
        "main_score": 0.8125101378751014,
        "precision": 0.7987479724249796,
        "recall": 0.8467153284671532
      },
      {
        "accuracy": 0.589098532494759,
        "f1": 0.5296888043948476,
        "hf_subset": "arz-eng",
        "languages": [
          "arz-Arab",
          "eng-Latn"
        ],
        "main_score": 0.5296888043948476,
        "precision": 0.5082123269230187,
        "recall": 0.589098532494759
      },
      {
        "accuracy": 0.808,
        "f1": 0.7687285714285715,
        "hf_subset": "epo-eng",
        "languages": [
          "epo-Latn",
          "eng-Latn"
        ],
        "main_score": 0.7687285714285715,
        "precision": 0.752854700854701,
        "recall": 0.808
      },
      {
        "accuracy": 0.985,
        "f1": 0.9801666666666667,
        "hf_subset": "deu-eng",
        "languages": [
          "deu-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9801666666666667,
        "precision": 0.9778333333333332,
        "recall": 0.985
      },
      {
        "accuracy": 0.42452830188679247,
        "f1": 0.3593557346239616,
        "hf_subset": "hye-eng",
        "languages": [
          "hye-Armn",
          "eng-Latn"
        ],
        "main_score": 0.3593557346239616,
        "precision": 0.3383730940714111,
        "recall": 0.42452830188679247
      },
      {
        "accuracy": 0.823,
        "f1": 0.7916888023088022,
        "hf_subset": "afr-eng",
        "languages": [
          "afr-Latn",
          "eng-Latn"
        ],
        "main_score": 0.7916888023088022,
        "precision": 0.7796575757575759,
        "recall": 0.823
      },
      {
        "accuracy": 0.4692400482509047,
        "f1": 0.4079905705748323,
        "hf_subset": "gla-eng",
        "languages": [
          "gla-Latn",
          "eng-Latn"
        ],
        "main_score": 0.4079905705748323,
        "precision": 0.38833452237948723,
        "recall": 0.4692400482509047
      },
      {
        "accuracy": 0.794,
        "f1": 0.7494345238095238,
        "hf_subset": "isl-eng",
        "languages": [
          "isl-Latn",
          "eng-Latn"
        ],
        "main_score": 0.7494345238095238,
        "precision": 0.7318277777777777,
        "recall": 0.794
      },
      {
        "accuracy": 0.5194805194805194,
        "f1": 0.4430633339724248,
        "hf_subset": "awa-eng",
        "languages": [
          "awa-Deva",
          "eng-Latn"
        ],
        "main_score": 0.4430633339724248,
        "precision": 0.4182708032708033,
        "recall": 0.5194805194805194
      },
      {
        "accuracy": 0.708,
        "f1": 0.6569429828341592,
        "hf_subset": "ido-eng",
        "languages": [
          "ido-Latn",
          "eng-Latn"
        ],
        "main_score": 0.6569429828341592,
        "precision": 0.6390747582972583,
        "recall": 0.708
      },
      {
        "accuracy": 0.899,
        "f1": 0.8742666666666666,
        "hf_subset": "kor-eng",
        "languages": [
          "kor-Hang",
          "eng-Latn"
        ],
        "main_score": 0.8742666666666666,
        "precision": 0.8630277777777778,
        "recall": 0.899
      },
      {
        "accuracy": 0.08928571428571429,
        "f1": 0.06183491437812238,
        "hf_subset": "amh-eng",
        "languages": [
          "amh-Ethi",
          "eng-Latn"
        ],
        "main_score": 0.06183491437812238,
        "precision": 0.05703833616780045,
        "recall": 0.08928571428571429
      },
      {
        "accuracy": 0.376,
        "f1": 0.31880655080213904,
        "hf_subset": "eus-eng",
        "languages": [
          "eus-Latn",
          "eng-Latn"
        ],
        "main_score": 0.31880655080213904,
        "precision": 0.302810155487787,
        "recall": 0.376
      },
      {
        "accuracy": 0.783,
        "f1": 0.7382312271062271,
        "hf_subset": "mkd-eng",
        "languages": [
          "mkd-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.7382312271062271,
        "precision": 0.7204325757575757,
        "recall": 0.783
      },
      {
        "accuracy": 0.895,
        "f1": 0.8662000000000001,
        "hf_subset": "tur-eng",
        "languages": [
          "tur-Latn",
          "eng-Latn"
        ],
        "main_score": 0.8662000000000001,
        "precision": 0.8532833333333333,
        "recall": 0.895
      },
      {
        "accuracy": 0.83,
        "f1": 0.7897968253968254,
        "hf_subset": "pes-eng",
        "languages": [
          "pes-Arab",
          "eng-Latn"
        ],
        "main_score": 0.7897968253968254,
        "precision": 0.7729833333333334,
        "recall": 0.83
      },
      {
        "accuracy": 0.68,
        "f1": 0.617469597069597,
        "hf_subset": "heb-eng",
        "languages": [
          "heb-Hebr",
          "eng-Latn"
        ],
        "main_score": 0.617469597069597,
        "precision": 0.5941325396825395,
        "recall": 0.68
      },
      {
        "accuracy": 0.697,
        "f1": 0.6410668534080298,
        "hf_subset": "aze-eng",
        "languages": [
          "aze-Latn",
          "eng-Latn"
        ],
        "main_score": 0.6410668534080298,
        "precision": 0.6208123274672188,
        "recall": 0.697
      },
      {
        "accuracy": 0.907,
        "f1": 0.8854500000000001,
        "hf_subset": "hun-eng",
        "languages": [
          "hun-Latn",
          "eng-Latn"
        ],
        "main_score": 0.8854500000000001,
        "precision": 0.8761261904761904,
        "recall": 0.907
      },
      {
        "accuracy": 0.923,
        "f1": 0.9037333333333334,
        "hf_subset": "bul-eng",
        "languages": [
          "bul-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.9037333333333334,
        "precision": 0.89475,
        "recall": 0.923
      },
      {
        "accuracy": 0.046,
        "f1": 0.028957431267562846,
        "hf_subset": "kab-eng",
        "languages": [
          "kab-Latn",
          "eng-Latn"
        ],
        "main_score": 0.028957431267562846,
        "precision": 0.025696311554885404,
        "recall": 0.046
      },
      {
        "accuracy": 0.925,
        "f1": 0.9065714285714285,
        "hf_subset": "cat-eng",
        "languages": [
          "cat-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9065714285714285,
        "precision": 0.8988666666666668,
        "recall": 0.925
      },
      {
        "accuracy": 0.5782881002087683,
        "f1": 0.5172411480399614,
        "hf_subset": "dsb-eng",
        "languages": [
          "dsb-Latn",
          "eng-Latn"
        ],
        "main_score": 0.5172411480399614,
        "precision": 0.49791066043675647,
        "recall": 0.5782881002087683
      },
      {
        "accuracy": 0.450402144772118,
        "f1": 0.38420883732837335,
        "hf_subset": "kat-eng",
        "languages": [
          "kat-Geor",
          "eng-Latn"
        ],
        "main_score": 0.38420883732837335,
        "precision": 0.36316837020590376,
        "recall": 0.450402144772118
      },
      {
        "accuracy": 0.732,
        "f1": 0.6802058730158731,
        "hf_subset": "urd-eng",
        "languages": [
          "urd-Arab",
          "eng-Latn"
        ],
        "main_score": 0.6802058730158731,
        "precision": 0.6589465986394557,
        "recall": 0.732
      },
      {
        "accuracy": 0.84,
        "f1": 0.8028190476190477,
        "hf_subset": "wuu-eng",
        "languages": [
          "wuu-Hans",
          "eng-Latn"
        ],
        "main_score": 0.8028190476190477,
        "precision": 0.7874333333333334,
        "recall": 0.84
      },
      {
        "accuracy": 0.629,
        "f1": 0.5811872598162072,
        "hf_subset": "oci-eng",
        "languages": [
          "oci-Latn",
          "eng-Latn"
        ],
        "main_score": 0.5811872598162072,
        "precision": 0.5643857932607933,
        "recall": 0.629
      },
      {
        "accuracy": 0.3677277716794731,
        "f1": 0.3051884098660396,
        "hf_subset": "arq-eng",
        "languages": [
          "arq-Arab",
          "eng-Latn"
        ],
        "main_score": 0.3051884098660396,
        "precision": 0.28595729696168776,
        "recall": 0.3677277716794731
      },
      {
        "accuracy": 0.919,
        "f1": 0.9029079365079364,
        "hf_subset": "ron-eng",
        "languages": [
          "ron-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9029079365079364,
        "precision": 0.8962083333333333,
        "recall": 0.919
      },
      {
        "accuracy": 0.9011299435028248,
        "f1": 0.8732580037664783,
        "hf_subset": "bos-eng",
        "languages": [
          "bos-Latn",
          "eng-Latn"
        ],
        "main_score": 0.8732580037664783,
        "precision": 0.861111111111111,
        "recall": 0.9011299435028248
      },
      {
        "accuracy": 0.702,
        "f1": 0.6454268822680588,
        "hf_subset": "nds-eng",
        "languages": [
          "nds-Latn",
          "eng-Latn"
        ],
        "main_score": 0.6454268822680588,
        "precision": 0.6242962301587301,
        "recall": 0.702
      },
      {
        "accuracy": 0.864,
        "f1": 0.8324333333333332,
        "hf_subset": "tgl-eng",
        "languages": [
          "tgl-Latn",
          "eng-Latn"
        ],
        "main_score": 0.8324333333333332,
        "precision": 0.8195857142857143,
        "recall": 0.864
      },
      {
        "accuracy": 0.892,
        "f1": 0.8669412698412697,
        "hf_subset": "glg-eng",
        "languages": [
          "glg-Latn",
          "eng-Latn"
        ],
        "main_score": 0.8669412698412697,
        "precision": 0.857225,
        "recall": 0.892
      },
      {
        "accuracy": 0.679,
        "f1": 0.6131976911976912,
        "hf_subset": "ben-eng",
        "languages": [
          "ben-Beng",
          "eng-Latn"
        ],
        "main_score": 0.6131976911976912,
        "precision": 0.5879857142857142,
        "recall": 0.679
      },
      {
        "accuracy": 0.2257617728531856,
        "f1": 0.1639591018674854,
        "hf_subset": "khm-eng",
        "languages": [
          "khm-Khmr",
          "eng-Latn"
        ],
        "main_score": 0.1639591018674854,
        "precision": 0.1481430020164041,
        "recall": 0.2257617728531856
      },
      {
        "accuracy": 0.921,
        "f1": 0.9018999999999999,
        "hf_subset": "ukr-eng",
        "languages": [
          "ukr-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.9018999999999999,
        "precision": 0.8931166666666667,
        "recall": 0.921
      },
      {
        "accuracy": 0.5774647887323944,
        "f1": 0.5186787391012743,
        "hf_subset": "max-eng",
        "languages": [
          "max-Deva",
          "eng-Latn"
        ],
        "main_score": 0.5186787391012743,
        "precision": 0.5000670690811536,
        "recall": 0.5774647887323944
      },
      {
        "accuracy": 0.842,
        "f1": 0.8043151515151514,
        "hf_subset": "lat-eng",
        "languages": [
          "lat-Latn",
          "eng-Latn"
        ],
        "main_score": 0.8043151515151514,
        "precision": 0.789002380952381,
        "recall": 0.842
      },
      {
        "accuracy": 0.34507042253521125,
        "f1": 0.28426417088388917,
        "hf_subset": "xho-eng",
        "languages": [
          "xho-Latn",
          "eng-Latn"
        ],
        "main_score": 0.28426417088388917,
        "precision": 0.27016264252179745,
        "recall": 0.34507042253521125
      },
      {
        "accuracy": 0.975,
        "f1": 0.9675,
        "hf_subset": "spa-eng",
        "languages": [
          "spa-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9675,
        "precision": 0.9638333333333332,
        "recall": 0.975
      },
      {
        "accuracy": 0.49038461538461536,
        "f1": 0.4285485347985348,
        "hf_subset": "tzl-eng",
        "languages": [
          "tzl-Latn",
          "eng-Latn"
        ],
        "main_score": 0.4285485347985348,
        "precision": 0.40789072039072044,
        "recall": 0.49038461538461536
      },
      {
        "accuracy": 0.808,
        "f1": 0.7676888888888889,
        "hf_subset": "ara-eng",
        "languages": [
          "ara-Arab",
          "eng-Latn"
        ],
        "main_score": 0.7676888888888889,
        "precision": 0.751146679197995,
        "recall": 0.808
      },
      {
        "accuracy": 0.932,
        "f1": 0.9132333333333333,
        "hf_subset": "vie-eng",
        "languages": [
          "vie-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9132333333333333,
        "precision": 0.9046166666666666,
        "recall": 0.932
      },
      {
        "accuracy": 0.939,
        "f1": 0.9201666666666666,
        "hf_subset": "ces-eng",
        "languages": [
          "ces-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9201666666666666,
        "precision": 0.9113333333333332,
        "recall": 0.939
      },
      {
        "accuracy": 0.937,
        "f1": 0.9189666666666667,
        "hf_subset": "jpn-eng",
        "languages": [
          "jpn-Jpan",
          "eng-Latn"
        ],
        "main_score": 0.9189666666666667,
        "precision": 0.9108333333333334,
        "recall": 0.937
      },
      {
        "accuracy": 0.805,
        "f1": 0.7620829365079365,
        "hf_subset": "bel-eng",
        "languages": [
          "bel-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.7620829365079365,
        "precision": 0.7452202380952381,
        "recall": 0.805
      },
      {
        "accuracy": 0.3159090909090909,
        "f1": 0.2737643239113827,
        "hf_subset": "mon-eng",
        "languages": [
          "mon-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.2737643239113827,
        "precision": 0.26231552173074446,
        "recall": 0.3159090909090909
      },
      {
        "accuracy": 0.961,
        "f1": 0.9495666666666667,
        "hf_subset": "nld-eng",
        "languages": [
          "nld-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9495666666666667,
        "precision": 0.94425,
        "recall": 0.961
      },
      {
        "accuracy": 0.327,
        "f1": 0.2774788939342648,
        "hf_subset": "war-eng",
        "languages": [
          "war-Latn",
          "eng-Latn"
        ],
        "main_score": 0.2774788939342648,
        "precision": 0.2640957906558526,
        "recall": 0.327
      },
      {
        "accuracy": 0.163,
        "f1": 0.12590704875361738,
        "hf_subset": "bre-eng",
        "languages": [
          "bre-Latn",
          "eng-Latn"
        ],
        "main_score": 0.12590704875361738,
        "precision": 0.11741775687275688,
        "recall": 0.163
      },
      {
        "accuracy": 0.948,
        "f1": 0.9340666666666666,
        "hf_subset": "por-eng",
        "languages": [
          "por-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9340666666666666,
        "precision": 0.9274166666666668,
        "recall": 0.948
      },
      {
        "accuracy": 0.806,
        "f1": 0.7672064213564213,
        "hf_subset": "ile-eng",
        "languages": [
          "ile-Latn",
          "eng-Latn"
        ],
        "main_score": 0.7672064213564213,
        "precision": 0.7524789682539682,
        "recall": 0.806
      },
      {
        "accuracy": 0.582,
        "f1": 0.515417040149393,
        "hf_subset": "mar-eng",
        "languages": [
          "mar-Deva",
          "eng-Latn"
        ],
        "main_score": 0.515417040149393,
        "precision": 0.49356257631257633,
        "recall": 0.582
      },
      {
        "accuracy": 0.6717557251908397,
        "f1": 0.6203318098356265,
        "hf_subset": "fao-eng",
        "languages": [
          "fao-Latn",
          "eng-Latn"
        ],
        "main_score": 0.6203318098356265,
        "precision": 0.6035032715376226,
        "recall": 0.6717557251908397
      },
      {
        "accuracy": 0.878,
        "f1": 0.849648484848485,
        "hf_subset": "slk-eng",
        "languages": [
          "slk-Latn",
          "eng-Latn"
        ],
        "main_score": 0.849648484848485,
        "precision": 0.8371333333333332,
        "recall": 0.878
      },
      {
        "accuracy": 0.3076923076923077,
        "f1": 0.24259491985336737,
        "hf_subset": "tel-eng",
        "languages": [
          "tel-Telu",
          "eng-Latn"
        ],
        "main_score": 0.24259491985336737,
        "precision": 0.22321682946682947,
        "recall": 0.3076923076923077
      },
      {
        "accuracy": 0.56,
        "f1": 0.5002547038169227,
        "hf_subset": "cym-eng",
        "languages": [
          "cym-Latn",
          "eng-Latn"
        ],
        "main_score": 0.5002547038169227,
        "precision": 0.4813712905452036,
        "recall": 0.56
      },
      {
        "accuracy": 0.909,
        "f1": 0.8844666666666667,
        "hf_subset": "srp-eng",
        "languages": [
          "srp-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.8844666666666667,
        "precision": 0.8733333333333334,
        "recall": 0.909
      },
      {
        "accuracy": 0.5982142857142857,
        "f1": 0.5209041950113379,
        "hf_subset": "swg-eng",
        "languages": [
          "swg-Latn",
          "eng-Latn"
        ],
        "main_score": 0.5209041950113379,
        "precision": 0.4959077380952381,
        "recall": 0.5982142857142857
      },
      {
        "accuracy": 0.876,
        "f1": 0.8419333333333334,
        "hf_subset": "hin-eng",
        "languages": [
          "hin-Deva",
          "eng-Latn"
        ],
        "main_score": 0.8419333333333334,
        "precision": 0.8270333333333333,
        "recall": 0.876
      },
      {
        "accuracy": 0.831,
        "f1": 0.795026984126984,
        "hf_subset": "yue-eng",
        "languages": [
          "yue-Hant",
          "eng-Latn"
        ],
        "main_score": 0.795026984126984,
        "precision": 0.780525,
        "recall": 0.831
      },
      {
        "accuracy": 0.941,
        "f1": 0.9247000000000001,
        "hf_subset": "fra-eng",
        "languages": [
          "fra-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9247000000000001,
        "precision": 0.91725,
        "recall": 0.941
      },
      {
        "accuracy": 0.089,
        "f1": 0.06970872469864038,
        "hf_subset": "cor-eng",
        "languages": [
          "cor-Latn",
          "eng-Latn"
        ],
        "main_score": 0.06970872469864038,
        "precision": 0.06449965424104635,
        "recall": 0.089
      },
      {
        "accuracy": 0.6956521739130435,
        "f1": 0.6447793981955472,
        "hf_subset": "hsb-eng",
        "languages": [
          "hsb-Latn",
          "eng-Latn"
        ],
        "main_score": 0.6447793981955472,
        "precision": 0.6264994667168581,
        "recall": 0.6956521739130435
      },
      {
        "accuracy": 0.921,
        "f1": 0.9006333333333334,
        "hf_subset": "zsm-eng",
        "languages": [
          "zsm-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9006333333333334,
        "precision": 0.8917999999999999,
        "recall": 0.921
      },
      {
        "accuracy": 0.073,
        "f1": 0.06196165557951272,
        "hf_subset": "ber-eng",
        "languages": [
          "ber-Tfng",
          "eng-Latn"
        ],
        "main_score": 0.06196165557951272,
        "precision": 0.06059192086679771,
        "recall": 0.073
      },
      {
        "accuracy": 0.149,
        "f1": 0.12110608514292724,
        "hf_subset": "pam-eng",
        "languages": [
          "pam-Latn",
          "eng-Latn"
        ],
        "main_score": 0.12110608514292724,
        "precision": 0.11470596229207093,
        "recall": 0.149
      },
      {
        "accuracy": 0.116,
        "f1": 0.09608807189542483,
        "hf_subset": "kzj-eng",
        "languages": [
          "kzj-Latn",
          "eng-Latn"
        ],
        "main_score": 0.09608807189542483,
        "precision": 0.09173191763368714,
        "recall": 0.116
      },
      {
        "accuracy": 0.104,
        "f1": 0.08369317567353281,
        "hf_subset": "dtp-eng",
        "languages": [
          "dtp-Latn",
          "eng-Latn"
        ],
        "main_score": 0.08369317567353281,
        "precision": 0.07926034238391755,
        "recall": 0.104
      },
      {
        "accuracy": 0.844,
        "f1": 0.8089199134199134,
        "hf_subset": "nno-eng",
        "languages": [
          "nno-Latn",
          "eng-Latn"
        ],
        "main_score": 0.8089199134199134,
        "precision": 0.7948833333333333,
        "recall": 0.844
      },
      {
        "accuracy": 0.838,
        "f1": 0.8012675324675325,
        "hf_subset": "ell-eng",
        "languages": [
          "ell-Grek",
          "eng-Latn"
        ],
        "main_score": 0.8012675324675325,
        "precision": 0.7861075757575757,
        "recall": 0.838
      },
      {
        "accuracy": 0.5233532934131736,
        "f1": 0.4588114036049743,
        "hf_subset": "orv-eng",
        "languages": [
          "orv-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.4588114036049743,
        "precision": 0.43717976244011125,
        "recall": 0.5233532934131736
      },
      {
        "accuracy": 0.596,
        "f1": 0.5436728494623655,
        "hf_subset": "sqi-eng",
        "languages": [
          "sqi-Latn",
          "eng-Latn"
        ],
        "main_score": 0.5436728494623655,
        "precision": 0.5279206205559146,
        "recall": 0.596
      },
      {
        "accuracy": 0.35960591133004927,
        "f1": 0.30469826604127875,
        "hf_subset": "tuk-eng",
        "languages": [
          "tuk-Latn",
          "eng-Latn"
        ],
        "main_score": 0.30469826604127875,
        "precision": 0.28997576041911016,
        "recall": 0.35960591133004927
      },
      {
        "accuracy": 0.735,
        "f1": 0.6764412698412697,
        "hf_subset": "cbk-eng",
        "languages": [
          "cbk-Latn",
          "eng-Latn"
        ],
        "main_score": 0.6764412698412697,
        "precision": 0.654725,
        "recall": 0.735
      }
    ]
  },
  "task_name": "Tatoeba"
}