{
  "dataset_revision": "12ca3b695563788fead87a982ad1a068284413f4",
  "evaluation_time": 22.314501523971558,
  "kg_co2_emissions": 0.0015823007591417243,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "accuracy": 0.8208452722063038,
        "ap": 0.7666049812200744,
        "ap_weighted": 0.7666049812200744,
        "f1": 0.8208035358974494,
        "f1_weighted": 0.8208035358974491,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.8208452722063038,
        "scores_per_experiment": [
          {
            "accuracy": 0.8202005730659025,
            "ap": 0.765809042929154,
            "ap_weighted": 0.765809042929154,
            "f1": 0.8201598768204272,
            "f1_weighted": 0.8201598768204272
          },
          {
            "accuracy": 0.8209169054441261,
            "ap": 0.7667977823366847,
            "ap_weighted": 0.7667977823366847,
            "f1": 0.8208724180478482,
            "f1_weighted": 0.820872418047848
          },
          {
            "accuracy": 0.8216332378223495,
            "ap": 0.7674734311580248,
            "ap_weighted": 0.7674734311580248,
            "f1": 0.8215928658497467,
            "f1_weighted": 0.8215928658497466
          },
          {
            "accuracy": 0.8202005730659025,
            "ap": 0.765809042929154,
            "ap_weighted": 0.765809042929154,
            "f1": 0.8201598768204272,
            "f1_weighted": 0.8201598768204272
          },
          {
            "accuracy": 0.8209169054441261,
            "ap": 0.7667977823366847,
            "ap_weighted": 0.7667977823366847,
            "f1": 0.8208724180478482,
            "f1_weighted": 0.820872418047848
          },
          {
            "accuracy": 0.8216332378223495,
            "ap": 0.7674734311580248,
            "ap_weighted": 0.7674734311580248,
            "f1": 0.8215928658497467,
            "f1_weighted": 0.8215928658497466
          },
          {
            "accuracy": 0.8202005730659025,
            "ap": 0.765809042929154,
            "ap_weighted": 0.765809042929154,
            "f1": 0.8201598768204272,
            "f1_weighted": 0.8201598768204272
          },
          {
            "accuracy": 0.8209169054441261,
            "ap": 0.7667977823366847,
            "ap_weighted": 0.7667977823366847,
            "f1": 0.8208724180478482,
            "f1_weighted": 0.820872418047848
          },
          {
            "accuracy": 0.8216332378223495,
            "ap": 0.7674734311580248,
            "ap_weighted": 0.7674734311580248,
            "f1": 0.8215928658497467,
            "f1_weighted": 0.8215928658497466
          },
          {
            "accuracy": 0.8202005730659025,
            "ap": 0.765809042929154,
            "ap_weighted": 0.765809042929154,
            "f1": 0.8201598768204272,
            "f1_weighted": 0.8201598768204272
          }
        ]
      }
    ]
  },
  "task_name": "CUADLicenseGrantLegalBenchClassification"
}