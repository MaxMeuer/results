{
  "dataset_revision": "b07c6ce548f6a7ac8d546e1bbe197a0086409190",
  "evaluation_time": 40.94149875640869,
  "kg_co2_emissions": 0.00324015518495559,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "accuracy": 0.749072265625,
        "f1": 0.6697583897902236,
        "f1_weighted": 0.76424146424533,
        "hf_subset": "default",
        "languages": [
          "swe-Latn"
        ],
        "main_score": 0.749072265625,
        "scores_per_experiment": [
          {
            "accuracy": 0.720703125,
            "f1": 0.6183114907902931,
            "f1_weighted": 0.7242657404045469
          },
          {
            "accuracy": 0.744140625,
            "f1": 0.6895558742585016,
            "f1_weighted": 0.7692571353121133
          },
          {
            "accuracy": 0.763671875,
            "f1": 0.6950667971104939,
            "f1_weighted": 0.7802157189604635
          },
          {
            "accuracy": 0.7939453125,
            "f1": 0.7043963946847329,
            "f1_weighted": 0.8013373312902428
          },
          {
            "accuracy": 0.71142578125,
            "f1": 0.6090488235617127,
            "f1_weighted": 0.725943465780203
          },
          {
            "accuracy": 0.76806640625,
            "f1": 0.6702936638181848,
            "f1_weighted": 0.7743315933983563
          },
          {
            "accuracy": 0.76611328125,
            "f1": 0.6919792965416004,
            "f1_weighted": 0.7761961985270395
          },
          {
            "accuracy": 0.76953125,
            "f1": 0.7097667929592926,
            "f1_weighted": 0.7934512645341574
          },
          {
            "accuracy": 0.70556640625,
            "f1": 0.6393403700260584,
            "f1_weighted": 0.7314418990841444
          },
          {
            "accuracy": 0.74755859375,
            "f1": 0.6698243941513661,
            "f1_weighted": 0.7659742951620332
          }
        ]
      }
    ]
  },
  "task_name": "SweRecClassification"
}