{
  "dataset_revision": "d66bd1f72af766a5cc4b0ca5e00c162f89e8cc46",
  "evaluation_time": 42.465240478515625,
  "kg_co2_emissions": 0.00235018376665616,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.9973861386138614,
          "accuracy_threshold": 0.7722398638725281,
          "ap": 0.9306466027614652,
          "f1": 0.8676028084252758,
          "f1_threshold": 0.7658869624137878,
          "precision": 0.8702213279678068,
          "recall": 0.865
        },
        "dot": {
          "accuracy": 0.9973861386138614,
          "accuracy_threshold": 0.772240161895752,
          "ap": 0.9306455823430307,
          "f1": 0.8676028084252758,
          "f1_threshold": 0.7658873200416565,
          "precision": 0.8702213279678068,
          "recall": 0.865
        },
        "euclidean": {
          "accuracy": 0.9973861386138614,
          "accuracy_threshold": 0.6749223470687866,
          "ap": 0.9306466027614652,
          "f1": 0.8676028084252758,
          "f1_threshold": 0.6842705011367798,
          "precision": 0.8702213279678068,
          "recall": 0.865
        },
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.934787627380593,
        "manhattan": {
          "accuracy": 0.9975049504950495,
          "accuracy_threshold": 34.123958587646484,
          "ap": 0.934787627380593,
          "f1": 0.874381800197824,
          "f1_threshold": 34.622642517089844,
          "precision": 0.8649706457925636,
          "recall": 0.884
        },
        "max": {
          "accuracy": 0.9975049504950495,
          "ap": 0.934787627380593,
          "f1": 0.874381800197824
        },
        "similarity": {
          "accuracy": 0.9973861386138614,
          "accuracy_threshold": 0.7722398638725281,
          "ap": 0.9306466027614652,
          "f1": 0.8676028084252758,
          "f1_threshold": 0.7658869624137878,
          "precision": 0.8702213279678068,
          "recall": 0.865
        }
      }
    ],
    "validation": [
      {
        "cosine": {
          "accuracy": 0.9977524752475248,
          "accuracy_threshold": 0.7588725686073303,
          "ap": 0.9422202258313933,
          "f1": 0.8868110236220472,
          "f1_threshold": 0.752123236656189,
          "precision": 0.873062015503876,
          "recall": 0.901
        },
        "dot": {
          "accuracy": 0.9977524752475248,
          "accuracy_threshold": 0.7588708400726318,
          "ap": 0.9422202997801807,
          "f1": 0.8868110236220472,
          "f1_threshold": 0.7521239519119263,
          "precision": 0.873062015503876,
          "recall": 0.901
        },
        "euclidean": {
          "accuracy": 0.9977524752475248,
          "accuracy_threshold": 0.6944456696510315,
          "ap": 0.9422202258313934,
          "f1": 0.8868110236220472,
          "f1_threshold": 0.7040975093841553,
          "precision": 0.873062015503876,
          "recall": 0.901
        },
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.9465912888353798,
        "manhattan": {
          "accuracy": 0.9978613861386139,
          "accuracy_threshold": 35.06341552734375,
          "ap": 0.9465912888353798,
          "f1": 0.8925373134328358,
          "f1_threshold": 35.10432052612305,
          "precision": 0.8881188118811881,
          "recall": 0.897
        },
        "max": {
          "accuracy": 0.9978613861386139,
          "ap": 0.9465912888353798,
          "f1": 0.8925373134328358
        },
        "similarity": {
          "accuracy": 0.9977524752475248,
          "accuracy_threshold": 0.7588725686073303,
          "ap": 0.9422202258313933,
          "f1": 0.8868110236220472,
          "f1_threshold": 0.752123236656189,
          "precision": 0.873062015503876,
          "recall": 0.901
        }
      }
    ]
  },
  "task_name": "SprintDuplicateQuestions"
}