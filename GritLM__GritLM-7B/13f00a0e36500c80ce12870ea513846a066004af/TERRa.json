{
  "dataset_revision": "7b58f24536063837d644aab9a023c62199b2a612",
  "evaluation_time": 4.13726282119751,
  "kg_co2_emissions": 0.000348679450380074,
  "mteb_version": "1.12.41",
  "scores": {
    "dev": [
      {
        "cosine": {
          "accuracy": 0.6123778501628665,
          "accuracy_threshold": 0.553301215171814,
          "ap": 0.5939057400313874,
          "f1": 0.6711409395973154,
          "f1_threshold": 0.4211321771144867,
          "precision": 0.5102040816326531,
          "recall": 0.9803921568627451
        },
        "dot": {
          "accuracy": 0.6123778501628665,
          "accuracy_threshold": 0.5533013939857483,
          "ap": 0.5939057400313874,
          "f1": 0.6711409395973154,
          "f1_threshold": 0.42113196849823,
          "precision": 0.5102040816326531,
          "recall": 0.9803921568627451
        },
        "euclidean": {
          "accuracy": 0.6123778501628665,
          "accuracy_threshold": 0.9451953172683716,
          "ap": 0.5939057400313874,
          "f1": 0.6711409395973154,
          "f1_threshold": 1.0759811401367188,
          "precision": 0.5102040816326531,
          "recall": 0.9803921568627451
        },
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.5941083907489362,
        "manhattan": {
          "accuracy": 0.6091205211726385,
          "accuracy_threshold": 47.57228088378906,
          "ap": 0.5941083907489362,
          "f1": 0.672686230248307,
          "f1_threshold": 52.79367446899414,
          "precision": 0.5137931034482759,
          "recall": 0.9738562091503268
        },
        "max": {
          "accuracy": 0.6123778501628665,
          "ap": 0.5941083907489362,
          "f1": 0.672686230248307
        },
        "similarity": {
          "accuracy": 0.6123778501628665,
          "accuracy_threshold": 0.553301215171814,
          "ap": 0.5939057400313874,
          "f1": 0.6711409395973154,
          "f1_threshold": 0.4211321771144867,
          "precision": 0.5102040816326531,
          "recall": 0.9803921568627451
        }
      }
    ]
  },
  "task_name": "TERRa"
}