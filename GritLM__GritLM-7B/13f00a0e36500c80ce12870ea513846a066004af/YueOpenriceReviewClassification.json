{
  "dataset_revision": "1300d045cf983bac23faadf3aa12a619624769da",
  "evaluation_time": 54.41183948516846,
  "kg_co2_emissions": 0.004210457539695763,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "accuracy": 0.357568359375,
        "f1": 0.31947347362809597,
        "f1_weighted": 0.3713570440908208,
        "hf_subset": "default",
        "languages": [
          "yue-Hant"
        ],
        "main_score": 0.357568359375,
        "scores_per_experiment": [
          {
            "accuracy": 0.3291015625,
            "f1": 0.3143983162071204,
            "f1_weighted": 0.3308960611986503
          },
          {
            "accuracy": 0.36669921875,
            "f1": 0.3194067592627688,
            "f1_weighted": 0.38844819782487244
          },
          {
            "accuracy": 0.3515625,
            "f1": 0.2992272406574408,
            "f1_weighted": 0.362196567430867
          },
          {
            "accuracy": 0.4052734375,
            "f1": 0.3626028940507115,
            "f1_weighted": 0.4202148770035277
          },
          {
            "accuracy": 0.359375,
            "f1": 0.3231160172967773,
            "f1_weighted": 0.37477235158854744
          },
          {
            "accuracy": 0.37060546875,
            "f1": 0.32260687991084847,
            "f1_weighted": 0.39096109364393333
          },
          {
            "accuracy": 0.33349609375,
            "f1": 0.3055589156535765,
            "f1_weighted": 0.34493720134101014
          },
          {
            "accuracy": 0.361328125,
            "f1": 0.32148880898647625,
            "f1_weighted": 0.3770790853411196
          },
          {
            "accuracy": 0.3662109375,
            "f1": 0.31611040891866277,
            "f1_weighted": 0.3847907480184043
          },
          {
            "accuracy": 0.33203125,
            "f1": 0.31021849533657686,
            "f1_weighted": 0.33927425751727625
          }
        ]
      }
    ]
  },
  "task_name": "YueOpenriceReviewClassification"
}