{
  "dataset_revision": "7b56c6cb1c9c8523249f407044c838660df3811a",
  "evaluation_time": 15.818243503570557,
  "kg_co2_emissions": 0.0009994904398204726,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "accuracy": 0.723193359375,
        "f1": 0.6038785756204046,
        "f1_weighted": 0.7713359443727256,
        "hf_subset": "default",
        "languages": [
          "vie-Latn"
        ],
        "main_score": 0.723193359375,
        "scores_per_experiment": [
          {
            "accuracy": 0.78515625,
            "f1": 0.6476125118751744,
            "f1_weighted": 0.8147309269933085
          },
          {
            "accuracy": 0.705078125,
            "f1": 0.5939396356387927,
            "f1_weighted": 0.7642104759707704
          },
          {
            "accuracy": 0.80810546875,
            "f1": 0.6510280168244215,
            "f1_weighted": 0.8246201722983972
          },
          {
            "accuracy": 0.64990234375,
            "f1": 0.5590130361894393,
            "f1_weighted": 0.7224262749449237
          },
          {
            "accuracy": 0.66162109375,
            "f1": 0.5668470353641819,
            "f1_weighted": 0.7260896931464786
          },
          {
            "accuracy": 0.75,
            "f1": 0.6273260829802769,
            "f1_weighted": 0.7959155234594195
          },
          {
            "accuracy": 0.7021484375,
            "f1": 0.5858372802485118,
            "f1_weighted": 0.7583207052354088
          },
          {
            "accuracy": 0.69482421875,
            "f1": 0.5738488566661117,
            "f1_weighted": 0.7361295804695829
          },
          {
            "accuracy": 0.73388671875,
            "f1": 0.6171744197697996,
            "f1_weighted": 0.7858828802173112
          },
          {
            "accuracy": 0.7412109375,
            "f1": 0.6161588806473358,
            "f1_weighted": 0.7850332109916539
          }
        ]
      }
    ]
  },
  "task_name": "VieStudentFeedbackClassification"
}