{
  "dataset_revision": "7335288588f14e5a687d97fc979194c2abe6f4e7",
  "evaluation_time": 17.996682167053223,
  "kg_co2_emissions": 0.0015676483064613394,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.6870748299319728,
          "accuracy_threshold": 0.6946618556976318,
          "ap": 0.7363596152990123,
          "f1": 0.708994708994709,
          "f1_threshold": 0.6834433078765869,
          "precision": 0.6536585365853659,
          "recall": 0.7745664739884393
        },
        "dot": {
          "accuracy": 0.6870748299319728,
          "accuracy_threshold": 0.6946617364883423,
          "ap": 0.7363578956608197,
          "f1": 0.708994708994709,
          "f1_threshold": 0.6834434270858765,
          "precision": 0.6536585365853659,
          "recall": 0.7745664739884393
        },
        "euclidean": {
          "accuracy": 0.6870748299319728,
          "accuracy_threshold": 0.7814577221870422,
          "ap": 0.7363596152990123,
          "f1": 0.708994708994709,
          "f1_threshold": 0.7956841588020325,
          "precision": 0.6536585365853659,
          "recall": 0.7745664739884393
        },
        "hf_subset": "default",
        "languages": [
          "fas-Arab"
        ],
        "main_score": 0.7384779980109168,
        "manhattan": {
          "accuracy": 0.6841593780369291,
          "accuracy_threshold": 38.98686218261719,
          "ap": 0.7384779980109168,
          "f1": 0.7106347897774113,
          "f1_threshold": 41.290672302246094,
          "precision": 0.6210374639769453,
          "recall": 0.8304431599229287
        },
        "max": {
          "accuracy": 0.6870748299319728,
          "ap": 0.7384779980109168,
          "f1": 0.7106347897774113
        },
        "similarity": {
          "accuracy": 0.6870748299319728,
          "accuracy_threshold": 0.6946618556976318,
          "ap": 0.7363596152990123,
          "f1": 0.708994708994709,
          "f1_threshold": 0.6834433078765869,
          "precision": 0.6536585365853659,
          "recall": 0.7745664739884393
        }
      }
    ]
  },
  "task_name": "FarsTail"
}