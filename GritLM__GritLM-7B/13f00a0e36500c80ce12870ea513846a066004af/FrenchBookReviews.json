{
  "dataset_revision": "534725e03fec6f560dbe8166e8ae3825314a6290",
  "evaluation_time": 34.467899322509766,
  "kg_co2_emissions": 0.0026562445156692797,
  "mteb_version": "1.12.41",
  "scores": {
    "train": [
      {
        "accuracy": 0.4703125,
        "f1": 0.3902198064326135,
        "f1_weighted": 0.4997803797699438,
        "hf_subset": "default",
        "languages": [
          "fra-Latn"
        ],
        "main_score": 0.4703125,
        "scores_per_experiment": [
          {
            "accuracy": 0.3583984375,
            "f1": 0.3157320047278932,
            "f1_weighted": 0.4107665796881892
          },
          {
            "accuracy": 0.640625,
            "f1": 0.49396462637469857,
            "f1_weighted": 0.6317677486198939
          },
          {
            "accuracy": 0.49609375,
            "f1": 0.401264701743233,
            "f1_weighted": 0.5225820385763553
          },
          {
            "accuracy": 0.40087890625,
            "f1": 0.3604467773689655,
            "f1_weighted": 0.44048112205614454
          },
          {
            "accuracy": 0.58251953125,
            "f1": 0.406798912356703,
            "f1_weighted": 0.5965496702269694
          },
          {
            "accuracy": 0.41748046875,
            "f1": 0.3998221804745956,
            "f1_weighted": 0.44982899386975683
          },
          {
            "accuracy": 0.50634765625,
            "f1": 0.404505566545384,
            "f1_weighted": 0.533583015269957
          },
          {
            "accuracy": 0.4951171875,
            "f1": 0.42571418819621804,
            "f1_weighted": 0.5288795299732751
          },
          {
            "accuracy": 0.443359375,
            "f1": 0.37416624697719714,
            "f1_weighted": 0.4804796681106825
          },
          {
            "accuracy": 0.3623046875,
            "f1": 0.3197828595612473,
            "f1_weighted": 0.402885431308214
          }
        ]
      }
    ]
  },
  "task_name": "FrenchBookReviews"
}