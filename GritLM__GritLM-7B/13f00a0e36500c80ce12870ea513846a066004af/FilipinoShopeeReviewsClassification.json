{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "evaluation_time": 36.845441818237305,
  "kg_co2_emissions": 0.002348185217219189,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "accuracy": 0.369091796875,
        "f1": 0.3555944668170144,
        "f1_weighted": 0.35558143250062424,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ],
        "main_score": 0.369091796875,
        "scores_per_experiment": [
          {
            "accuracy": 0.39794921875,
            "f1": 0.39433094764721954,
            "f1_weighted": 0.3943105900720226
          },
          {
            "accuracy": 0.3486328125,
            "f1": 0.3226195377924623,
            "f1_weighted": 0.32263531467068424
          },
          {
            "accuracy": 0.36328125,
            "f1": 0.34690338416446653,
            "f1_weighted": 0.3468680559382968
          },
          {
            "accuracy": 0.376953125,
            "f1": 0.3747517682849639,
            "f1_weighted": 0.37473588121029155
          },
          {
            "accuracy": 0.3701171875,
            "f1": 0.3534159162124243,
            "f1_weighted": 0.3533724966574367
          },
          {
            "accuracy": 0.39453125,
            "f1": 0.3645838318476021,
            "f1_weighted": 0.36455947490382806
          },
          {
            "accuracy": 0.3662109375,
            "f1": 0.35590833347014217,
            "f1_weighted": 0.3558425836181933
          },
          {
            "accuracy": 0.3818359375,
            "f1": 0.3718330503963288,
            "f1_weighted": 0.37182877094203826
          },
          {
            "accuracy": 0.3896484375,
            "f1": 0.38564461414944307,
            "f1_weighted": 0.38565191621761363
          },
          {
            "accuracy": 0.3017578125,
            "f1": 0.2859532842050916,
            "f1_weighted": 0.2860092407758369
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.369189453125,
        "f1": 0.3549159347779225,
        "f1_weighted": 0.3549015885038487,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ],
        "main_score": 0.369189453125,
        "scores_per_experiment": [
          {
            "accuracy": 0.3876953125,
            "f1": 0.380792753164542,
            "f1_weighted": 0.38078152221206574
          },
          {
            "accuracy": 0.35302734375,
            "f1": 0.3287041030025198,
            "f1_weighted": 0.3287447824290069
          },
          {
            "accuracy": 0.35693359375,
            "f1": 0.3367762659152406,
            "f1_weighted": 0.3367214262455151
          },
          {
            "accuracy": 0.40087890625,
            "f1": 0.39406270663965426,
            "f1_weighted": 0.3940301621700646
          },
          {
            "accuracy": 0.35498046875,
            "f1": 0.3379814989618741,
            "f1_weighted": 0.33794019881058834
          },
          {
            "accuracy": 0.40673828125,
            "f1": 0.3768288657275166,
            "f1_weighted": 0.37679356729058217
          },
          {
            "accuracy": 0.349609375,
            "f1": 0.33783712101426977,
            "f1_weighted": 0.3377734638327964
          },
          {
            "accuracy": 0.40087890625,
            "f1": 0.39611108165125064,
            "f1_weighted": 0.39610617255183
          },
          {
            "accuracy": 0.37890625,
            "f1": 0.37375716978144957,
            "f1_weighted": 0.37375884523581954
          },
          {
            "accuracy": 0.30224609375,
            "f1": 0.28630778192090695,
            "f1_weighted": 0.2863657442602179
          }
        ]
      }
    ]
  },
  "task_name": "FilipinoShopeeReviewsClassification"
}