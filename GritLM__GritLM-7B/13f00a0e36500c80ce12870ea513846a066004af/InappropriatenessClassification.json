{
  "dataset_revision": "601651fdc45ef243751676e62dd7a19f491c0285",
  "evaluation_time": 15.901878118515015,
  "kg_co2_emissions": 0.0009660415941915332,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "accuracy": 0.601123046875,
        "ap": 0.562329082429981,
        "ap_weighted": 0.562329082429981,
        "f1": 0.5981682525319226,
        "f1_weighted": 0.5981682525319226,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.601123046875,
        "scores_per_experiment": [
          {
            "accuracy": 0.61279296875,
            "ap": 0.5697307606192425,
            "ap_weighted": 0.5697307606192425,
            "f1": 0.6125889322641782,
            "f1_weighted": 0.6125889322641782
          },
          {
            "accuracy": 0.64404296875,
            "ap": 0.5966406476318077,
            "ap_weighted": 0.5966406476318077,
            "f1": 0.6418294576098214,
            "f1_weighted": 0.6418294576098214
          },
          {
            "accuracy": 0.60498046875,
            "ap": 0.5664746214761462,
            "ap_weighted": 0.5664746214761462,
            "f1": 0.6004952721284043,
            "f1_weighted": 0.6004952721284043
          },
          {
            "accuracy": 0.5498046875,
            "ap": 0.5281925497895078,
            "ap_weighted": 0.5281925497895078,
            "f1": 0.5428837029146897,
            "f1_weighted": 0.5428837029146897
          },
          {
            "accuracy": 0.5634765625,
            "ap": 0.5373139252533784,
            "ap_weighted": 0.5373139252533784,
            "f1": 0.5549176789068972,
            "f1_weighted": 0.5549176789068972
          },
          {
            "accuracy": 0.5517578125,
            "ap": 0.5283237940062389,
            "ap_weighted": 0.5283237940062389,
            "f1": 0.5507290845221879,
            "f1_weighted": 0.5507290845221879
          },
          {
            "accuracy": 0.63330078125,
            "ap": 0.5823227046996124,
            "ap_weighted": 0.5823227046996124,
            "f1": 0.631652470880977,
            "f1_weighted": 0.631652470880977
          },
          {
            "accuracy": 0.62939453125,
            "ap": 0.5791654305116034,
            "ap_weighted": 0.5791654305116034,
            "f1": 0.6270899291163984,
            "f1_weighted": 0.6270899291163984
          },
          {
            "accuracy": 0.599609375,
            "ap": 0.5607531317349138,
            "ap_weighted": 0.5607531317349138,
            "f1": 0.5987276731098605,
            "f1_weighted": 0.5987276731098605
          },
          {
            "accuracy": 0.6220703125,
            "ap": 0.5743732585773602,
            "ap_weighted": 0.5743732585773602,
            "f1": 0.6207683238658113,
            "f1_weighted": 0.6207683238658113
          }
        ]
      }
    ]
  },
  "task_name": "InappropriatenessClassification"
}