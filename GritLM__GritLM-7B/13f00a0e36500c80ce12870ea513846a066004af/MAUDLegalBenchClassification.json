{
  "dataset_revision": "12ca3b695563788fead87a982ad1a068284413f4",
  "evaluation_time": 56.08509802818298,
  "kg_co2_emissions": 0.004376179104801768,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "accuracy": 0.307666015625,
        "f1": 0.1709957491236809,
        "f1_weighted": 0.31942173178777744,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.307666015625,
        "scores_per_experiment": [
          {
            "accuracy": 0.34521484375,
            "f1": 0.18078739233330618,
            "f1_weighted": 0.35394360319729873
          },
          {
            "accuracy": 0.3095703125,
            "f1": 0.183608728158841,
            "f1_weighted": 0.3216946276581379
          },
          {
            "accuracy": 0.2724609375,
            "f1": 0.14748437797597264,
            "f1_weighted": 0.29034478218188847
          },
          {
            "accuracy": 0.27099609375,
            "f1": 0.15274341632689745,
            "f1_weighted": 0.28190318066537834
          },
          {
            "accuracy": 0.25341796875,
            "f1": 0.16533449143395068,
            "f1_weighted": 0.2722616103112567
          },
          {
            "accuracy": 0.3203125,
            "f1": 0.18947129853324127,
            "f1_weighted": 0.33058832113311654
          },
          {
            "accuracy": 0.3603515625,
            "f1": 0.18887572986216758,
            "f1_weighted": 0.4010436930618464
          },
          {
            "accuracy": 0.2666015625,
            "f1": 0.15685593788570082,
            "f1_weighted": 0.2601203445716915
          },
          {
            "accuracy": 0.3271484375,
            "f1": 0.1798833792056364,
            "f1_weighted": 0.34691526486976737
          },
          {
            "accuracy": 0.3505859375,
            "f1": 0.1649127395210949,
            "f1_weighted": 0.33540189022739236
          }
        ]
      }
    ]
  },
  "task_name": "MAUDLegalBenchClassification"
}