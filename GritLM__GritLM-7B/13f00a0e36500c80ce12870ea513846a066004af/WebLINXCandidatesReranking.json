{
  "dataset_revision": "ed1c933c2b3617e5700d8a7ebe07f5975969a453",
  "evaluation_time": 27689.498416900635,
  "kg_co2_emissions": 1.9700699634515315,
  "mteb_version": "1.12.41",
  "scores": {
    "test_cat": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.07667458088104155,
        "map": 0.09221460667128899,
        "mrr": 0.07667458088104155,
        "nAUC_map_diff1": 0.15234694928640213,
        "nAUC_map_max": 0.11877432119958933,
        "nAUC_map_std": 0.23941095225313966,
        "nAUC_mrr_diff1": 0.1408754679031553,
        "nAUC_mrr_max": 0.0987852094144019,
        "nAUC_mrr_std": 0.20123254134842788
      }
    ],
    "test_geo": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.1002973768840327,
        "map": 0.11931679055395074,
        "mrr": 0.1002973768840327,
        "nAUC_map_diff1": 0.19803977970819855,
        "nAUC_map_max": 0.06989615083572184,
        "nAUC_map_std": 0.09479986662553458,
        "nAUC_mrr_diff1": 0.2069876534551344,
        "nAUC_mrr_max": 0.0774990869240813,
        "nAUC_mrr_std": 0.0888322895279953
      }
    ],
    "test_iid": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.11344901428350665,
        "map": 0.12928138604914363,
        "mrr": 0.11344901428350665,
        "nAUC_map_diff1": 0.10245706989404454,
        "nAUC_map_max": 0.05709169406685736,
        "nAUC_map_std": 0.11345922558385098,
        "nAUC_mrr_diff1": 0.10863177746566062,
        "nAUC_mrr_max": 0.0689099377893828,
        "nAUC_mrr_std": 0.09965536258892013
      }
    ],
    "test_vis": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.10739077938964688,
        "map": 0.12487100626102693,
        "mrr": 0.10739077938964688,
        "nAUC_map_diff1": 0.09420746044325015,
        "nAUC_map_max": 0.07739982124157885,
        "nAUC_map_std": 0.1463003153352126,
        "nAUC_mrr_diff1": 0.10195746417536293,
        "nAUC_mrr_max": 0.08202499161961792,
        "nAUC_mrr_std": 0.12780916191147693
      }
    ],
    "test_web": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.09381714729997173,
        "map": 0.1090813685644145,
        "mrr": 0.09381714729997173,
        "nAUC_map_diff1": 0.15979883212686097,
        "nAUC_map_max": 0.07801340323851667,
        "nAUC_map_std": 0.11221334907885125,
        "nAUC_mrr_diff1": 0.1683043736053833,
        "nAUC_mrr_max": 0.08223672902930934,
        "nAUC_mrr_std": 0.10260394776645422
      }
    ],
    "validation": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.1366753901150519,
        "map": 0.15242178301756362,
        "mrr": 0.1366753901150519,
        "nAUC_map_diff1": 0.18001590470244705,
        "nAUC_map_max": 0.008179747490222159,
        "nAUC_map_std": -0.052903399415854147,
        "nAUC_mrr_diff1": 0.17970010589445715,
        "nAUC_mrr_max": 0.005452101756042742,
        "nAUC_mrr_std": -0.05670146621441461
      }
    ]
  },
  "task_name": "WebLINXCandidatesReranking"
}