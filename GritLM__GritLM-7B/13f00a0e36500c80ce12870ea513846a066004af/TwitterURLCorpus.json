{
  "dataset_revision": "8b6510b0b1fa4e4c4f879467980e9be563ec1cdf",
  "evaluation_time": 79.5962188243866,
  "kg_co2_emissions": 0.006613707092624522,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.8842899833119882,
          "accuracy_threshold": 0.716887354850769,
          "ap": 0.8454300390476421,
          "f1": 0.7712765957446809,
          "f1_threshold": 0.6974911689758301,
          "precision": 0.751021301429822,
          "recall": 0.7926547582383738
        },
        "dot": {
          "accuracy": 0.8842899833119882,
          "accuracy_threshold": 0.7168872356414795,
          "ap": 0.8454299711490176,
          "f1": 0.7712765957446809,
          "f1_threshold": 0.6974910497665405,
          "precision": 0.751021301429822,
          "recall": 0.7926547582383738
        },
        "euclidean": {
          "accuracy": 0.8842899833119882,
          "accuracy_threshold": 0.7524794340133667,
          "ap": 0.8454300417920262,
          "f1": 0.7712765957446809,
          "f1_threshold": 0.7778288125991821,
          "precision": 0.751021301429822,
          "recall": 0.7926547582383738
        },
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.8457536185359652,
        "manhattan": {
          "accuracy": 0.8843870066363954,
          "accuracy_threshold": 37.55437088012695,
          "ap": 0.8457536185359652,
          "f1": 0.7699502413109357,
          "f1_threshold": 38.931358337402344,
          "precision": 0.7488537952114112,
          "recall": 0.7922697874961503
        },
        "max": {
          "accuracy": 0.8843870066363954,
          "ap": 0.8457536185359652,
          "f1": 0.7712765957446809
        },
        "similarity": {
          "accuracy": 0.8842899833119882,
          "accuracy_threshold": 0.716887354850769,
          "ap": 0.8454300390476421,
          "f1": 0.7712765957446809,
          "f1_threshold": 0.6974911689758301,
          "precision": 0.751021301429822,
          "recall": 0.7926547582383738
        }
      }
    ]
  },
  "task_name": "TwitterURLCorpus"
}