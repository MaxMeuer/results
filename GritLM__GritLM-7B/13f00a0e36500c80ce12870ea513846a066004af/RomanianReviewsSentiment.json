{
  "dataset_revision": "358bcc95aeddd5d07a4524ee416f03d993099b23",
  "evaluation_time": 34.91241264343262,
  "kg_co2_emissions": 0.002656652594956572,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "accuracy": 0.48974609375,
        "f1": 0.41929177195959655,
        "f1_weighted": 0.5276716534360898,
        "hf_subset": "default",
        "languages": [
          "ron-Latn"
        ],
        "main_score": 0.48974609375,
        "scores_per_experiment": [
          {
            "accuracy": 0.50341796875,
            "f1": 0.4198795593436059,
            "f1_weighted": 0.5319339360244436
          },
          {
            "accuracy": 0.37841796875,
            "f1": 0.34249696912230054,
            "f1_weighted": 0.4000410021177043
          },
          {
            "accuracy": 0.44384765625,
            "f1": 0.3920432900763165,
            "f1_weighted": 0.48127037159376423
          },
          {
            "accuracy": 0.56298828125,
            "f1": 0.4616133823323013,
            "f1_weighted": 0.5998939386359703
          },
          {
            "accuracy": 0.490234375,
            "f1": 0.4225485687910131,
            "f1_weighted": 0.5372972560786997
          },
          {
            "accuracy": 0.50341796875,
            "f1": 0.42469606109720304,
            "f1_weighted": 0.5414549489273575
          },
          {
            "accuracy": 0.52099609375,
            "f1": 0.4471589579283939,
            "f1_weighted": 0.5585233156891452
          },
          {
            "accuracy": 0.5302734375,
            "f1": 0.4491863769410853,
            "f1_weighted": 0.5769041055778228
          },
          {
            "accuracy": 0.52099609375,
            "f1": 0.44814506548013366,
            "f1_weighted": 0.5574073693165998
          },
          {
            "accuracy": 0.44287109375,
            "f1": 0.38514948848361186,
            "f1_weighted": 0.4919902903993899
          }
        ]
      }
    ]
  },
  "task_name": "RomanianReviewsSentiment"
}