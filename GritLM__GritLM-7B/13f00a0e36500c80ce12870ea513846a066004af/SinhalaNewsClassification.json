{
  "dataset_revision": "7fb2f514ea683c5282dfec0a9672ece8de90ac50",
  "evaluation_time": 50.28778862953186,
  "kg_co2_emissions": 0.003957687168011737,
  "mteb_version": "1.12.41",
  "scores": {
    "train": [
      {
        "accuracy": 0.563720703125,
        "f1": 0.5430342400153403,
        "f1_weighted": 0.5666560074953249,
        "hf_subset": "default",
        "languages": [
          "sin-Sinh"
        ],
        "main_score": 0.563720703125,
        "scores_per_experiment": [
          {
            "accuracy": 0.59814453125,
            "f1": 0.5787352201959111,
            "f1_weighted": 0.6040503660613424
          },
          {
            "accuracy": 0.591796875,
            "f1": 0.5652543523625683,
            "f1_weighted": 0.592741465197844
          },
          {
            "accuracy": 0.52880859375,
            "f1": 0.5095483010920253,
            "f1_weighted": 0.5349883534257338
          },
          {
            "accuracy": 0.53173828125,
            "f1": 0.5056920058971393,
            "f1_weighted": 0.5246431983629424
          },
          {
            "accuracy": 0.52783203125,
            "f1": 0.5264763076635368,
            "f1_weighted": 0.5319570637988909
          },
          {
            "accuracy": 0.55322265625,
            "f1": 0.5192109879061277,
            "f1_weighted": 0.5469923809382069
          },
          {
            "accuracy": 0.572265625,
            "f1": 0.5575575951350848,
            "f1_weighted": 0.579479511141044
          },
          {
            "accuracy": 0.5546875,
            "f1": 0.5234045675022951,
            "f1_weighted": 0.5582233068038183
          },
          {
            "accuracy": 0.57958984375,
            "f1": 0.5587818203389965,
            "f1_weighted": 0.5876297457617665
          },
          {
            "accuracy": 0.59912109375,
            "f1": 0.5856812420597188,
            "f1_weighted": 0.6058546834616587
          }
        ]
      }
    ]
  },
  "task_name": "SinhalaNewsClassification"
}