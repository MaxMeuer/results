{
  "dataset_revision": "952c9525954c1dac50d5f95945eb5585bb6464e7",
  "evaluation_time": 43.292696952819824,
  "kg_co2_emissions": 0.003427397123704621,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "accuracy": 0.637158203125,
        "f1": 0.5020209730549412,
        "f1_weighted": 0.6922899731495209,
        "hf_subset": "default",
        "languages": [
          "heb-Hebr"
        ],
        "main_score": 0.637158203125,
        "scores_per_experiment": [
          {
            "accuracy": 0.70556640625,
            "f1": 0.5409463162710818,
            "f1_weighted": 0.721941502059284
          },
          {
            "accuracy": 0.6044921875,
            "f1": 0.49287319643604194,
            "f1_weighted": 0.6813227633461717
          },
          {
            "accuracy": 0.548828125,
            "f1": 0.46847573254406977,
            "f1_weighted": 0.6424373038018892
          },
          {
            "accuracy": 0.640625,
            "f1": 0.5149220351571328,
            "f1_weighted": 0.709540346110395
          },
          {
            "accuracy": 0.68896484375,
            "f1": 0.5315141854129806,
            "f1_weighted": 0.7369138219837392
          },
          {
            "accuracy": 0.58837890625,
            "f1": 0.4754716691024974,
            "f1_weighted": 0.6399970626997553
          },
          {
            "accuracy": 0.68701171875,
            "f1": 0.5368592438845249,
            "f1_weighted": 0.7385349700825394
          },
          {
            "accuracy": 0.5234375,
            "f1": 0.4220132921826338,
            "f1_weighted": 0.5801744252308352
          },
          {
            "accuracy": 0.705078125,
            "f1": 0.549940107540786,
            "f1_weighted": 0.76081475748071
          },
          {
            "accuracy": 0.67919921875,
            "f1": 0.48719395201766297,
            "f1_weighted": 0.7112227786998904
          }
        ]
      }
    ]
  },
  "task_name": "HebrewSentimentAnalysis"
}