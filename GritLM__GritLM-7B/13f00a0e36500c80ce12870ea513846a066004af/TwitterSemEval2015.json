{
  "dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1",
  "evaluation_time": 21.11170530319214,
  "kg_co2_emissions": 0.0015989818911786335,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.8500327829766943,
          "accuracy_threshold": 0.7549484968185425,
          "ap": 0.7123787200339213,
          "f1": 0.6475816993464052,
          "f1_threshold": 0.7328045964241028,
          "precision": 0.6417098445595855,
          "recall": 0.6535620052770449
        },
        "dot": {
          "accuracy": 0.8500327829766943,
          "accuracy_threshold": 0.7549485564231873,
          "ap": 0.712378345627312,
          "f1": 0.6475816993464052,
          "f1_threshold": 0.7328046560287476,
          "precision": 0.6417098445595855,
          "recall": 0.6535620052770449
        },
        "euclidean": {
          "accuracy": 0.8500327829766943,
          "accuracy_threshold": 0.7000734806060791,
          "ap": 0.7123786519370081,
          "f1": 0.6475816993464052,
          "f1_threshold": 0.7310203313827515,
          "precision": 0.6417098445595855,
          "recall": 0.6535620052770449
        },
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.7130422859312942,
        "manhattan": {
          "accuracy": 0.8505096262740657,
          "accuracy_threshold": 34.37346649169922,
          "ap": 0.7130422859312942,
          "f1": 0.6470067080116441,
          "f1_threshold": 36.88031768798828,
          "precision": 0.6217465336900997,
          "recall": 0.6744063324538259
        },
        "max": {
          "accuracy": 0.8505096262740657,
          "ap": 0.7130422859312942,
          "f1": 0.6475816993464052
        },
        "similarity": {
          "accuracy": 0.8500327829766943,
          "accuracy_threshold": 0.7549484968185425,
          "ap": 0.7123787200339213,
          "f1": 0.6475816993464052,
          "f1_threshold": 0.7328045964241028,
          "precision": 0.6417098445595855,
          "recall": 0.6535620052770449
        }
      }
    ]
  },
  "task_name": "TwitterSemEval2015"
}