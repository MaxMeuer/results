{
  "dataset_revision": "387ae4582c8054cb52ef57ef0941f19bd8012abf",
  "evaluation_time": 0.6439061164855957,
  "kg_co2_emissions": 0.00011198021182521054,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.712,
          "accuracy_threshold": 0.8072742819786072,
          "ap": 0.8223458597548486,
          "f1": 0.8258064516129032,
          "f1_threshold": 0.8072742819786072,
          "precision": 0.7111111111111111,
          "recall": 0.9846153846153847
        },
        "dot": {
          "accuracy": 0.712,
          "accuracy_threshold": 0.8072739839553833,
          "ap": 0.8223458597548486,
          "f1": 0.8258064516129032,
          "f1_threshold": 0.8072739839553833,
          "precision": 0.7111111111111111,
          "recall": 0.9846153846153847
        },
        "euclidean": {
          "accuracy": 0.712,
          "accuracy_threshold": 0.6208449602127075,
          "ap": 0.8223458597548486,
          "f1": 0.8258064516129032,
          "f1_threshold": 0.6208449602127075,
          "precision": 0.7111111111111111,
          "recall": 0.9846153846153847
        },
        "hf_subset": "default",
        "languages": [
          "ces-Latn"
        ],
        "main_score": 0.8244481270773977,
        "manhattan": {
          "accuracy": 0.7146666666666667,
          "accuracy_threshold": 8.888727188110352,
          "ap": 0.8244481270773977,
          "f1": 0.8287999999999999,
          "f1_threshold": 9.873022079467773,
          "precision": 0.7095890410958904,
          "recall": 0.9961538461538462
        },
        "max": {
          "accuracy": 0.7146666666666667,
          "ap": 0.8244481270773977,
          "f1": 0.8287999999999999
        },
        "similarity": {
          "accuracy": 0.712,
          "accuracy_threshold": 0.8072742819786072,
          "ap": 0.8223458597548486,
          "f1": 0.8258064516129032,
          "f1_threshold": 0.8072742819786072,
          "precision": 0.7111111111111111,
          "recall": 0.9846153846153847
        }
      }
    ],
    "validation": [
      {
        "cosine": {
          "accuracy": 0.6459016393442623,
          "accuracy_threshold": 0.8340224027633667,
          "ap": 0.7150079593625103,
          "f1": 0.7759336099585061,
          "f1_threshold": 0.8231435418128967,
          "precision": 0.6426116838487973,
          "recall": 0.9790575916230366
        },
        "dot": {
          "accuracy": 0.6459016393442623,
          "accuracy_threshold": 0.8340225219726562,
          "ap": 0.7149840515020883,
          "f1": 0.7759336099585061,
          "f1_threshold": 0.8231432437896729,
          "precision": 0.6426116838487973,
          "recall": 0.9790575916230366
        },
        "euclidean": {
          "accuracy": 0.6459016393442623,
          "accuracy_threshold": 0.576155424118042,
          "ap": 0.7150079593625103,
          "f1": 0.7759336099585061,
          "f1_threshold": 0.5947373509407043,
          "precision": 0.6426116838487973,
          "recall": 0.9790575916230366
        },
        "hf_subset": "default",
        "languages": [
          "ces-Latn"
        ],
        "main_score": 0.7150079593625103,
        "manhattan": {
          "accuracy": 0.6426229508196721,
          "accuracy_threshold": 9.220223426818848,
          "ap": 0.710430630958528,
          "f1": 0.7733887733887733,
          "f1_threshold": 9.304771423339844,
          "precision": 0.6413793103448275,
          "recall": 0.9738219895287958
        },
        "max": {
          "accuracy": 0.6459016393442623,
          "ap": 0.7150079593625103,
          "f1": 0.7759336099585061
        },
        "similarity": {
          "accuracy": 0.6459016393442623,
          "accuracy_threshold": 0.8340224027633667,
          "ap": 0.7150079593625103,
          "f1": 0.7759336099585061,
          "f1_threshold": 0.8231435418128967,
          "precision": 0.6426116838487973,
          "recall": 0.9790575916230366
        }
      }
    ]
  },
  "task_name": "CTKFactsNLI"
}