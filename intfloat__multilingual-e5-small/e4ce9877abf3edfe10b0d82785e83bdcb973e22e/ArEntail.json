{
  "dataset_revision": "4da4316c6e3287746ab74ff67dd252ad128fceff",
  "evaluation_time": 0.8210132122039795,
  "kg_co2_emissions": 0.0001307925506801246,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.727,
          "accuracy_threshold": 0.9167771339416504,
          "ap": 0.7841730237745712,
          "f1": 0.7506297229219144,
          "f1_threshold": 0.8977869749069214,
          "precision": 0.6468885672937771,
          "recall": 0.894
        },
        "dot": {
          "accuracy": 0.727,
          "accuracy_threshold": 0.9167771339416504,
          "ap": 0.7841706179811935,
          "f1": 0.7506297229219144,
          "f1_threshold": 0.8977869153022766,
          "precision": 0.6468885672937771,
          "recall": 0.894
        },
        "euclidean": {
          "accuracy": 0.727,
          "accuracy_threshold": 0.4079776406288147,
          "ap": 0.7841753436634026,
          "f1": 0.7506297229219144,
          "f1_threshold": 0.45213496685028076,
          "precision": 0.6468885672937771,
          "recall": 0.894
        },
        "hf_subset": "default",
        "languages": [
          "ara-Arab"
        ],
        "main_score": 0.7841753436634026,
        "manhattan": {
          "accuracy": 0.721,
          "accuracy_threshold": 6.490774154663086,
          "ap": 0.779163924978791,
          "f1": 0.7472150814053129,
          "f1_threshold": 6.991239070892334,
          "precision": 0.6536731634182908,
          "recall": 0.872
        },
        "max": {
          "accuracy": 0.727,
          "ap": 0.7841753436634026,
          "f1": 0.7506297229219144
        },
        "similarity": {
          "accuracy": 0.727,
          "accuracy_threshold": 0.9167771339416504,
          "ap": 0.7841730237745712,
          "f1": 0.7506297229219144,
          "f1_threshold": 0.8977869749069214,
          "precision": 0.6468885672937771,
          "recall": 0.894
        }
      }
    ]
  },
  "task_name": "ArEntail"
}