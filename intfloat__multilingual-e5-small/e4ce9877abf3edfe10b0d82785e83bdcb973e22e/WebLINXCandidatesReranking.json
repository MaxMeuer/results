{
  "dataset_revision": "ed1c933c2b3617e5700d8a7ebe07f5975969a453",
  "evaluation_time": 903.6800475120544,
  "kg_co2_emissions": 0.16794115412675897,
  "mteb_version": "1.12.41",
  "scores": {
    "test_cat": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.08838550026752275,
        "map": 0.10485454277925554,
        "mrr": 0.08838550026752275,
        "nAUC_map_diff1": 0.192065848635695,
        "nAUC_map_max": 0.07766507964645708,
        "nAUC_map_std": 0.2447136055213264,
        "nAUC_mrr_diff1": 0.18613034388468952,
        "nAUC_mrr_max": 0.07829846511528742,
        "nAUC_mrr_std": 0.22331803066906464
      }
    ],
    "test_geo": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.10977073566068685,
        "map": 0.12671020869171945,
        "mrr": 0.10977073566068685,
        "nAUC_map_diff1": 0.0957443287472777,
        "nAUC_map_max": 0.013632683461023101,
        "nAUC_map_std": 0.18997584635443393,
        "nAUC_mrr_diff1": 0.10247016419820314,
        "nAUC_mrr_max": 0.016404057459693522,
        "nAUC_mrr_std": 0.18724800706434158
      }
    ],
    "test_iid": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.0987504691259907,
        "map": 0.11433487448368043,
        "mrr": 0.0987504691259907,
        "nAUC_map_diff1": -0.029085209436306162,
        "nAUC_map_max": 0.04301201258063328,
        "nAUC_map_std": 0.08379654329062075,
        "nAUC_mrr_diff1": -0.035954167816911936,
        "nAUC_mrr_max": 0.05752693754759389,
        "nAUC_mrr_std": 0.06583937965583928
      }
    ],
    "test_vis": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.09640467801566328,
        "map": 0.1133249573510551,
        "mrr": 0.09640467801566328,
        "nAUC_map_diff1": 0.115336572185604,
        "nAUC_map_max": -0.015250701772767997,
        "nAUC_map_std": 0.11087069475474153,
        "nAUC_mrr_diff1": 0.12075412077010328,
        "nAUC_mrr_max": -0.006922249856401121,
        "nAUC_mrr_std": 0.09512874581597759
      }
    ],
    "test_web": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.09852048749949514,
        "map": 0.11446025875679745,
        "mrr": 0.09852048749949514,
        "nAUC_map_diff1": 0.22123967610792783,
        "nAUC_map_max": 0.08754836431044093,
        "nAUC_map_std": -0.04016921052111217,
        "nAUC_mrr_diff1": 0.22663210455954894,
        "nAUC_mrr_max": 0.08886847225456777,
        "nAUC_mrr_std": -0.053891780961410216
      }
    ],
    "validation": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.12424691629149738,
        "map": 0.14011847035195665,
        "mrr": 0.12424691629149738,
        "nAUC_map_diff1": 0.22122187530230925,
        "nAUC_map_max": 0.0058234066593383415,
        "nAUC_map_std": 0.09886427036857791,
        "nAUC_mrr_diff1": 0.22591938184381266,
        "nAUC_mrr_max": 0.011570416817799295,
        "nAUC_mrr_std": 0.09591697870586183
      }
    ]
  },
  "task_name": "WebLINXCandidatesReranking"
}