{
  "dataset_revision": "673a610d6d3dd91a547a0d57ae1b56f37ebbf6a1",
  "evaluation_time": 35.6524453163147,
  "kg_co2_emissions": 0.007093339272896599,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.623095703125,
        "f1": 0.6049638842557341,
        "f1_weighted": 0.6050756456918711,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.623095703125,
        "scores_per_experiment": [
          {
            "accuracy": 0.6357421875,
            "f1": 0.6142626922897848,
            "f1_weighted": 0.6144416822225993
          },
          {
            "accuracy": 0.63818359375,
            "f1": 0.6216583873894326,
            "f1_weighted": 0.621810713386953
          },
          {
            "accuracy": 0.61865234375,
            "f1": 0.6013273615158058,
            "f1_weighted": 0.6014854881448954
          },
          {
            "accuracy": 0.62841796875,
            "f1": 0.610217648422155,
            "f1_weighted": 0.6102952226549911
          },
          {
            "accuracy": 0.61865234375,
            "f1": 0.6059714324337762,
            "f1_weighted": 0.6060545962686321
          },
          {
            "accuracy": 0.59521484375,
            "f1": 0.5786321269416174,
            "f1_weighted": 0.5786779816612788
          },
          {
            "accuracy": 0.6328125,
            "f1": 0.6124030164646636,
            "f1_weighted": 0.6125097343412839
          },
          {
            "accuracy": 0.611328125,
            "f1": 0.5895252957817709,
            "f1_weighted": 0.5896244179153922
          },
          {
            "accuracy": 0.6298828125,
            "f1": 0.6094687476613936,
            "f1_weighted": 0.6096015885015298
          },
          {
            "accuracy": 0.6220703125,
            "f1": 0.6061721336569404,
            "f1_weighted": 0.606255031821155
          }
        ]
      }
    ]
  },
  "task_name": "RuSciBenchGRNTIClassification"
}